<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="http://jekyllrb.com" version="3.4.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-01-25T20:46:33+08:00</updated><id>http://localhost:4000/</id><title type="html">Base</title><subtitle>Knowledge base template for Jekyll.</subtitle><author><name>{&quot;name&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil}</name></author><entry><title type="html">Feature Engineering Part II – Missing Values</title><link href="http://localhost:4000/machine-learning/Missing-data-imputation/" rel="alternate" type="text/html" title="Feature Engineering Part II -- Missing Values" /><published>2018-01-25T00:00:00+08:00</published><updated>2018-01-25T00:00:00+08:00</updated><id>http://localhost:4000/machine-learning/Missing-data-imputation</id><content type="html" xml:base="http://localhost:4000/machine-learning/Missing-data-imputation/">&lt;h2 id=&quot;imputation&quot;&gt;Imputation&lt;/h2&gt;

&lt;p&gt;a. Replace it with a number outside the normal range like -999 
b. Replace it with mean or median
c. Reconstruct values&lt;/p&gt;

&lt;p&gt;Method a is useful because it gives trees the possibility to take missing values into a separate category. The downside is that performance of linear models and neural networks will suffer.&lt;/p&gt;

&lt;p&gt;Method b is beneficial for simple linear models and neural networks. But tree-based methods can be harder to select object with missing values in the first place.&lt;/p&gt;

&lt;h3 id=&quot;method-c-reconstruct-values&quot;&gt;Method c: Reconstruct values&lt;/h3&gt;

&lt;p&gt;When data points are dependent to each other like time series data, we can approximate NAs using observations in the neighborhood.&lt;/p&gt;

&lt;p&gt;Challenges come when data points are independent.&lt;/p&gt;

&lt;h2 id=&quot;feature-generation&quot;&gt;Feature Generation&lt;/h2&gt;

&lt;p&gt;Sometimes we create a binary feature, isnull, indicating which rows have missing values for this feature. Through this way, we can address concerns about trees and neural networks while computing mean or median. The drawback is that we will double the number of columns.&lt;/p&gt;

&lt;p&gt;This method is especially worth trying for missing data occured in test set only. The intuition behind is that categories absent in the training set will be treated randomly eventually.&lt;/p&gt;

&lt;p&gt;As an alternative, we may have a try on frequency encoding with missing values as a new category.&lt;/p&gt;

&lt;h2 id=&quot;xgboost-can-handle-nan&quot;&gt;Xgboost can handle NaN.&lt;/h2&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil}</name></author><summary type="html">Imputation</summary></entry><entry><title type="html">Bag of words</title><link href="http://localhost:4000/machine-learning/bag-of-words/" rel="alternate" type="text/html" title="Bag of words" /><published>2018-01-22T00:00:00+08:00</published><updated>2018-01-22T00:00:00+08:00</updated><id>http://localhost:4000/machine-learning/bag-of-words</id><content type="html" xml:base="http://localhost:4000/machine-learning/bag-of-words/">&lt;p&gt;If text and images are the only data we got, it’s more convenient to apply specific approach for these types of data.&lt;/p&gt;

&lt;p&gt;The common scenario is that we have text or images as additional data set, we need to grasp different features that can be input to machine learning models as a complementary to our main data frame of samples and features.&lt;/p&gt;

&lt;h2 id=&quot;bag-of-words-bow&quot;&gt;Bag of words (BOW)&lt;/h2&gt;

&lt;p&gt;Pipeline of applying BOW:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Preprocessing:
 Lowercase, stemming, lemmatization, stopwords&lt;/li&gt;
  &lt;li&gt;N-grams can help to use local context&lt;/li&gt;
  &lt;li&gt;Postprocessing: TF-IDF&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;tf-idf&quot;&gt;TF-IDF&lt;/h3&gt;

&lt;p&gt;Intuition: normalize data column-wise&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sklearn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_extraction&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TfidfVectorizer&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;term-frequency-tf&quot;&gt;Term Frequency (TF)&lt;/h4&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[:,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;inverse-document-frequency-idf&quot;&gt;Inverse Document Frequency (IDF)&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;idf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idf&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;n-grams&quot;&gt;N-grams&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sklearn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_extraction&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CountVectorizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;Ngram_range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;analyzer&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;If you have 28 unique symbols, the number of all possible combinations is 28x28&lt;/p&gt;

&lt;h2 id=&quot;next-embeddings&quot;&gt;Next: Embeddings&lt;/h2&gt;

&lt;p&gt;Embeddings like Word2Vec&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil}</name></author><summary type="html">If text and images are the only data we got, it’s more convenient to apply specific approach for these types of data.</summary></entry><entry><title type="html">Word2Vec</title><link href="http://localhost:4000/recommender/Word2Vec/" rel="alternate" type="text/html" title="Word2Vec" /><published>2018-01-22T00:00:00+08:00</published><updated>2018-01-22T00:00:00+08:00</updated><id>http://localhost:4000/recommender/Word2Vec</id><content type="html" xml:base="http://localhost:4000/recommender/Word2Vec/">&lt;p&gt;At Openrice, I’m always looking for ways to leverage massive data set to automate customer-facing experiences.&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil}</name></author><summary type="html">At Openrice, I’m always looking for ways to leverage massive data set to automate customer-facing experiences.</summary></entry><entry><title type="html">Statistical tests</title><link href="http://localhost:4000/statistics/Statistical-tests/" rel="alternate" type="text/html" title="Statistical tests" /><published>2018-01-22T00:00:00+08:00</published><updated>2018-01-22T00:00:00+08:00</updated><id>http://localhost:4000/statistics/Statistical-tests</id><content type="html" xml:base="http://localhost:4000/statistics/Statistical-tests/"></content><author><name>{&quot;name&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil}</name></author><summary type="html"></summary></entry><entry><title type="html">PyMongo</title><link href="http://localhost:4000/etl/PyMongo/" rel="alternate" type="text/html" title="PyMongo" /><published>2018-01-22T00:00:00+08:00</published><updated>2018-01-22T00:00:00+08:00</updated><id>http://localhost:4000/etl/PyMongo</id><content type="html" xml:base="http://localhost:4000/etl/PyMongo/"></content><author><name>{&quot;name&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil}</name></author><summary type="html"></summary></entry><entry><title type="html">Deep Neural Networds</title><link href="http://localhost:4000/recommender/Deep-Learning/" rel="alternate" type="text/html" title="Deep Neural Networds" /><published>2018-01-22T00:00:00+08:00</published><updated>2018-01-22T00:00:00+08:00</updated><id>http://localhost:4000/recommender/Deep-Learning</id><content type="html" xml:base="http://localhost:4000/recommender/Deep-Learning/"></content><author><name>{&quot;name&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil}</name></author><summary type="html"></summary></entry><entry><title type="html">Collaborative Filtering</title><link href="http://localhost:4000/recommender/Collaborative-Filtering/" rel="alternate" type="text/html" title="Collaborative Filtering" /><published>2018-01-22T00:00:00+08:00</published><updated>2018-01-22T00:00:00+08:00</updated><id>http://localhost:4000/recommender/Collaborative-Filtering</id><content type="html" xml:base="http://localhost:4000/recommender/Collaborative-Filtering/"></content><author><name>{&quot;name&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil}</name></author><summary type="html"></summary></entry><entry><title type="html">A/B Testing</title><link href="http://localhost:4000/statistics/AB-testing/" rel="alternate" type="text/html" title="A/B Testing" /><published>2018-01-22T00:00:00+08:00</published><updated>2018-01-22T00:00:00+08:00</updated><id>http://localhost:4000/statistics/AB-testing</id><content type="html" xml:base="http://localhost:4000/statistics/AB-testing/"></content><author><name>{&quot;name&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil}</name></author><summary type="html"></summary></entry><entry><title type="html">Machine Learning Recap</title><link href="http://localhost:4000/machine-learning/machine-learning-recap/" rel="alternate" type="text/html" title="Machine Learning Recap" /><published>2018-01-15T00:00:00+08:00</published><updated>2018-01-15T00:00:00+08:00</updated><id>http://localhost:4000/machine-learning/machine-learning-recap</id><content type="html" xml:base="http://localhost:4000/machine-learning/machine-learning-recap/">&lt;h2 id=&quot;no-free-lunch-theorem&quot;&gt;No Free Lunch Theorem&lt;/h2&gt;

&lt;p&gt;Basically, No Free Lunch Theorem states that there is no methods which outperform all others on all tasks.&lt;/p&gt;

&lt;p&gt;The reason behind is that every method relies on certain assumptions about data or task. If these assumptions fail, it will perform poorly.&lt;/p&gt;

&lt;p&gt;For us, this means that we cannot every competition with just a single algorithm. So it is important for us to have a clear mind map of various models based off different assumptions.&lt;/p&gt;

&lt;p&gt;Then let’s start getting familiar with the four popular families of machine learning algorithms.&lt;/p&gt;

&lt;h2 id=&quot;linear-model&quot;&gt;Linear Model&lt;/h2&gt;

&lt;p&gt;Linear models try to separate objects with a plane which divides space into two parts.&lt;/p&gt;

&lt;p&gt;With 2 sets of points, it is quite intuitive to separate them using a line. This approach can be generalized for a higher dimensional space. This is the main idea behind linear models.&lt;/p&gt;

&lt;p&gt;Logistic regression or SVM are all linear models withdifferent loss functions.&lt;/p&gt;

&lt;h3 id=&quot;linear-models-are-especially-good-for-sparse-high-dimensional-data&quot;&gt;Linear models are especially good for sparse high dimensional data.&lt;/h3&gt;

&lt;h2 id=&quot;tree-based-methods&quot;&gt;Tree-Based Methods&lt;/h2&gt;

&lt;p&gt;In general, tree-based models are very powerful and can be a good default method for tabular data.&lt;/p&gt;

&lt;p&gt;Intuitively, a single decision tree can be considered as dividing space into boxes and approximating data with a constant inside of these boxes. It uses divide-and-conquer approach to recur sub-split spaces into sub-spaces.&lt;/p&gt;

&lt;p&gt;The way of true axis splits and corresponding constants produces several approaches for building decision trees. Moreover, such trees can be combined together in a lot of ways. All this leads to a wide variety of tree-based algorithms, most famous of them being random forest and Gradient Boosted Decision Trees.&lt;/p&gt;

&lt;p&gt;Scikit-Learn contains quite good implementation of random forest. For gradient boost decision trees, you may find XGBoost and LightGBM with higher speed and accuracy.&lt;/p&gt;

&lt;h2 id=&quot;k-nearest-neighbors-k-nn&quot;&gt;k-Nearest Neighbors (k-NN)&lt;/h2&gt;

&lt;p&gt;The intuition behind k-NN is very simple. Closer objects will likely to have same labels.&lt;/p&gt;

&lt;h2 id=&quot;neural-networks&quot;&gt;Neural Networks&lt;/h2&gt;

&lt;p&gt;Neural Nets is a special class of machine learning models, which deserve a separate topic.&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil}</name></author><summary type="html">No Free Lunch Theorem</summary></entry><entry><title type="html">Data mining, statistics, machine learning and AI</title><link href="http://localhost:4000/machine-learning/dm-stat-ml/" rel="alternate" type="text/html" title="Data mining, statistics, machine learning and AI" /><published>2018-01-15T00:00:00+08:00</published><updated>2018-01-15T00:00:00+08:00</updated><id>http://localhost:4000/machine-learning/dm-stat-ml</id><content type="html" xml:base="http://localhost:4000/machine-learning/dm-stat-ml/">&lt;p&gt;Artificial Intelligence is the study of how to create intelligent agents. Most tasks that require intelligence require an ability to induce new knowledge from experiences. Thus, a large area within AI is machine learning. Procedures in machine learning include ideas derived directly from, or inspired by, classical statistics, but they don’t have to be. Data mining is an area that has taken much of its inspiration and techniques from machine learning (and some, also, from statistics), but aims at either to discover / generate some preliminary insights in an area where there really was little knowledge beforehand, or to be able to predict future observations accurately.&lt;/p&gt;

&lt;p&gt;There are considerable overlaps among them. If we have to make some distinctions, they are:&lt;/p&gt;

&lt;p&gt;Statistics is about &lt;strong&gt;numbers&lt;/strong&gt;. It is mostly employed towards better understanding particular data generating processes. Thus, it usually starts with a formally specified model, and from this are derived procedures to accurately extract that model from noisy instances (i.e., estimation–by optimizing some loss function) and to be able to distinguish it from other possibilities (i.e., inferences based on known properties of sampling distributions). The prototypical statistical technique is regression.&lt;/p&gt;

&lt;p&gt;Data Mining is about using Statistics as well as other programming methods to find &lt;strong&gt;patterns&lt;/strong&gt; hidden in the data so that you can explain some phenomenon. Data Mining builds intuition about what is really happening in some data and is still little more towards math than programming, but uses both. Common data mining techniques would include cluster analyses, classification and regression trees, and neural networks.&lt;/p&gt;

&lt;p&gt;Machine Learning uses Data Mining techniques and other learning algorithms to build models of what is happening behind some data so that it can predict future outcomes. Math is the basis for many of the algorithms, but this is more towards programming. A computer program is said to learn some task from experience if its performance at the task improves with experience, according to some performance measure. Machine learning involves the study of algorithms that can extract information automatically (i.e., without on-line human guidance).&lt;/p&gt;

&lt;p&gt;Artificial Intelligence uses models built by Machine Learning and other ways to reason about the world and give rise to intelligent behavior whether this is playing a game or driving a robot/car. Artificial Intelligence has some goal to achieve by predicting how actions will affect the model of the world and chooses the actions that will best achieve that goal. Very programming based.&lt;/p&gt;</content><author><name>{&quot;name&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil}</name></author><summary type="html">Artificial Intelligence is the study of how to create intelligent agents. Most tasks that require intelligence require an ability to induce new knowledge from experiences. Thus, a large area within AI is machine learning. Procedures in machine learning include ideas derived directly from, or inspired by, classical statistics, but they don’t have to be. Data mining is an area that has taken much of its inspiration and techniques from machine learning (and some, also, from statistics), but aims at either to discover / generate some preliminary insights in an area where there really was little knowledge beforehand, or to be able to predict future observations accurately.</summary></entry></feed>