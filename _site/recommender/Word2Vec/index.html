<!DOCTYPE html>

<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<meta name="generator" content="Jekyll v3.4.3">

		<link rel="stylesheet" href="/css/screen.css">
		<link rel="apple-touch-icon" href="/apple-touch-icon.png">
		<link rel="icon" type="image/png" href="/touch-icon.png" sizes="192x192">
		<link rel="icon" type="image/png" href="/images/favicon.png">
		<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Merriweather:400italic,400,300italic,300,700,700italic|Open+Sans:400italic,600italic,700italic,700,600,400|Inconsolata:400,700">
		<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>

		<!-- Begin Jekyll SEO tag v2.1.0 -->
<title>From Word2Vec to Item Recommendation - Sara</title>
<meta property="og:title" content="From Word2Vec to Item Recommendation" />
<meta name="description" content="Recommendation System Powered by Word2Vec" />
<meta property="og:description" content="Recommendation System Powered by Word2Vec" />
<link rel="canonical" href="/recommender/Word2Vec/" />
<meta property="og:url" content="/recommender/Word2Vec/" />
<meta property="og:site_name" content="Sara" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-01-22T00:00:00+08:00" />
<script type="application/ld+json">
{"@context": "http://schema.org",
"@type": "BlogPosting",
"headline": "From Word2Vec to Item Recommendation",
"datePublished": "2018-01-22T00:00:00+08:00",
"description": "Recommendation System Powered by Word2Vec",
"publisher": {"@type": "Organization",
"logo": {"@type": "ImageObject",
"url": "/siteicon.png"}},
"url": "/recommender/Word2Vec/"}</script>
<!-- End Jekyll SEO tag -->

		<link type="application/atom+xml" rel="alternate" href="/feed.xml" title="Sara" />

		
	</head>

	<body class="">
		<header>
			<div class="wrapper">
				<section class="top-bar">
					<a class="nav-toggle" id="open-nav" href="#">&#9776;</a>
<nav>
	<a class="editor-link btn" href="cloudcannon:collections/_data/navigation.yml" class="btn" style="padding: 5px;"><strong>&#9998;</strong> Edit navigation</a>
	
	

		
		<a href="/" class="">Blog</a>
	
</nav>

				</section>
				<section class="hero_search">
					<h1>5AM</h1>
					<p>The hour when Legends are waking up, or going to sleep</p>
					<form action="/search/" method="get">
	<input type="search" name="q"  placeholder="What would you like to know?" autofocus>
	<svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"/>
    <path d="M0 0h24v24H0z" fill="none"/>
</svg>
	<input type="submit" value="Search" style="display: none;">
</form>
				</section>
			</div>

		</header>
		<section class="content">
			<div class="wrapper">
				<section class="tutorial">
	<section class="sidebar sticky">
		<ul>
			
			
				<li><a href="#step-1-make-every-users-to-read-list-as-a-sentence-and-feed-it-into-word2vec">Step 1. Make every user’s to-read list as a sentence and feed it into word2vec</a></li>
			
				<li><a href="#step-2-build-a-skip-gram-model">Step 2. Build a skip-gram model</a></li>
			
				<li><a href="#step-3-start-training">Step 3. Start Training</a></li>
			
				<li><a href="#step-4-save-the-final-embeddings-for-recommendation">Step 4. Save the final embeddings for recommendation</a></li>
			
		</ul>
	</section>
	<section class="tutorial-content">
		<h1>From Word2Vec to Item Recommendation</h1>

		
		
		<div class="tutorial-main">
			<p>As a data analyst, I’m always looking for ways to leverage massive dataset to automate customer-facing experiences. Recently I’m focusing on using word2vec to power a recommendation engine.</p>

<p>Following is a minimalist script explaining how to apply Word2Vec for item recommendation and the data science behind it.</p>

<p>The demo data can be downloaded here: https://github.com/zygmuntz/goodbooks-10k</p>

<h2 id="step-1-make-every-users-to-read-list-as-a-sentence-and-feed-it-into-word2vec">Step 1. Make every user’s to-read list as a sentence and feed it into word2vec</h2>

<p>Given a dataset large enough, it can find out similar books</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="kn">import</span> <span class="nb">xrange</span>
</code></pre>
</div>

<p>It’s worth mentioning that the syntax of <code class="highlighter-rouge">xrange</code> and <code class="highlighter-rouge">range</code> are the same.</p>

<ul>
  <li>xrange(stop)</li>
  <li>xrange(start, stop[, step])</li>
</ul>

<p>The major difference is that <code class="highlighter-rouge">range</code> outputs tuples while <code class="highlighter-rouge">xrange</code> gives us a generator, which is more efficient when data are too large to fit in memory.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">tr</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span> <span class="s">'to_read.csv'</span> <span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span> <span class="s">'books.csv'</span> <span class="p">)</span>
<span class="n">bt</span> <span class="o">=</span> <span class="n">tr</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span> <span class="n">b</span><span class="p">[[</span> <span class="s">'book_id'</span><span class="p">,</span> <span class="s">'title'</span><span class="p">]],</span> <span class="n">on</span> <span class="o">=</span> <span class="s">'book_id'</span> <span class="p">)</span>
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">bt</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre>
</div>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>user_id</th>
      <th>book_id</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>535344</th>
      <td>12001</td>
      <td>1587</td>
      <td>Saving CeeCee Honeycutt</td>
    </tr>
    <tr>
      <th>127117</th>
      <td>996</td>
      <td>4055</td>
      <td>Jesus' Son</td>
    </tr>
    <tr>
      <th>57970</th>
      <td>43087</td>
      <td>457</td>
      <td>The Historian</td>
    </tr>
    <tr>
      <th>197783</th>
      <td>14942</td>
      <td>148</td>
      <td>Girl with a Pearl Earring</td>
    </tr>
    <tr>
      <th>754480</th>
      <td>40397</td>
      <td>359</td>
      <td>And the Mountains Echoed</td>
    </tr>
    <tr>
      <th>105202</th>
      <td>9323</td>
      <td>187</td>
      <td>Uglies (Uglies, #1)</td>
    </tr>
    <tr>
      <th>692645</th>
      <td>40233</td>
      <td>3727</td>
      <td>The Rook (The Checquy Files, #1)</td>
    </tr>
    <tr>
      <th>905310</th>
      <td>42392</td>
      <td>1028</td>
      <td>Truly Madly Guilty</td>
    </tr>
    <tr>
      <th>728410</th>
      <td>2452</td>
      <td>4402</td>
      <td>The Perfect Hope (Inn Boonsboro, #3)</td>
    </tr>
    <tr>
      <th>100382</th>
      <td>15170</td>
      <td>4</td>
      <td>To Kill a Mockingbird</td>
    </tr>
  </tbody>
</table>
</div>

<p>Detailed exploration and data cleaning process -&gt;</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">raw</span> <span class="o">=</span> <span class="n">bt</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">'user_id'</span><span class="p">)[</span><span class="s">'title'</span><span class="p">]</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">raw</span><span class="p">)</span><span class="o">==</span><span class="n">tr</span><span class="o">.</span><span class="n">user_id</span><span class="o">.</span><span class="n">nunique</span><span class="p">()</span>
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span> <span class="k">for</span> <span class="n">books</span> <span class="ow">in</span> <span class="n">raw</span><span class="o">.</span><span class="n">values</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">books</span><span class="p">]</span>
<span class="n">n_words</span> <span class="o">=</span> <span class="n">bt</span><span class="o">.</span><span class="n">title</span><span class="o">.</span><span class="n">nunique</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">"There are {} books and {} to-read records"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_words</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)))</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>There are 9951 books and 912705 to-read records
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">build_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">n_words</span><span class="p">):</span>
    <span class="n">count</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">count</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">collections</span><span class="o">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">words</span><span class="p">)</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="n">n_words</span><span class="p">))</span>
    <span class="n">dictionary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">count</span><span class="p">:</span>
        <span class="n">dictionary</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dictionary</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">dictionary</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
    <span class="n">reversed_dictionary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">dictionary</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">dictionary</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
    <span class="k">return</span> <span class="n">data</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">,</span> <span class="n">reversed_dictionary</span>


</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">data</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">,</span> <span class="n">reverse_dictionary</span> <span class="o">=</span> <span class="n">build_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">,</span><span class="n">n_words</span><span class="p">)</span>
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">words</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>"The Aviator's Wife"
</code></pre>
</div>

<h2 id="step-2-build-a-skip-gram-model">Step 2. Build a skip-gram model</h2>

<p>We can create our own word embeddings easily using <code class="highlighter-rouge">gensim</code>, an open source Python library focusing on topic modeling.</p>

<p>Here I will have a try on <code class="highlighter-rouge">TensorFlow</code>.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">data_index</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">def</span> <span class="nf">generate_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_skips</span><span class="p">,</span> <span class="n">skip_window</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">data_index</span>
    <span class="k">assert</span> <span class="n">batch_size</span> <span class="o">%</span> <span class="n">num_skips</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">num_skips</span> <span class="o">&lt;=</span> <span class="mi">2</span><span class="o">*</span><span class="n">skip_window</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">span</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">skip_window</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c"># [ skip_window target skip_window ]</span>
    <span class="nb">buffer</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">span</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">data_index</span> <span class="o">+</span> <span class="n">span</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
        <span class="n">data_index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="nb">buffer</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">data_index</span><span class="p">:</span><span class="n">data_index</span> <span class="o">+</span> <span class="n">span</span><span class="p">])</span>
    <span class="n">data_index</span> <span class="o">+=</span> <span class="n">span</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">//</span> <span class="n">num_skips</span><span class="p">):</span>
        <span class="n">context_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">span</span><span class="p">)</span> <span class="k">if</span> <span class="n">w</span> <span class="o">!=</span> <span class="n">skip_window</span><span class="p">]</span>
        <span class="n">words_to_use</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">context_words</span><span class="p">,</span> <span class="n">num_skips</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">context_word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words_to_use</span><span class="p">):</span>
            <span class="n">batch</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">num_skips</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="nb">buffer</span><span class="p">[</span><span class="n">skip_window</span><span class="p">]</span>
            <span class="n">labels</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">num_skips</span> <span class="o">+</span> <span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">buffer</span><span class="p">[</span><span class="n">context_word</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">data_index</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">book</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[:</span><span class="n">span</span><span class="p">]:</span>
                <span class="nb">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">book</span><span class="p">)</span>
            <span class="n">data_index</span> <span class="o">=</span> <span class="n">span</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">data_index</span><span class="p">])</span>
            <span class="n">data_index</span> <span class="o">+=</span> <span class="mi">1</span>
  <span class="c"># Backtrack a little bit to avoid skipping words in the end of a batch</span>
    <span class="n">data_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_index</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">-</span> <span class="n">span</span><span class="p">)</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">batch</span><span class="p">,</span> <span class="n">labels</span>

<span class="n">batch</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">generate_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_skips</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">skip_window</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">):</span>
  <span class="k">print</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">batch</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span>
        <span class="s">'-&gt;'</span><span class="p">,</span> <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>


</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>86 Me Before You (Me Before You, #1) -&gt; 720 The Aviator's Wife
86 Me Before You (Me Before You, #1) -&gt; 380 The Husband's Secret
380 The Husband's Secret -&gt; 86 Me Before You (Me Before You, #1)
380 The Husband's Secret -&gt; 175 A Little Life
175 A Little Life -&gt; 380 The Husband's Secret
175 A Little Life -&gt; 51 Go Set a Watchman
51 Go Set a Watchman -&gt; 175 A Little Life
51 Go Set a Watchman -&gt; 1095 The Marriage of Opposites
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">embedding_size</span> <span class="o">=</span> <span class="mi">128</span>  <span class="c"># Dimension of the embedding vector.</span>
<span class="n">skip_window</span> <span class="o">=</span> <span class="mi">1</span>       <span class="c"># How many words to consider left and right.</span>
<span class="n">num_skips</span> <span class="o">=</span> <span class="mi">2</span>         <span class="c"># How many times to reuse an input to generate a label.</span>
<span class="n">num_sampled</span> <span class="o">=</span> <span class="mi">64</span>      <span class="c"># Number of negative examples to sample.</span>

<span class="c"># We pick a random validation set to sample nearest neighbors. Here we limit the</span>
<span class="c"># validation samples to the words that have a low numeric ID, which by</span>
<span class="c"># construction are also the most frequent. These 3 variables are used only for</span>
<span class="c"># displaying model accuracy, they don't affect calculation.</span>
<span class="n">valid_size</span> <span class="o">=</span> <span class="mi">16</span>     <span class="c"># Random set of words to evaluate similarity on.</span>
<span class="n">valid_window</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c"># Only pick dev samples in the head of the distribution.</span>
<span class="n">valid_examples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">valid_window</span><span class="p">,</span> <span class="n">valid_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>


<span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>

<span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>

  <span class="c"># Input data.</span>
    <span class="n">train_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">])</span>
    <span class="n">train_labels</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">valid_examples</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

  <span class="c"># Ops and variables pinned to the CPU because of missing GPU implementation</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">'/cpu:0'</span><span class="p">):</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">([</span><span class="n">n_words</span><span class="p">,</span><span class="n">embedding_size</span><span class="p">],</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">))</span>
        <span class="n">embed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">train_inputs</span><span class="p">)</span>

        <span class="c"># Construct the variables for the NCE loss</span>
        <span class="n">nce_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">n_words</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">],</span>
                                <span class="n">stddev</span><span class="o">=</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">)))</span>
        <span class="n">nce_biases</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">n_words</span><span class="p">]))</span>

    <span class="c"># Compute the average NCE loss for the batch.</span>
    <span class="c"># tf.nce_loss automatically draws a new sample of the negative labels each</span>
    <span class="c"># time we evaluate the loss.</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">nce_loss</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">nce_weights</span><span class="p">,</span>
                     <span class="n">biases</span><span class="o">=</span><span class="n">nce_biases</span><span class="p">,</span>
                     <span class="n">labels</span><span class="o">=</span><span class="n">train_labels</span><span class="p">,</span>
                     <span class="n">inputs</span><span class="o">=</span><span class="n">embed</span><span class="p">,</span>
                     <span class="n">num_sampled</span><span class="o">=</span><span class="n">num_sampled</span><span class="p">,</span>
                     <span class="n">num_classes</span><span class="o">=</span><span class="n">n_words</span><span class="p">))</span>

    <span class="c"># Construct the SGD optimizer using a learning rate of 1.0.</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

    <span class="c"># Compute the cosine similarity between minibatch examples and all embeddings.</span>
    <span class="n">norm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">embeddings</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    <span class="n">normalized_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span> <span class="o">/</span> <span class="n">norm</span>
    <span class="n">valid_embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span>
      <span class="n">normalized_embeddings</span><span class="p">,</span> <span class="n">valid_dataset</span><span class="p">)</span>
    <span class="n">similarity</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span>
      <span class="n">valid_embeddings</span><span class="p">,</span> <span class="n">normalized_embeddings</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c"># Add variable initializer.</span>
    <span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>


</code></pre>
</div>

<h2 id="step-3-start-training">Step 3. Start Training</h2>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">num_steps</span> <span class="o">=</span> <span class="mi">100001</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">graph</span><span class="p">)</span> <span class="k">as</span> <span class="n">session</span><span class="p">:</span>
    <span class="c"># We must initialize all variables before we use them.</span>
    <span class="n">init</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Initialized'</span><span class="p">)</span>

    <span class="n">average_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
        <span class="n">batch_inputs</span><span class="p">,</span> <span class="n">batch_labels</span> <span class="o">=</span> <span class="n">generate_batch</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_skips</span><span class="p">,</span> <span class="n">skip_window</span><span class="p">)</span>
        <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">train_inputs</span><span class="p">:</span> <span class="n">batch_inputs</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">:</span> <span class="n">batch_labels</span><span class="p">}</span>

        <span class="c"># We perform one update step by evaluating the optimizer op (including it</span>
        <span class="c"># in the list of returned values for session.run()</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">loss_val</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>
        <span class="n">average_loss</span> <span class="o">+=</span> <span class="n">loss_val</span>

        <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">2000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">step</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">average_loss</span> <span class="o">/=</span> <span class="mi">2000</span>
          <span class="c"># The average loss is an estimate of the loss over the last 2000 batches.</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'Average loss at step '</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="s">': '</span><span class="p">,</span> <span class="n">average_loss</span><span class="p">)</span>
            <span class="n">average_loss</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c"># Note that this is expensive (~20% slowdown if computed every 500 steps)</span>
        <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">sim</span> <span class="o">=</span> <span class="n">similarity</span><span class="o">.</span><span class="nb">eval</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">valid_size</span><span class="p">):</span>
                <span class="n">valid_word</span> <span class="o">=</span> <span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">valid_examples</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
                <span class="n">top_k</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c"># number of nearest neighbors</span>
                <span class="n">nearest</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">sim</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[</span><span class="mi">1</span><span class="p">:</span><span class="n">top_k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
                <span class="n">log_str</span> <span class="o">=</span> <span class="s">'Nearest to </span><span class="si">%</span><span class="s">s:'</span> <span class="o">%</span> <span class="n">valid_word</span>
                <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">top_k</span><span class="p">):</span>
                    <span class="n">close_word</span> <span class="o">=</span> <span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">nearest</span><span class="p">[</span><span class="n">k</span><span class="p">]]</span>
                    <span class="n">log_str</span> <span class="o">=</span> <span class="s">'</span><span class="si">%</span><span class="s">s </span><span class="si">%</span><span class="s">s,'</span> <span class="o">%</span> <span class="p">(</span><span class="n">log_str</span><span class="p">,</span> <span class="n">close_word</span><span class="p">)</span>
                <span class="k">print</span><span class="p">(</span><span class="n">log_str</span><span class="p">)</span>
    <span class="n">final_embeddings</span> <span class="o">=</span> <span class="n">normalized_embeddings</span><span class="o">.</span><span class="nb">eval</span><span class="p">()</span>

</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Initialized
Average loss at step  0 :  201.053588867
Nearest to The Hitchhiker's Guide to the Galaxy (Hitchhiker's Guide to the Galaxy, #1): Batman: The Dark Knight Returns (The Dark Knight Saga, #1), Selected Stories, If I Stay (If I Stay, #1),
Nearest to Cinder (The Lunar Chronicles, #1): Eleven Minutes, The Five Love Languages of Children, Austenland (Austenland, #1),
Nearest to The Kite Runner: The Rumor, Labyrinth (Languedoc, #1), The Sea of Trolls (Sea of Trolls, #1),
Nearest to The Glass Castle: The Witch's Daughter (The Witch's Daughter, #1), The Love Dare, Holding Up the Universe,
Nearest to Extremely Loud and Incredibly Close: Not My Daughter, The Buried Giant, The Andromeda Strain,
Nearest to Love in the Time of Cholera: The Woodlanders, The Ringworld Engineers (Ringworld #2), One Fish, Two Fish, Red Fish, Blue Fish,
Nearest to The Catcher in the Rye: Ancillary Sword (Imperial Radch, #2), I Shall Wear Midnight (Discworld, #38; Tiffany Aching, #4), The Sands of Time,
Nearest to Crime and Punishment: Dairy Queen (Dairy Queen, #1), Among Others, Millennium Snow, Vol. 1,
Nearest to Animal Farm: How to Save a Life, Officer Buckle &amp; Gloria, The Perfect Storm: A True Story of Men Against the Sea,
Nearest to Wicked: The Life and Times of the Wicked Witch of the West (The Wicked Years, #1): My Life, Salem Falls, The Ironwood Tree (The Spiderwick Chronicles, #4),
Nearest to The Perks of Being a Wallflower: The Dragonslayer (Bone, #4), Deliverance, Rendezvous with Rama (Rama, #1),
Nearest to War and Peace: East, The Elvenbane (Halfblood Chronicles, #1), Medium Raw: A Bloody Valentine to the World of Food and the People Who Cook,
Nearest to Brave New World: On Mystic Lake, Fledgling, 77 Shadow Street (Pendleton, #1),
Nearest to The Girl with the Dragon Tattoo (Millennium, #1): The Secret of the Unicorn (Tintin, #11), Player Piano, Angels Fall,
Nearest to The Casual Vacancy: The Prayer of Jabez:  Breaking Through to the Blessed Life, The Weekenders, The Assassin's Blade (Throne of Glass, #0.1-0.5),
Nearest to Lord of the Flies: Cloudy With a Chance of Meatballs, First Frost (Waverley Family, #2), Passing,
Average loss at step  2000 :  50.8570902784
Average loss at step  4000 :  8.48856670332
Average loss at step  6000 :  5.50654278445
Average loss at step  8000 :  4.89954230261
Average loss at step  10000 :  4.66325466359
Nearest to The Hitchhiker's Guide to the Galaxy (Hitchhiker's Guide to the Galaxy, #1): The Notebook (The Notebook, #1), Me Talk Pretty One Day, 20th Century Ghosts,
Nearest to Cinder (The Lunar Chronicles, #1): The Five Love Languages of Children, Eleven Minutes, Bully (Fall Away, #1),
Nearest to The Kite Runner: Tuesdays with Morrie, Catch-22, The Omnivore's Dilemma: A Natural History of Four Meals,
Nearest to The Glass Castle: The Witch's Daughter (The Witch's Daughter, #1), In Cold Blood, Hot, Flat, and Crowded: Why We Need a Green Revolution--and How It Can Renew America,
Nearest to Extremely Loud and Incredibly Close: The Poisonwood Bible, The Buried Giant, The Andromeda Strain,
Nearest to Love in the Time of Cholera: One Fish, Two Fish, Red Fish, Blue Fish, Little Women (Little Women, #1), The Red Garden,
Nearest to The Catcher in the Rye: Ancillary Sword (Imperial Radch, #2), The Personal MBA: Master the Art of Business, I Shall Wear Midnight (Discworld, #38; Tiffany Aching, #4),
Nearest to Crime and Punishment: Dairy Queen (Dairy Queen, #1), Catch-22, The Great Gatsby,
Nearest to Animal Farm: How to Save a Life, Infinite Jest, The Perfect Storm: A True Story of Men Against the Sea,
Nearest to Wicked: The Life and Times of the Wicked Witch of the West (The Wicked Years, #1): My Life, Atonement, Orphan X (Orphan X, #1),
Nearest to The Perks of Being a Wallflower: Catch-22, Rendezvous with Rama (Rama, #1), The Dragonslayer (Bone, #4),
Nearest to War and Peace: Everything, Everything, Medium Raw: A Bloody Valentine to the World of Food and the People Who Cook, East,
Nearest to Brave New World: Fledgling, Outlander (Outlander, #1), Tuesdays with Morrie,
Nearest to The Girl with the Dragon Tattoo (Millennium, #1): The Secret of the Unicorn (Tintin, #11), Dewey: The Small-Town Library Cat Who Touched the World, Story of O (Story of O #1),
Nearest to The Casual Vacancy: The Weekenders, The Prayer of Jabez:  Breaking Through to the Blessed Life, Life and Other Near-Death Experiences,
Nearest to Lord of the Flies: Passing, Snow, A Short History of Nearly Everything,
Average loss at step  12000 :  4.49367789495
Average loss at step  14000 :  4.3219834007
Average loss at step  16000 :  4.2673389703
Average loss at step  18000 :  4.09554072869
Average loss at step  20000 :  4.04103688669
Nearest to The Hitchhiker's Guide to the Galaxy (Hitchhiker's Guide to the Galaxy, #1): The Notebook (The Notebook, #1), Me Talk Pretty One Day, The Iliad,
Nearest to Cinder (The Lunar Chronicles, #1): The Five Love Languages of Children, Delirium (Delirium, #1), The Fault in Our Stars,
Nearest to The Kite Runner: Tuesdays with Morrie, The Omnivore's Dilemma: A Natural History of Four Meals, The House of the Spirits,
Nearest to The Glass Castle: In Cold Blood, The Witch's Daughter (The Witch's Daughter, #1), The Bonesetter's Daughter,
Nearest to Extremely Loud and Incredibly Close: The Poisonwood Bible, The Widow of the South, The Memory Keeper's Daughter,
Nearest to Love in the Time of Cholera: I Know Why the Caged Bird Sings, One Fish, Two Fish, Red Fish, Blue Fish, The Underground Railroad,
Nearest to The Catcher in the Rye: Ancillary Sword (Imperial Radch, #2), Democracy in America , The Personal MBA: Master the Art of Business,
Nearest to Crime and Punishment: Dairy Queen (Dairy Queen, #1), Zen and the Art of Motorcycle Maintenance: An Inquiry Into Values, The Great Gatsby,
Nearest to Animal Farm: Infinite Jest, Julie and Julia: 365 Days, 524 Recipes, 1 Tiny Apartment Kitchen: How One Girl Risked Her Marriage, Her Job, and Her Sanity to Master the Art of Living, How to Save a Life,
Nearest to Wicked: The Life and Times of the Wicked Witch of the West (The Wicked Years, #1): Atonement, I, Claudius (Claudius, #1), My Life,
Nearest to The Perks of Being a Wallflower: Catch-22, Lolita, Rendezvous with Rama (Rama, #1),
Nearest to War and Peace: Norwegian Wood, Everything, Everything, Medium Raw: A Bloody Valentine to the World of Food and the People Who Cook,
Nearest to Brave New World: Tuesdays with Morrie, Blue Like Jazz: Nonreligious Thoughts on Christian Spirituality, Outlander (Outlander, #1),
Nearest to The Girl with the Dragon Tattoo (Millennium, #1): The Secret of the Unicorn (Tintin, #11), Dewey: The Small-Town Library Cat Who Touched the World, The Housekeeper and the Professor,
Nearest to The Casual Vacancy: The Fault in Our Stars, The Weekenders, The Prayer of Jabez:  Breaking Through to the Blessed Life,
Nearest to Lord of the Flies: Passing, Snow, A Short History of Nearly Everything,
Average loss at step  22000 :  3.99072129893
Average loss at step  24000 :  3.90868891752
Average loss at step  26000 :  3.8537153455
Average loss at step  28000 :  3.77176816392
Average loss at step  30000 :  3.82985007036
Nearest to The Hitchhiker's Guide to the Galaxy (Hitchhiker's Guide to the Galaxy, #1): The Notebook (The Notebook, #1), Me Talk Pretty One Day, The Iliad,
Nearest to Cinder (The Lunar Chronicles, #1): The Fault in Our Stars, Delirium (Delirium, #1), Ruby Red (Precious Stone Trilogy, #1),
Nearest to The Kite Runner: Tuesdays with Morrie, The House of the Spirits, The Omnivore's Dilemma: A Natural History of Four Meals,
Nearest to The Glass Castle: The Bonesetter's Daughter, The Other Boleyn Girl (The Plantagenet and Tudor Novels, #9), In Cold Blood,
Nearest to Extremely Loud and Incredibly Close: The Poisonwood Bible, The Widow of the South, The Memory Keeper's Daughter,
Nearest to Love in the Time of Cholera: I Know Why the Caged Bird Sings, Atlas Shrugged, The Underground Railroad,
Nearest to The Catcher in the Rye: Democracy in America , Ancillary Sword (Imperial Radch, #2), Everything We Keep (Everything We Keep, #1),
Nearest to Crime and Punishment: Zen and the Art of Motorcycle Maintenance: An Inquiry Into Values, Slaughterhouse-Five, The Great Gatsby,
Nearest to Animal Farm: The Good Earth (House of Earth, #1), Infinite Jest, Julie and Julia: 365 Days, 524 Recipes, 1 Tiny Apartment Kitchen: How One Girl Risked Her Marriage, Her Job, and Her Sanity to Master the Art of Living,
Nearest to Wicked: The Life and Times of the Wicked Witch of the West (The Wicked Years, #1): Atonement, I, Claudius (Claudius, #1), Ender's Game (Ender's Saga, #1),
Nearest to The Perks of Being a Wallflower: Catch-22, To Kill a Mockingbird, The Garden of Eden,
Nearest to War and Peace: Norwegian Wood, Everything, Everything, Jonathan Strange &amp; Mr Norrell,
Nearest to Brave New World: Outlander (Outlander, #1), Tuesdays with Morrie, Blue Like Jazz: Nonreligious Thoughts on Christian Spirituality,
Nearest to The Girl with the Dragon Tattoo (Millennium, #1): The Secret of the Unicorn (Tintin, #11), Dewey: The Small-Town Library Cat Who Touched the World, The Housekeeper and the Professor,
Nearest to The Casual Vacancy: The Fault in Our Stars, The Prayer of Jabez:  Breaking Through to the Blessed Life, Wild: From Lost to Found on the Pacific Crest Trail,
Nearest to Lord of the Flies: Snow, Passing, A Short History of Nearly Everything,
Average loss at step  32000 :  3.72429895306
Average loss at step  34000 :  3.67616319287
Average loss at step  36000 :  3.65157854366
Average loss at step  38000 :  3.60568102384
Average loss at step  40000 :  3.5629105919
Nearest to The Hitchhiker's Guide to the Galaxy (Hitchhiker's Guide to the Galaxy, #1): The Notebook (The Notebook, #1), Me Talk Pretty One Day, A Streetcar Named Desire,
Nearest to Cinder (The Lunar Chronicles, #1): The Fault in Our Stars, Embrace (The Violet Eden Chapters, #1), Seven Deadly Wonders (Jack West Jr, #1),
Nearest to The Kite Runner: Tuesdays with Morrie, The Omnivore's Dilemma: A Natural History of Four Meals, The House of the Spirits,
Nearest to The Glass Castle: The Bonesetter's Daughter, The Other Boleyn Girl (The Plantagenet and Tudor Novels, #9), Peony in Love,
Nearest to Extremely Loud and Incredibly Close: The Poisonwood Bible, The Widow of the South, The Memory Keeper's Daughter,
Nearest to Love in the Time of Cholera: I Know Why the Caged Bird Sings, Embrace the Night (Cassandra Palmer, #3), The Grapes of Wrath,
Nearest to The Catcher in the Rye: Democracy in America , Ancillary Sword (Imperial Radch, #2), War and Peace,
Nearest to Crime and Punishment: Zen and the Art of Motorcycle Maintenance: An Inquiry Into Values, Slaughterhouse-Five, To Have and Have Not,
Nearest to Animal Farm: The Good Earth (House of Earth, #1), Infinite Jest, Julie and Julia: 365 Days, 524 Recipes, 1 Tiny Apartment Kitchen: How One Girl Risked Her Marriage, Her Job, and Her Sanity to Master the Art of Living,
Nearest to Wicked: The Life and Times of the Wicked Witch of the West (The Wicked Years, #1): Ender's Game (Ender's Saga, #1), Atonement, I, Claudius (Claudius, #1),
Nearest to The Perks of Being a Wallflower: Catch-22, To Kill a Mockingbird, The Garden of Eden,
Nearest to War and Peace: Norwegian Wood, Jonathan Strange &amp; Mr Norrell, Nine Stories,
Nearest to Brave New World: Outlander (Outlander, #1), Blue Like Jazz: Nonreligious Thoughts on Christian Spirituality, Tuesdays with Morrie,
Nearest to The Girl with the Dragon Tattoo (Millennium, #1): The Secret of the Unicorn (Tintin, #11), The Housekeeper and the Professor, Dewey: The Small-Town Library Cat Who Touched the World,
Nearest to The Casual Vacancy: The Fault in Our Stars, The Prayer of Jabez:  Breaking Through to the Blessed Life, Wild: From Lost to Found on the Pacific Crest Trail,
Nearest to Lord of the Flies: Snow, Harry Potter and the Deathly Hallows (Harry Potter, #7), A Short History of Nearly Everything,
Average loss at step  42000 :  3.5407050733
Average loss at step  44000 :  3.60401493472
Average loss at step  46000 :  3.55708793867
Average loss at step  48000 :  3.49812850416
Average loss at step  50000 :  3.46880627728
Nearest to The Hitchhiker's Guide to the Galaxy (Hitchhiker's Guide to the Galaxy, #1): The Notebook (The Notebook, #1), The Plot Against America, Me Talk Pretty One Day,
Nearest to Cinder (The Lunar Chronicles, #1): The Fault in Our Stars, Seven Deadly Wonders (Jack West Jr, #1), Embrace (The Violet Eden Chapters, #1),
Nearest to The Kite Runner: Tuesdays with Morrie, A Clockwork Orange, The Omnivore's Dilemma: A Natural History of Four Meals,
Nearest to The Glass Castle: The Bonesetter's Daughter, Peony in Love, In Cold Blood,
Nearest to Extremely Loud and Incredibly Close: The Poisonwood Bible, The Widow of the South, Lolita,
Nearest to Love in the Time of Cholera: I Know Why the Caged Bird Sings, The Catcher in the Rye, The Grapes of Wrath,
Nearest to The Catcher in the Rye: Democracy in America , Ancillary Sword (Imperial Radch, #2), Love in the Time of Cholera,
Nearest to Crime and Punishment: Zen and the Art of Motorcycle Maintenance: An Inquiry Into Values, Slaughterhouse-Five, To Have and Have Not,
Nearest to Animal Farm: The Good Earth (House of Earth, #1), Infinite Jest, Gone with the Wind,
Nearest to Wicked: The Life and Times of the Wicked Witch of the West (The Wicked Years, #1): Ender's Game (Ender's Saga, #1), Atonement, I, Claudius (Claudius, #1),
Nearest to The Perks of Being a Wallflower: Catch-22, To Kill a Mockingbird, The Garden of Eden,
Nearest to War and Peace: Norwegian Wood, Nine Stories, The God Delusion,
Nearest to Brave New World: The Giver (The Giver, #1), Tuesdays with Morrie, The Old Man and the Sea,
Nearest to The Girl with the Dragon Tattoo (Millennium, #1): The Secret of the Unicorn (Tintin, #11), The Housekeeper and the Professor, Dewey: The Small-Town Library Cat Who Touched the World,
Nearest to The Casual Vacancy: The Fault in Our Stars, Wild: From Lost to Found on the Pacific Crest Trail, The Prayer of Jabez:  Breaking Through to the Blessed Life,
Nearest to Lord of the Flies: Snow, Harry Potter and the Deathly Hallows (Harry Potter, #7), The Kite Runner,
Average loss at step  52000 :  3.43265928054
Average loss at step  54000 :  3.42167698848
Average loss at step  56000 :  3.39775447059
Average loss at step  58000 :  3.47592809737
Average loss at step  60000 :  3.47204799223
Nearest to The Hitchhiker's Guide to the Galaxy (Hitchhiker's Guide to the Galaxy, #1): Middlesex, A Clockwork Orange, The Notebook (The Notebook, #1),
Nearest to Cinder (The Lunar Chronicles, #1): The Fault in Our Stars, Embrace (The Violet Eden Chapters, #1), Seven Deadly Wonders (Jack West Jr, #1),
Nearest to The Kite Runner: Tuesdays with Morrie, A Clockwork Orange, 1984,
Nearest to The Glass Castle: The Bonesetter's Daughter, Peony in Love, In Cold Blood,
Nearest to Extremely Loud and Incredibly Close: The Poisonwood Bible, The Widow of the South, Lolita,
Nearest to Love in the Time of Cholera: I Know Why the Caged Bird Sings, The Divine Comedy, War and Peace,
Nearest to The Catcher in the Rye: Ancillary Sword (Imperial Radch, #2), Love in the Time of Cholera, Democracy in America ,
Nearest to Crime and Punishment: Slaughterhouse-Five, To Have and Have Not, Zen and the Art of Motorcycle Maintenance: An Inquiry Into Values,
Nearest to Animal Farm: The Good Earth (House of Earth, #1), Gone with the Wind, Rabbit, Run (Rabbit Angstrom #1),
Nearest to Wicked: The Life and Times of the Wicked Witch of the West (The Wicked Years, #1): Ender's Game (Ender's Saga, #1), Atonement, 1984,
Nearest to The Perks of Being a Wallflower: Catch-22, To Kill a Mockingbird, The Golden Compass (His Dark Materials, #1),
Nearest to War and Peace: Nine Stories, Jonathan Strange &amp; Mr Norrell, Harry Potter and the Deathly Hallows (Harry Potter, #7),
Nearest to Brave New World: Middlesex, The Giver (The Giver, #1), Catch-22,
Nearest to The Girl with the Dragon Tattoo (Millennium, #1): The Secret of the Unicorn (Tintin, #11), The Housekeeper and the Professor, Dewey: The Small-Town Library Cat Who Touched the World,
Nearest to The Casual Vacancy: The Fault in Our Stars, Wild: From Lost to Found on the Pacific Crest Trail, The Prayer of Jabez:  Breaking Through to the Blessed Life,
Nearest to Lord of the Flies: Snow, The Kite Runner, Harry Potter and the Deathly Hallows (Harry Potter, #7),
Average loss at step  62000 :  3.3924072181
Average loss at step  64000 :  3.36924781489
Average loss at step  66000 :  3.33055694771
Average loss at step  68000 :  3.31733061743
Average loss at step  70000 :  3.31788296509
Nearest to The Hitchhiker's Guide to the Galaxy (Hitchhiker's Guide to the Galaxy, #1): A Clockwork Orange, Catch-22, Middlesex,
Nearest to Cinder (The Lunar Chronicles, #1): Seven Deadly Wonders (Jack West Jr, #1), Embrace (The Violet Eden Chapters, #1), The Fault in Our Stars,
Nearest to The Kite Runner: Tuesdays with Morrie, A Clockwork Orange, 1984,
Nearest to The Glass Castle: Peony in Love, The Bonesetter's Daughter, The Other Boleyn Girl (The Plantagenet and Tudor Novels, #9),
Nearest to Extremely Loud and Incredibly Close: The Poisonwood Bible, Lolita, The Omnivore's Dilemma: A Natural History of Four Meals,
Nearest to Love in the Time of Cholera: The Catcher in the Rye, War and Peace, The Divine Comedy,
Nearest to The Catcher in the Rye: Love in the Time of Cholera, Pride and Prejudice, Ancillary Sword (Imperial Radch, #2),
Nearest to Crime and Punishment: Zen and the Art of Motorcycle Maintenance: An Inquiry Into Values, Slaughterhouse-Five, To Have and Have Not,
Nearest to Animal Farm: The Good Earth (House of Earth, #1), Gone with the Wind, Infinite Jest,
Nearest to Wicked: The Life and Times of the Wicked Witch of the West (The Wicked Years, #1): Ender's Game (Ender's Saga, #1), Atonement, The Hitchhiker's Guide to the Galaxy (Hitchhiker's Guide to the Galaxy, #1),
Nearest to The Perks of Being a Wallflower: Catch-22, To Kill a Mockingbird, The Curious Incident of the Dog in the Night-Time,
Nearest to War and Peace: Harry Potter and the Deathly Hallows (Harry Potter, #7), Jonathan Strange &amp; Mr Norrell, The God Delusion,
Nearest to Brave New World: Catch-22, Middlesex, To Kill a Mockingbird,
Nearest to The Girl with the Dragon Tattoo (Millennium, #1): The Secret of the Unicorn (Tintin, #11), The Housekeeper and the Professor, Dewey: The Small-Town Library Cat Who Touched the World,
Nearest to The Casual Vacancy: Wild: From Lost to Found on the Pacific Crest Trail, The Fault in Our Stars, The Prayer of Jabez:  Breaking Through to the Blessed Life,
Nearest to Lord of the Flies: Snow, Harry Potter and the Deathly Hallows (Harry Potter, #7), The Kite Runner,
Average loss at step  72000 :  3.37482465214
Average loss at step  74000 :  3.42257392645
Average loss at step  76000 :  3.31818997788
Average loss at step  78000 :  3.29870229846
Average loss at step  80000 :  3.26437604576
Nearest to The Hitchhiker's Guide to the Galaxy (Hitchhiker's Guide to the Galaxy, #1): A Clockwork Orange, Catch-22, Middlesex,
Nearest to Cinder (The Lunar Chronicles, #1): Seven Deadly Wonders (Jack West Jr, #1), The Fault in Our Stars, Embrace (The Violet Eden Chapters, #1),
Nearest to The Kite Runner: Tuesdays with Morrie, A Clockwork Orange, Life of Pi,
Nearest to The Glass Castle: The Bonesetter's Daughter, Peony in Love, The Other Boleyn Girl (The Plantagenet and Tudor Novels, #9),
Nearest to Extremely Loud and Incredibly Close: The Poisonwood Bible, Lolita, The Omnivore's Dilemma: A Natural History of Four Meals,
Nearest to Love in the Time of Cholera: The Catcher in the Rye, Pride and Prejudice, The Grapes of Wrath,
Nearest to The Catcher in the Rye: Love in the Time of Cholera, Pride and Prejudice, Ancillary Sword (Imperial Radch, #2),
Nearest to Crime and Punishment: Slaughterhouse-Five, Zen and the Art of Motorcycle Maintenance: An Inquiry Into Values, To Have and Have Not,
Nearest to Animal Farm: The Good Earth (House of Earth, #1), Gone with the Wind, Infinite Jest,
Nearest to Wicked: The Life and Times of the Wicked Witch of the West (The Wicked Years, #1): Ender's Game (Ender's Saga, #1), Atonement, The Secret Life of Bees,
Nearest to The Perks of Being a Wallflower: To Kill a Mockingbird, Catch-22, The Curious Incident of the Dog in the Night-Time,
Nearest to War and Peace: Harry Potter and the Deathly Hallows (Harry Potter, #7), Gravity's Rainbow, Nine Stories,
Nearest to Brave New World: The Giver (The Giver, #1), Middlesex, Catch-22,
Nearest to The Girl with the Dragon Tattoo (Millennium, #1): The Secret of the Unicorn (Tintin, #11), The Housekeeper and the Professor, Dewey: The Small-Town Library Cat Who Touched the World,
Nearest to The Casual Vacancy: The Fault in Our Stars, The Prayer of Jabez:  Breaking Through to the Blessed Life, Barefoot Contessa Back to Basics,
Nearest to Lord of the Flies: The Kite Runner, Snow, 1984,
Average loss at step  82000 :  3.25000136065
Average loss at step  84000 :  3.25605355531
Average loss at step  86000 :  3.29702677715
Average loss at step  88000 :  3.38729267067
Average loss at step  90000 :  3.27407476926
Nearest to The Hitchhiker's Guide to the Galaxy (Hitchhiker's Guide to the Galaxy, #1): A Clockwork Orange, Catch-22, Middlesex,
Nearest to Cinder (The Lunar Chronicles, #1): Seven Deadly Wonders (Jack West Jr, #1), Embrace (The Violet Eden Chapters, #1), The Fault in Our Stars,
Nearest to The Kite Runner: Tuesdays with Morrie, A Clockwork Orange, Lolita,
Nearest to The Glass Castle: The Bonesetter's Daughter, Peony in Love, A Tree Grows in Brooklyn,
Nearest to Extremely Loud and Incredibly Close: The Poisonwood Bible, Lolita, The Omnivore's Dilemma: A Natural History of Four Meals,
Nearest to Love in the Time of Cholera: Pride and Prejudice, The Catcher in the Rye, War and Peace,
Nearest to The Catcher in the Rye: Love in the Time of Cholera, Pride and Prejudice, The Divine Comedy,
Nearest to Crime and Punishment: Slaughterhouse-Five, To Have and Have Not, Zen and the Art of Motorcycle Maintenance: An Inquiry Into Values,
Nearest to Animal Farm: The Good Earth (House of Earth, #1), Gone with the Wind, Infinite Jest,
Nearest to Wicked: The Life and Times of the Wicked Witch of the West (The Wicked Years, #1): Ender's Game (Ender's Saga, #1), Atonement, The Hitchhiker's Guide to the Galaxy (Hitchhiker's Guide to the Galaxy, #1),
Nearest to The Perks of Being a Wallflower: Catch-22, To Kill a Mockingbird, The Curious Incident of the Dog in the Night-Time,
Nearest to War and Peace: Harry Potter and the Deathly Hallows (Harry Potter, #7), Gravity's Rainbow, Love in the Time of Cholera,
Nearest to Brave New World: Catch-22, The Giver (The Giver, #1), To Kill a Mockingbird,
Nearest to The Girl with the Dragon Tattoo (Millennium, #1): The Secret of the Unicorn (Tintin, #11), The Housekeeper and the Professor, Dewey: The Small-Town Library Cat Who Touched the World,
Nearest to The Casual Vacancy: Wonder, The Fault in Our Stars, The Prayer of Jabez:  Breaking Through to the Blessed Life,
Nearest to Lord of the Flies: The Kite Runner, 1984, Pilgrim at Tinker Creek,
Average loss at step  92000 :  3.24749968338
Average loss at step  94000 :  3.21547119522
Average loss at step  96000 :  3.20347243738
Average loss at step  98000 :  3.19260094315
Average loss at step  100000 :  3.22952575564
Nearest to The Hitchhiker's Guide to the Galaxy (Hitchhiker's Guide to the Galaxy, #1): A Clockwork Orange, Catch-22, Middlesex,
Nearest to Cinder (The Lunar Chronicles, #1): Seven Deadly Wonders (Jack West Jr, #1), The Fault in Our Stars, Embrace (The Violet Eden Chapters, #1),
Nearest to The Kite Runner: Tuesdays with Morrie, A Clockwork Orange, 1984,
Nearest to The Glass Castle: The Bonesetter's Daughter, Peony in Love, A Tree Grows in Brooklyn,
Nearest to Extremely Loud and Incredibly Close: The Poisonwood Bible, Lolita, The Omnivore's Dilemma: A Natural History of Four Meals,
Nearest to Love in the Time of Cholera: The Catcher in the Rye, Pride and Prejudice, The Great Gatsby,
Nearest to The Catcher in the Rye: Love in the Time of Cholera, Pride and Prejudice, War and Peace,
Nearest to Crime and Punishment: Slaughterhouse-Five, Zen and the Art of Motorcycle Maintenance: An Inquiry Into Values, To Have and Have Not,
Nearest to Animal Farm: The Good Earth (House of Earth, #1), Gone with the Wind, Infinite Jest,
Nearest to Wicked: The Life and Times of the Wicked Witch of the West (The Wicked Years, #1): Ender's Game (Ender's Saga, #1), The Hitchhiker's Guide to the Galaxy (Hitchhiker's Guide to the Galaxy, #1), Atonement,
Nearest to The Perks of Being a Wallflower: Catch-22, To Kill a Mockingbird, The Curious Incident of the Dog in the Night-Time,
Nearest to War and Peace: Harry Potter and the Deathly Hallows (Harry Potter, #7), Gravity's Rainbow, The Unbearable Lightness of Being,
Nearest to Brave New World: The Giver (The Giver, #1), Catch-22, The Old Man and the Sea,
Nearest to The Girl with the Dragon Tattoo (Millennium, #1): The Secret of the Unicorn (Tintin, #11), Dewey: The Small-Town Library Cat Who Touched the World, Love, Rosie,
Nearest to The Casual Vacancy: Barefoot Contessa Back to Basics, The Fault in Our Stars, Wild: From Lost to Found on the Pacific Crest Trail,
Nearest to Lord of the Flies: The Kite Runner, 1984, World War Z: An Oral History of the Zombie War,
</code></pre>
</div>

<h2 id="step-4-save-the-final-embeddings-for-recommendation">Step 4. Save the final embeddings for recommendation</h2>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">final_embeddings</span><span class="o">.</span><span class="n">shape</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>(9951, 128)
</code></pre>
</div>

<p>Code for recommendation based on cosine similarity  » https://howtoanalyse.github.io/recommender/Word2Vec-Gensim/</p>


		</div>
		<div class="tutorial-share">
			<a class="modalOpen no-smooth" href="#subscribe">
			<svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M20 4H4c-1.1 0-1.99.9-1.99 2L2 18c0 1.1.9 2 2 2h16c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 4l-8 5-8-5V6l8 5 8-5v2z"/><path d="M0 0h24v24H0z" fill="none"/></svg>
		
 Subscribe</a>
			<a class="modalOpen no-smooth" href="#share">
			<svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
				<path d="M0 0h24v24H0z" fill="none"/>
				<path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81 1.66 0 3-1.34 3-3s-1.34-3-3-3-3 1.34-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9c-1.66 0-3 1.34-3 3s1.34 3 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.16c-.05.21-.08.43-.08.65 0 1.61 1.31 2.92 2.92 2.92 1.61 0 2.92-1.31 2.92-2.92s-1.31-2.92-2.92-2.92z"/>
			</svg>
		
 Share</a>
		</div>
	</section>
</section>





	<div class="related">
		<h3>Related tutorials</h3>

		<ul class="listing">
	
		<li >
			<a href="/deep-learning-with-numpy/Deep-Neural-Network/">
				<div class="icon">
		<svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
			<path d="M6 2c-1.1 0-1.99.9-1.99 2L4 20c0 1.1.89 2 1.99 2H18c1.1 0 2-.9 2-2V8l-6-6H6zm7 7V3.5L18.5 9H13z"/>
			<path d="M0 0h24v24H0z" fill="none"/>
		</svg>
</div>
				<article>
					<div>
						<h3>Deeper neural network</h3>
						<p class="description">Implement a shallow neural network using NumPy</p>
					</div>
				</article>
			</a>
		</li>
	
		<li >
			<a href="/machine-learning/Numeric-feature-engineering/">
				<div class="icon"></div>
				<article>
					<div>
						<h3>Feature Engineering on Numeric features</h3>
						<p class="description">Feature Preprocessing and generation with respect to models</p>
					</div>
				</article>
			</a>
		</li>
	
		<li >
			<a href="/machine-learning/Missing-data-imputation/">
				<div class="icon">
		<svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
			<path d="M6 2c-1.1 0-1.99.9-1.99 2L4 20c0 1.1.89 2 1.99 2H18c1.1 0 2-.9 2-2V8l-6-6H6zm7 7V3.5L18.5 9H13z"/>
			<path d="M0 0h24v24H0z" fill="none"/>
		</svg>
</div>
				<article>
					<div>
						<h3>Feature Engineering Part II -- Missing Values</h3>
						<p class="description">Feature Preprocessing and generation with respect to models. Approaches to fill NA</p>
					</div>
				</article>
			</a>
		</li>
	
		<li >
			<a href="/machine-learning/Categorical-feature-engineering/">
				<div class="icon"></div>
				<article>
					<div>
						<h3>Feature Engineering on Categorical and Ordinal features"</h3>
						<p class="description">Feature Preprocessing and generation with respect to models</p>
					</div>
				</article>
			</a>
		</li>
	
		<li >
			<a href="/recommender/Word2Vec-Gensim/">
				<div class="icon">
		<svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
			<path d="M6 2c-1.1 0-1.99.9-1.99 2L4 20c0 1.1.89 2 1.99 2H18c1.1 0 2-.9 2-2V8l-6-6H6zm7 7V3.5L18.5 9H13z"/>
			<path d="M0 0h24v24H0z" fill="none"/>
		</svg>
</div>
				<article>
					<div>
						<h3>Build a Recommender Engine with Gensim</h3>
						<p class="description">Recoemmend to-read books using Gensim</p>
					</div>
				</article>
			</a>
		</li>
	
		<li >
			<a href="/top%204%25%20kaggle%20solution/PCA/">
				<div class="icon">
		<svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
			<path d="M6 2c-1.1 0-1.99.9-1.99 2L4 20c0 1.1.89 2 1.99 2H18c1.1 0 2-.9 2-2V8l-6-6H6zm7 7V3.5L18.5 9H13z"/>
			<path d="M0 0h24v24H0z" fill="none"/>
		</svg>
</div>
				<article>
					<div>
						<h3>PCA can do more than dimension reduction</h3>
						<p class="description">Apply pca on generated features</p>
					</div>
				</article>
			</a>
		</li>
	
		<li >
			<a href="/top%204%25%20kaggle%20solution/Model-Training/">
				<div class="icon">
		<svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
			<path d="M6 2c-1.1 0-1.99.9-1.99 2L4 20c0 1.1.89 2 1.99 2H18c1.1 0 2-.9 2-2V8l-6-6H6zm7 7V3.5L18.5 9H13z"/>
			<path d="M0 0h24v24H0z" fill="none"/>
		</svg>
</div>
				<article>
					<div>
						<h3>Stacking in Machine Learning</h3>
						<p class="description">How weak regressors beat builky ones</p>
					</div>
				</article>
			</a>
		</li>
	
		<li >
			<a href="/top%204%25%20kaggle%20solution/EDA/">
				<div class="icon">
		<svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
			<path d="M6 2c-1.1 0-1.99.9-1.99 2L4 20c0 1.1.89 2 1.99 2H18c1.1 0 2-.9 2-2V8l-6-6H6zm7 7V3.5L18.5 9H13z"/>
			<path d="M0 0h24v24H0z" fill="none"/>
		</svg>
</div>
				<article>
					<div>
						<h3>From Exploratory Analysis to Feature Engineering</h3>
						<p class="description">Generated hundres of features based on exploratory data analysis</p>
					</div>
				</article>
			</a>
		</li>
	
		<li >
			<a href="/machine-learning/bag-of-words/">
				<div class="icon">
		<svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
			<path d="M6 2c-1.1 0-1.99.9-1.99 2L4 20c0 1.1.89 2 1.99 2H18c1.1 0 2-.9 2-2V8l-6-6H6zm7 7V3.5L18.5 9H13z"/>
			<path d="M0 0h24v24H0z" fill="none"/>
		</svg>
</div>
				<article>
					<div>
						<h3>Bag of words</h3>
						<p class="description">Feature Extraction I -- Bag of words</p>
					</div>
				</article>
			</a>
		</li>
	
		<li >
			<a href="/deep-learning-with-numpy/Two-Layder-Neural-Network/">
				<div class="icon">
		<svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
			<path d="M6 2c-1.1 0-1.99.9-1.99 2L4 20c0 1.1.89 2 1.99 2H18c1.1 0 2-.9 2-2V8l-6-6H6zm7 7V3.5L18.5 9H13z"/>
			<path d="M0 0h24v24H0z" fill="none"/>
		</svg>
</div>
				<article>
					<div>
						<h3>Two layer neural network</h3>
						<p class="description">Implement a shallow neural network using NumPy</p>
					</div>
				</article>
			</a>
		</li>
	
</ul>
	</div>



	<div id="disqus_thread"></div>
	<script>
		var disqus_shortname = 'greendata-1';
		var disqus_config = function () {
			this.page.url = "/recommender/Word2Vec/";
			this.page.identifier = "/recommender/Word2Vec";
		};
		(function() {  // DON'T EDIT BELOW THIS LINE
			var d = document, s = d.createElement('script');
			s.src = '//' + disqus_shortname + '.disqus.com/embed.js';
			s.setAttribute('data-timestamp', +new Date());
			(d.head || d.body).appendChild(s);
		})();
	</script>
	<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>


<div id="subscribe" class="modalDialog">
	<div>
		<a title="Close" class="close">&times;</a>
		<form action="" method="post">
			<h2>Subscribe</h2>
			<p>The latest tutorials sent straight to your inbox.</p>
			<input type="email" name="EMAIL" placeholder="Email">
			<input type="submit" value="Sign up">
		</form>
	</div>
</div>

<div id="share" class="modalDialog sharing">
	<div>
		<a title="Close" class="close">&times;</a>
		<h2>Share</h2>
		<p>Share this tutorial with your community.</p>
		<ul>
			<li><a class="twitter" target="_blank" href="https://twitter.com/intent/tweet?url=%2Frecommender%2FWord2Vec%2F">
		<svg class="twitter" fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M22.46,6C21.69,6.35 20.86,6.58 20,6.69C20.88,6.16 21.56,5.32 21.88,4.31C21.05,4.81 20.13,5.16 19.16,5.36C18.37,4.5 17.26,4 16,4C13.65,4 11.73,5.92 11.73,8.29C11.73,8.63 11.77,8.96 11.84,9.27C8.28,9.09 5.11,7.38 3,4.79C2.63,5.42 2.42,6.16 2.42,6.94C2.42,8.43 3.17,9.75 4.33,10.5C3.62,10.5 2.96,10.3 2.38,10C2.38,10 2.38,10 2.38,10.03C2.38,12.11 3.86,13.85 5.82,14.24C5.46,14.34 5.08,14.39 4.69,14.39C4.42,14.39 4.15,14.36 3.89,14.31C4.43,16 6,17.26 7.89,17.29C6.43,18.45 4.58,19.13 2.56,19.13C2.22,19.13 1.88,19.11 1.54,19.07C3.44,20.29 5.7,21 8.12,21C16,21 20.33,14.46 20.33,8.79C20.33,8.6 20.33,8.42 20.32,8.23C21.16,7.63 21.88,6.87 22.46,6Z" /></svg>
	
 Twitter</a></li>
			<li><a class="facebook" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=%2Frecommender%2FWord2Vec%2F">
		<svg class="facebook" xmlns="http://www.w3.org/2000/svg" width="25" height="25" viewBox="15.8 15.8 25 25"><path d="M32.8 24.7h-3.2v-2.1c0-0.8 0.5-1 0.9-1s2.3 0 2.3 0v-3.5l-3.1 0c-3.5 0-4.3 2.6-4.3 4.3v2.3h-2v3.6h2c0 4.6 0 10.2 0 10.2h4.2c0 0 0-5.6 0-10.2h2.8L32.8 24.7z"/></svg>
	
 Facebook</a></li>
			<li><a class="google-plus" target="_blank" href="https://plus.google.com/share?url=%2Frecommender%2FWord2Vec%2F">
		<svg class="google-plus" fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M23,11H21V9H19V11H17V13H19V15H21V13H23M8,11V13.4H12C11.8,14.4 10.8,16.4 8,16.4C5.6,16.4 3.7,14.4 3.7,12C3.7,9.6 5.6,7.6 8,7.6C9.4,7.6 10.3,8.2 10.8,8.7L12.7,6.9C11.5,5.7 9.9,5 8,5C4.1,5 1,8.1 1,12C1,15.9 4.1,19 8,19C12,19 14.7,16.2 14.7,12.2C14.7,11.7 14.7,11.4 14.6,11H8Z" /></svg>
	
 Google+</a></li>
			<li><a class="hacker-news" target="_blank" href="http://news.ycombinator.com/submitlink?u=%2Frecommender%2FWord2Vec%2F&t=From%20Word2Vec%20to%20Item%20Recommendation">
		<svg class="hacker-news" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M18.8 3.3c-0.4 0.8-5.6 11.1-5.6 11.2 0 2 0 6.2 0 6.2 -0.1 0-2.2 0-2.3 0 0 0 0-5.9 0-6.2 0 0-5.5-10.9-5.6-11.1C5.3 3.3 5.3 3.3 5.3 3.3 5.3 3.3 5.3 3.3 5.3 3.3c0 0 0 0 0.1 0 0.9 0 1.7 0 2.6 0 0 0 0 0 0 0.1 0.1 0.1 4 8.3 4.1 8.3 0 0 4.2-8.3 4.3-8.4 0.8 0 1.6 0 2.4 0 0 0 0 0 0 0C18.8 3.3 18.8 3.3 18.8 3.3z"/></svg>
	
 Hacker News</a></li>
		</ul>
	</div>
</div>

<script src="/js/main.js"></script>

<script>
	var headings = document.querySelectorAll("h2[id]");

	console.log(headings);
	for (var i = 0; i < headings.length; i++) {
		var anchorLink = document.createElement("a");
		anchorLink.innerText = "#";
		anchorLink.href = "#" + headings[i].id;
		anchorLink.classList.add("header-link");

		headings[i].appendChild(anchorLink);
	}

	$(".modalOpen").click(function() {
		var id = $(this).attr("href");
		$(id).addClass("show");

		$(window).click(function() {
			$(".show").removeClass("show");
		});

		$(id + " > div").click(function(event){
			event.stopPropagation();
		});
		return false;
	});

	$(".close").click(function() {
		$(".show").removeClass("show");
		return false;
	});

	$(document).keyup(function(e) {
		if (e.keyCode == 27) {
			$(".show").removeClass("show");
		}
	});
</script>

			</div>
		</section>

		<footer>
	<div class="wrapper">
		<p class="edit-footer"><a class="editor-link btn" href="cloudcannon:collections/_data/footer.yml" class="btn" style="padding: 5px;"><strong>&#9998;</strong> Edit footer</a></p>
		<ul class="footer-links">
			
				<li><a target="_blank" href="https://facebook.com/" class="Facebook-icon">
					
						
		<svg class="facebook" fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M19,4V7H17A1,1 0 0,0 16,8V10H19V13H16V20H13V13H11V10H13V7.5C13,5.56 14.57,4 16.5,4M20,2H4A2,2 0 0,0 2,4V20A2,2 0 0,0 4,22H20A2,2 0 0,0 22,20V4C22,2.89 21.1,2 20,2Z" /></svg>
	

					
					</a></li>
			
				<li><a target="_blank" href="https://twitter.com/" class="Twitter-icon">
					
						
		<svg class="twitter" fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M22.46,6C21.69,6.35 20.86,6.58 20,6.69C20.88,6.16 21.56,5.32 21.88,4.31C21.05,4.81 20.13,5.16 19.16,5.36C18.37,4.5 17.26,4 16,4C13.65,4 11.73,5.92 11.73,8.29C11.73,8.63 11.77,8.96 11.84,9.27C8.28,9.09 5.11,7.38 3,4.79C2.63,5.42 2.42,6.16 2.42,6.94C2.42,8.43 3.17,9.75 4.33,10.5C3.62,10.5 2.96,10.3 2.38,10C2.38,10 2.38,10 2.38,10.03C2.38,12.11 3.86,13.85 5.82,14.24C5.46,14.34 5.08,14.39 4.69,14.39C4.42,14.39 4.15,14.36 3.89,14.31C4.43,16 6,17.26 7.89,17.29C6.43,18.45 4.58,19.13 2.56,19.13C2.22,19.13 1.88,19.11 1.54,19.07C3.44,20.29 5.7,21 8.12,21C16,21 20.33,14.46 20.33,8.79C20.33,8.6 20.33,8.42 20.32,8.23C21.16,7.63 21.88,6.87 22.46,6Z" /></svg>
	

					
					</a></li>
			
				<li><a target="_blank" href="https://youtube.com/" class="YouTube-icon">
					
						
		<svg class="youtube" fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M10,16.5V7.5L16,12M20,4.4C19.4,4.2 15.7,4 12,4C8.3,4 4.6,4.19 4,4.38C2.44,4.9 2,8.4 2,12C2,15.59 2.44,19.1 4,19.61C4.6,19.81 8.3,20 12,20C15.7,20 19.4,19.81 20,19.61C21.56,19.1 22,15.59 22,12C22,8.4 21.56,4.91 20,4.4Z" /></svg>
	

					
					</a></li>
			
				<li><a  href="/feed.xml" class="RSS-icon">
					
						
		<svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="none"/><circle cx="6.18" cy="17.82" r="2.18"/><path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"/></svg>
		

					
					</a></li>
			
		</ul>
		<p class="copyright">&copy; Sara 2018. All rights reserved.</p>
	</div>
</footer>
		<script>
			$(function() {
				$('a[href*=\\#]').not(".no-smooth").on('click', function(event){
					var el = $(this.hash);
					if (el.length > 0) {
						// event.preventDefault();
						$('html,body').animate({scrollTop:$(this.hash).offset().top - 50}, 500);
					}
				});

				$('svg').click(function() {
					$(this).parent('form').submit();
				});
			});

			document.getElementById("open-nav").addEventListener("click", function (event) {
				event.preventDefault();
				document.body.classList.toggle("nav-open");
			});
		</script>
		
	</body>
</html>
