<!DOCTYPE html>

<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<meta name="generator" content="Jekyll v3.4.3">

		<link rel="stylesheet" href="/css/screen.css">
		<link rel="apple-touch-icon" href="/apple-touch-icon.png">
		<link rel="icon" type="image/png" href="/touch-icon.png" sizes="192x192">
		<link rel="icon" type="image/png" href="/images/favicon.png">
		<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Merriweather:400italic,400,300italic,300,700,700italic|Open+Sans:400italic,600italic,700italic,700,600,400|Inconsolata:400,700">
		<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>

		<!-- Begin Jekyll SEO tag v2.1.0 -->
<title>Search - Base</title>
<meta property="og:title" content="Search" />
<meta name="description" content="Knowledge base template for Jekyll." />
<meta property="og:description" content="Knowledge base template for Jekyll." />
<link rel="canonical" href="https://orange-ape.cloudvent.net//search/" />
<meta property="og:url" content="https://orange-ape.cloudvent.net//search/" />
<meta property="og:site_name" content="Base" />
<script type="application/ld+json">
{"@context": "http://schema.org",
"@type": "WebPage",
"headline": "Search",
"description": "Knowledge base template for Jekyll.",
"publisher": {"@type": "Organization",
"logo": {"@type": "ImageObject",
"url": "https://orange-ape.cloudvent.net//siteicon.png"}},
"url": "https://orange-ape.cloudvent.net//search/"}</script>
<!-- End Jekyll SEO tag -->

		<link type="application/atom+xml" rel="alternate" href="https://orange-ape.cloudvent.net//feed.xml" title="Base" />

		
	</head>

	<body class="">
		<header>
			<div class="wrapper">
				<section class="top-bar">
					<a class="nav-toggle" id="open-nav" href="#">&#9776;</a>
<nav>
	<a class="editor-link btn" href="cloudcannon:collections/_data/navigation.yml" class="btn" style="padding: 5px;"><strong>&#9998;</strong> Edit navigation</a>
	
	

		
		<a href="/" class="">Tutorials</a>
	
	

		
		<a href="/videos/" class="">Videos</a>
	
	

		
		<a href="/faq/" class="">FAQ</a>
	
</nav>

				</section>
				<section class="hero_search">
					<h1>Tutorials</h1>
					<p>Everything you need to know about running our software.</p>
					<form action="/search/" method="get">
	<input type="search" name="q"  placeholder="What would you like to know?" autofocus>
	<svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"/>
    <path d="M0 0h24v24H0z" fill="none"/>
</svg>
	<input type="submit" value="Search" style="display: none;">
</form>
				</section>
			</div>

		</header>
		<section class="content">
			<div class="wrapper">
				<p><span id="search-process">Loading</span> results <span id="search-query-container" style="display: none;">for "<strong id="search-query"></strong>"</span></p>
<ul id="search-results"></ul>

<script>
	window.data = {
		
			
				
					
					
					"machine-learning-numeric-feature-engineering": {
						"id": "machine-learning-numeric-feature-engineering",
						"title": "Feature Engineering on Numeric features",
						"categories": "Machine-Learning",
						"url": " /machine-learning/Numeric-feature-engineering/",
						"content": "Feature Preprocessing\n\nScaling\n\nScaling numeric features ensures that their initial impact on models are relatively the same.\n\nWinsorization or Rank Transformation to deal with Outliers\n\nOutliers are not only exist in features but our target as well.\n\nWinsorization\n\nTo protect linear models from outliers, we can clip features values between two chosen values of lower bound and upper bound. We can choose them as some percentiles of that feature. For example, first and 99s percentiles. This procedure of clipping is known as winsorization\n\nRank Transformation\n\nRank transformation sets spaces between properly assorted values to be equal. It can be a better choice than MinMaxScaler if we have outliers bacause it moves outliers closer to normal objects.\n\nThere are 2 choice while applying rank transformation to the test set:\n\n\n  store the creative mapping from features values to their rank values\n  concatenate train and test sets before applying the rank transformation.\n\n\nscipy.stats.rankdata\n\n\n\nLog transformation\n\nLog transformation acts as a representative of methametical transformations that often helps non-tree-based models and especially neural networks. As an alternative, you may extract a square root of the data.\n\nBoth of these transformations can:\n\n  drive extremely big values closer to the features’ average value.\n  make values near zero to be more distinguishable.\n\n\nDespite the simplicity, one of these transformations can improve your neural network’s results significantly.\n\nIn conclusion, linear models, KNN, and neural networks can benefit hugely from this.\n\nFeature Generation\n\nFeature generation is a process of deriving new features using logic or knowledge derived after data exploration and hypothesis checking.\n\nExamples:\n\n\n  Real Estate price and Real Estate squared area -&gt; price per meter square\n  Horizontal distance and the vertical difference in heights -&gt; direct distance.\n\n\nSuch examples can be of help not only for linear models. For example, although gradient within decision tree is a very powerful model, it still experiences difficulties with approximation of multiplications and divisions. And adding size features explicitly can lead to a more robust model with less amount of trees.\n\nApart from examples like adding, multiplications, divisions, and other features interactions, we can also:\n\n\n  A new feature indicating fractional part of these prices. For example, if some product costs 2.49, the fractional part of its price is 0.49. This feature can help the model utilize the differences in people’s perception of these prices.\n\n\nAlso, we can find similar patterns in tasks which require distinguishing between a human and a robot. For example, if we will have some kind of financial data like auctions, we could observe that people tend to set round numbers as prices, and there are something like 0.935, blah, blah,, blah, very long number here. Or, if we are trying to find spambots on social networks, we can be sure that no human ever read messages with an exact interval of one second."
					}
					
				
			
		
			
				
					,
					
					"machine-learning-missing-data-imputation": {
						"id": "machine-learning-missing-data-imputation",
						"title": "Feature Engineering Part II -- Missing Values",
						"categories": "Machine-Learning",
						"url": " /machine-learning/Missing-data-imputation/",
						"content": "Imputation\n\na. Replace it with a number outside the normal range like -999 \nb. Replace it with mean or median\nc. Reconstruct values\n\nMethod a is useful because it gives trees the possibility to take missing values into a separate category. The downside is that performance of linear models and neural networks will suffer.\n\nMethod b is beneficial for simple linear models and neural networks. But tree-based methods can be harder to select object with missing values in the first place.\n\nMethod c: Reconstruct values\n\nWhen data points are dependent to each other like time series data, we can approximate NAs using observations in the neighborhood.\n\nChallenges come when data points are independent.\n\nFeature Generation\n\nSometimes we create a binary feature, isnull, indicating which rows have missing values for this feature. Through this way, we can address concerns about trees and neural networks while computing mean or median. The drawback is that we will double the number of columns.\n\nThis method is especially worth trying for missing data occured in test set only. The intuition behind is that categories absent in the training set will be treated randomly eventually.\n\nAs an alternative, we may have a try on frequency encoding with missing values as a new category.\n\nXgboost can handle NaN."
					}
					
				
			
		
			
				
					,
					
					"machine-learning-categorical-feature-engineering": {
						"id": "machine-learning-categorical-feature-engineering",
						"title": "Feature Engineering on Categorical and Ordinal features&quot;",
						"categories": "Machine-Learning",
						"url": " /machine-learning/Categorical-feature-engineering/",
						"content": "Ordinal features refer to ordered categorical features. Examples include\n\n  Driver’s license: A,B,C,D\n  Education level: Bachelor, Master, Doctoral\n  …\n\n\nDifference between Numeric and Ordinal Features with values 1,2,3…\n\nFor numerical features with values 1,2,3…, we can conclude that the distance between first, and the second class is equal to the distance between second and the third class, but because for ordinal features, we can’t tell which distance is bigger.\n\nAs these numeric features, we can’t sort and integrate an ordinal feature the other way, and expect to get similar performance.\n\nLabel Encoding\n\nThe simplest way to encode a categorical feature is to map it’s unique values to different numbers.\n\nThere are different method. Be creative in constructing them\n\nFollowing are three examples:\n\n\n  Sorted alphabetical order\n    sklearn.preprocessing.LabelEncoding\n\n    \n  \n  Order of appearance\n    pandas.factorize\n\n    \n  \n\n\nFrequency Encoding\n\nencoding = df.groupby('col1').size()\nencoding = encoding / len(df)\ndf['enc']=df.col1.map(encoding)\n\n\n\nLabel encoding and frequency encoding work fine with tree-based models because models can split feature, and extract most of the useful values in categories on its own.\n\nBut linear models might get confused by non linear dependence of categorical features. The non-linear dependence refers to cases like:\n+——+————+———–+\n| id   | Cat       | target    |\n+——+————+———–+\n| 5    | 1 | 1       |\n| 6    | 2 | 0     |\n| 7    | 3 | 1       |\n| 8    | 4 | 1       |\n+——+————+———–+\n\nOne-hot Encoding\n\nOne-hot Encoding creates a new column for each unique value of our categorical feature and put one in the appropriate place. Everything else will be 0. This works well for linear models, k-NN and Neural Nets.\n\nSituations where we have a few important numeric features and hundreds of binary features generated by one-hot encoding may slow down tree-based models, not always improving their results.\n\nFeature Generation\n\nFeature interaction between several categorical features\n\nThis method works well with non-tree based models."
					}
					
				
			
		
			
				
					,
					
					"top-204-25-20kaggle-20solution-pca": {
						"id": "top-204-25-20kaggle-20solution-pca",
						"title": "PCA can do more than dimension reduction",
						"categories": "Top 4% Kaggle Solution",
						"url": " /top%204%25%20kaggle%20solution/PCA/",
						"content": "I was at first inspired by a discussion on Kaggle forum that doing PCA helps obtain a big boost on leaderboard.\n\nimport numpy as np\nfrom sklearn.preprocessing import RobustScaler, Imputer\nfrom sklearn.decomposition import PCA\n\n\n\n\npiped = np.load(\"piped.npy\")\ny_raw = np.load(\"y_raw.npy\")\n\n\n\nWhen it comes to data transformation, we should always do the transformation on training and test set separately. Because when you train a classifier, you cannot use any information from the test set.\n\ntrain_X = scaled_pipe[:len(y_raw)]\ntest_X = scaled_pipe[len(y_raw):]\nprint(\"Shape of training X: {}\".format(train_X.shape))\nprint(\"Shape of test X: {}\".format(test_X.shape))\n\n\n\nShape of training X: (1458, 427)\nShape of test X: (1459, 427)\n\n\n\nscaler = RobustScaler()\nscaled_train = scaler.fit(train_X).transform(train_X)\nscaled_test = scaler.transform(test_X)\n\n\n\n\nThe appropriate way to apply PCA is to run it on the training set, save the principal components that you use, and then use them to transform the points in your test set.\n\nfit_transform() and transform functions in sklearn simplified code for us.\n\npca = PCA(n_components=410)\nX_scaled=pca.fit_transform(scaled_train)\ntest_X_scaled = pca.transform(scaled_test)\n\n\n\ny_log = np.log(y_raw)\ny_log.shape\n\n\n\n(1458,)\n\n\n\nX = Imputer().fit_transform(scaled_train)\ny = Imputer().fit_transform(y_log.reshape(-1,1)).ravel()\n\n\n\nSave the processed data for model training\n\nnp.save(\"X\",X)\nnp.save(\"scaled_test\",scaled_test)\nnp.save(\"y\",y)"
					}
					
				
			
		
			
				
					,
					
					"top-204-25-20kaggle-20solution-model-training": {
						"id": "top-204-25-20kaggle-20solution-model-training",
						"title": "Stacking in Machine Learning",
						"categories": "Top 4% Kaggle Solution",
						"url": " /top%204%25%20kaggle%20solution/Model-Training/",
						"content": "There are methods that are common in machine learning projects like cross validation, grid searching and stacking.\n\nSo I wrapped them into a common module in helper.py.\n\nIn this script, I will call those functions directly. Feel free to have a check at my github for source code.\n\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor, BayesianRidge\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom xgboost import XGBRegressor\n\nimport numpy as np\nimport pandas as pd\n\n\n\n\n/Users/zxy/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n\n\n\nHere is a tip for building your own module.\n\nDuring training, it’s likely to do some modifications on the module file. We need to reload the module every time we have modified it.\n\nJust as following (“helpers” is the name of module I built):\n\nimport helpers\nfrom importlib import reload\nreload(helpers)\nfrom helpers import *\n\n\n\nLoad processed data\n\nI have documented details of my data processing pipeline. Feel free to have a check on my blog.\n\nX = np.load(\"X.npy\")\ny = np.load(\"y.npy\")\nscaled_test = np.load(\"scaled_test.npy\")\n\n\n\nmodels = [LinearRegression(),Ridge(),Lasso(alpha=0.01,max_iter=10000),RandomForestRegressor(),GradientBoostingRegressor(),SVR(),LinearSVR(),\n          ElasticNet(alpha=0.001,max_iter=10000),SGDRegressor(max_iter=1000,tol=1e-3),BayesianRidge(),KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5),\n          ExtraTreesRegressor(),XGBRegressor()]\n\nnames = [\"LR\", \"Ridge\", \"Lasso\", \"RF\", \"GBR\", \"SVR\", \"LinSVR\", \"Ela\",\"SGD\",\"Bay\",\"Ker\",\"Extra\",\"Xgb\"]\nfor name, model in zip(names, models):\n    score = rmse_cv(model, X, y)\n    print(\"{}: {:.6f}, {:.4f}\".format(name,score.mean(),score.std()))\n\n\n\n\n\nlasso_best = grid(Lasso()).grid_get(X,y,{'alpha': [0.0004,0.0005,0.0007,0.0006,0.0009,0.0008],'max_iter':[10000]})\n\n\n\n{'max_iter': 10000, 'alpha': 0.0005} 0.107202347338\n                                 params  mean_test_score  std_test_score\n0  {'max_iter': 10000, 'alpha': 0.0004}         0.107326        0.003200\n1  {'max_iter': 10000, 'alpha': 0.0005}         0.107202        0.003151\n2  {'max_iter': 10000, 'alpha': 0.0007}         0.107412        0.003104\n3  {'max_iter': 10000, 'alpha': 0.0006}         0.107268        0.003121\n4  {'max_iter': 10000, 'alpha': 0.0009}         0.107871        0.003099\n5  {'max_iter': 10000, 'alpha': 0.0008}         0.107619        0.003103\n\n\n\nridge_best=grid(Ridge()).grid_get(X,y,{'alpha':[35,40,45,50,55,60,65,70,80,90]})\n\n\n\n{'alpha': 35} 0.108637386149\n          params  mean_test_score  std_test_score\n0  {'alpha': 35}         0.108637        0.003011\n1  {'alpha': 40}         0.108655        0.002995\n2  {'alpha': 45}         0.108684        0.002982\n3  {'alpha': 50}         0.108719        0.002972\n4  {'alpha': 55}         0.108761        0.002964\n5  {'alpha': 60}         0.108806        0.002958\n6  {'alpha': 65}         0.108855        0.002953\n7  {'alpha': 70}         0.108906        0.002950\n8  {'alpha': 80}         0.109014        0.002945\n9  {'alpha': 90}         0.109128        0.002943\n\n\n\nsvr_best = grid(SVR()).grid_get(X,y,{'C':[11,12,13,14,15],'kernel':[\"rbf\"],\"gamma\":[0.0003,0.0004],\"epsilon\":[0.008,0.009]})\n\n\n\n\n{'gamma': 0.0004, 'epsilon': 0.008, 'C': 15, 'kernel': 'rbf'} 0.106137364476\n                                               params  mean_test_score  \\\n0   {'gamma': 0.0003, 'epsilon': 0.008, 'C': 11, '...         0.106940   \n1   {'gamma': 0.0004, 'epsilon': 0.008, 'C': 11, '...         0.106369   \n2   {'gamma': 0.0003, 'epsilon': 0.009, 'C': 11, '...         0.106881   \n3   {'gamma': 0.0004, 'epsilon': 0.009, 'C': 11, '...         0.106355   \n4   {'gamma': 0.0003, 'epsilon': 0.008, 'C': 12, '...         0.106825   \n5   {'gamma': 0.0004, 'epsilon': 0.008, 'C': 12, '...         0.106260   \n6   {'gamma': 0.0003, 'epsilon': 0.009, 'C': 12, '...         0.106781   \n7   {'gamma': 0.0004, 'epsilon': 0.009, 'C': 12, '...         0.106246   \n8   {'gamma': 0.0003, 'epsilon': 0.008, 'C': 13, '...         0.106715   \n9   {'gamma': 0.0004, 'epsilon': 0.008, 'C': 13, '...         0.106181   \n10  {'gamma': 0.0003, 'epsilon': 0.009, 'C': 13, '...         0.106698   \n11  {'gamma': 0.0004, 'epsilon': 0.009, 'C': 13, '...         0.106187   \n12  {'gamma': 0.0003, 'epsilon': 0.008, 'C': 14, '...         0.106623   \n13  {'gamma': 0.0004, 'epsilon': 0.008, 'C': 14, '...         0.106150   \n14  {'gamma': 0.0003, 'epsilon': 0.009, 'C': 14, '...         0.106607   \n15  {'gamma': 0.0004, 'epsilon': 0.009, 'C': 14, '...         0.106164   \n16  {'gamma': 0.0003, 'epsilon': 0.008, 'C': 15, '...         0.106535   \n17  {'gamma': 0.0004, 'epsilon': 0.008, 'C': 15, '...         0.106137   \n18  {'gamma': 0.0003, 'epsilon': 0.009, 'C': 15, '...         0.106500   \n19  {'gamma': 0.0004, 'epsilon': 0.009, 'C': 15, '...         0.106151   \n\n    std_test_score  \n0         0.003543  \n1         0.003544  \n2         0.003533  \n3         0.003535  \n4         0.003526  \n5         0.003554  \n6         0.003526  \n7         0.003545  \n8         0.003523  \n9         0.003563  \n10        0.003527  \n11        0.003558  \n12        0.003528  \n13        0.003564  \n14        0.003527  \n15        0.003560  \n16        0.003534  \n17        0.003562  \n18        0.003532  \n19        0.003562  \n\n\n\nparam_grid={'alpha':[0.2,0.3,0.4,0.5], 'kernel':[\"polynomial\"], 'degree':[3],'coef0':[0.8,1,1.2]}\nker_best = grid(KernelRidge()).grid_get(X,y,param_grid)\n\n\n\n\n\n{'coef0': 1.2, 'kernel': 'polynomial', 'alpha': 0.3, 'degree': 3} 0.10715301547\n                                               params  mean_test_score  \\\n0   {'coef0': 0.8, 'kernel': 'polynomial', 'alpha'...         0.108579   \n1   {'coef0': 1, 'kernel': 'polynomial', 'alpha': ...         0.107362   \n2   {'coef0': 1.2, 'kernel': 'polynomial', 'alpha'...         0.107167   \n3   {'coef0': 0.8, 'kernel': 'polynomial', 'alpha'...         0.110112   \n4   {'coef0': 1, 'kernel': 'polynomial', 'alpha': ...         0.107802   \n5   {'coef0': 1.2, 'kernel': 'polynomial', 'alpha'...         0.107153   \n6   {'coef0': 0.8, 'kernel': 'polynomial', 'alpha'...         0.111889   \n7   {'coef0': 1, 'kernel': 'polynomial', 'alpha': ...         0.108480   \n8   {'coef0': 1.2, 'kernel': 'polynomial', 'alpha'...         0.107397   \n9   {'coef0': 0.8, 'kernel': 'polynomial', 'alpha'...         0.113770   \n10  {'coef0': 1, 'kernel': 'polynomial', 'alpha': ...         0.109262   \n11  {'coef0': 1.2, 'kernel': 'polynomial', 'alpha'...         0.107748   \n\n    std_test_score  \n0         0.003201  \n1         0.003276  \n2         0.003350  \n3         0.003130  \n4         0.003183  \n5         0.003245  \n6         0.003097  \n7         0.003131  \n8         0.003180  \n9         0.003082  \n10        0.003099  \n11        0.003137  \n\n\n\nela_best = grid(ElasticNet()).grid_get(X,y,{'alpha':[0.0005,0.0008,0.004,0.005],'l1_ratio':[0.08,0.1,0.3,0.5,0.7],'max_iter':[10000]})\n\n\n\n{'max_iter': 10000, 'alpha': 0.0008, 'l1_ratio': 0.7} 0.107251756349\n                                               params  mean_test_score  \\\n0   {'max_iter': 10000, 'alpha': 0.0005, 'l1_ratio...         0.112391   \n1   {'max_iter': 10000, 'alpha': 0.0005, 'l1_ratio...         0.111869   \n2   {'max_iter': 10000, 'alpha': 0.0005, 'l1_ratio...         0.109277   \n3   {'max_iter': 10000, 'alpha': 0.0005, 'l1_ratio...         0.107973   \n4   {'max_iter': 10000, 'alpha': 0.0005, 'l1_ratio...         0.107442   \n5   {'max_iter': 10000, 'alpha': 0.0008, 'l1_ratio...         0.110606   \n6   {'max_iter': 10000, 'alpha': 0.0008, 'l1_ratio...         0.110087   \n7   {'max_iter': 10000, 'alpha': 0.0008, 'l1_ratio...         0.107988   \n8   {'max_iter': 10000, 'alpha': 0.0008, 'l1_ratio...         0.107347   \n9   {'max_iter': 10000, 'alpha': 0.0008, 'l1_ratio...         0.107252   \n10  {'max_iter': 10000, 'alpha': 0.004, 'l1_ratio'...         0.107578   \n11  {'max_iter': 10000, 'alpha': 0.004, 'l1_ratio'...         0.107517   \n12  {'max_iter': 10000, 'alpha': 0.004, 'l1_ratio'...         0.109248   \n13  {'max_iter': 10000, 'alpha': 0.004, 'l1_ratio'...         0.111574   \n14  {'max_iter': 10000, 'alpha': 0.004, 'l1_ratio'...         0.112626   \n15  {'max_iter': 10000, 'alpha': 0.005, 'l1_ratio'...         0.107584   \n16  {'max_iter': 10000, 'alpha': 0.005, 'l1_ratio'...         0.107669   \n17  {'max_iter': 10000, 'alpha': 0.005, 'l1_ratio'...         0.110322   \n18  {'max_iter': 10000, 'alpha': 0.005, 'l1_ratio'...         0.112269   \n19  {'max_iter': 10000, 'alpha': 0.005, 'l1_ratio'...         0.113654   \n\n    std_test_score  \n0         0.003620  \n1         0.003571  \n2         0.003351  \n3         0.003292  \n4         0.003221  \n5         0.003449  \n6         0.003396  \n7         0.003274  \n8         0.003195  \n9         0.003124  \n10        0.003153  \n11        0.003139  \n12        0.003104  \n13        0.003132  \n14        0.003148  \n15        0.003122  \n16        0.003103  \n17        0.003101  \n18        0.003143  \n19        0.003150  \n\n\n\nbay = BayesianRidge()\n\n\n\nStacking\n\nlasso = Lasso(alpha=0.0005,max_iter=10000)\nridge = Ridge(alpha=60)\nsvr = SVR(gamma= 0.0004,kernel='rbf',C=13,epsilon=0.009)\nker = KernelRidge(alpha=0.2 ,kernel='polynomial',degree=3 , coef0=0.8)\nela = ElasticNet(alpha=0.005,l1_ratio=0.08,max_iter=10000)\nbay = BayesianRidge()\n\n\n\n# stack_model = stacking(mod=[lasso_best,ridge_best,svr_best,ker_best,ela_best,bay],meta_model=ker_best)\nstack_model = stacking(mod=[lasso,ridge,svr,ker,ela,bay],meta_model=ker)\nprint(rmse_cv(stack_model,X,y))\n\n\n\n\n[ 0.10162302  0.10740187  0.11592729  0.09769622  0.10343146]\n\n\n\nX_train_stack, X_test_stack = stack_model.get_oof(X,y,scaled_test)\nX_train_add = np.hstack((X,X_train_stack))\nX_test_add = np.hstack((scaled_test,X_test_stack))\nprint(rmse_cv(stack_model,X_train_add,y))\n\n\n\n\n\n[ 0.09625364  0.10155954  0.11045188  0.09230963  0.09976153]\n\n\n\nFinally!\n\nstack_model.fit(X,y)\n\n\n\nstacking(meta_model=KernelRidge(alpha=0.2, coef0=0.8, degree=3, gamma=None, kernel='polynomial',\n      kernel_params=None),\n     mod=[Lasso(alpha=0.0005, copy_X=True, fit_intercept=True, max_iter=10000,\n   normalize=False, positive=False, precompute=False, random_state=None,\n   selection='cyclic', tol=0.0001, warm_start=False), Ridge(alpha=60, copy_X=True, fit_intercept=True, max_iter=None,\n   normalize=False, random_state=No...True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n       normalize=False, tol=0.001, verbose=False)])\n\n\n\npred = np.exp(stack_model.predict(scaled_test))\n\n\n\nsub=pd.read_csv(\"sample_submission.csv\")\nsub.tail(1)\n\n\n\n\n\n\n  \n    \n      \n      Id\n      SalePrice\n    \n  \n  \n    \n      1458\n      2919\n      187741.866657\n    \n  \n\n\n\nfrom time import gmtime, strftime\nsubmName = strftime(\"%Y%m%d%H%M%S\", gmtime()) + '_submission.csv'\nsub['SalePrice']=pred\nsub.to_csv(submName, index=False)"
					}
					
				
			
		
			
				
					,
					
					"top-204-25-20kaggle-20solution-eda": {
						"id": "top-204-25-20kaggle-20solution-eda",
						"title": "From Exploratory Analysis to Feature Engineering",
						"categories": "Top 4% Kaggle Solution",
						"url": " /top%204%25%20kaggle%20solution/EDA/",
						"content": "Following can be considered as a routinue for me every time I start doing exploratory analysis for a new machine learning project:\n\n  What is the problem I’m going to solve?\n  Know more about dependent variable (‘SalePrice’ in this case)\n  What data can I get as independent variabls and how are they related?\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom ggplot import *\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, LabelEncoder\nimport numpy as np\nfrom scipy.stats import skew\n\n\n\n\ntrain = pd.read_csv('train.csv')\ntest = pd.read_csv('test.csv')\n\n\n\ntrain.sample(5)\n\n\n\n\n\n\n  \n    \n      \n      Id\n      MSSubClass\n      MSZoning\n      LotFrontage\n      LotArea\n      Street\n      Alley\n      LotShape\n      LandContour\n      Utilities\n      ...\n      PoolArea\n      PoolQC\n      Fence\n      MiscFeature\n      MiscVal\n      MoSold\n      YrSold\n      SaleType\n      SaleCondition\n      SalePrice\n    \n  \n  \n    \n      1121\n      1122\n      20\n      RL\n      84.0\n      10084\n      Pave\n      NaN\n      Reg\n      Lvl\n      AllPub\n      ...\n      0\n      NaN\n      NaN\n      NaN\n      0\n      7\n      2006\n      New\n      Partial\n      212900\n    \n    \n      26\n      27\n      20\n      RL\n      60.0\n      7200\n      Pave\n      NaN\n      Reg\n      Lvl\n      AllPub\n      ...\n      0\n      NaN\n      NaN\n      NaN\n      0\n      5\n      2010\n      WD\n      Normal\n      134800\n    \n    \n      592\n      593\n      20\n      RL\n      60.0\n      6600\n      Pave\n      NaN\n      Reg\n      Lvl\n      AllPub\n      ...\n      0\n      NaN\n      NaN\n      NaN\n      0\n      6\n      2008\n      WD\n      Normal\n      138000\n    \n    \n      886\n      887\n      90\n      RL\n      70.0\n      8393\n      Pave\n      NaN\n      Reg\n      Lvl\n      AllPub\n      ...\n      0\n      NaN\n      NaN\n      NaN\n      0\n      6\n      2006\n      WD\n      Family\n      145000\n    \n    \n      275\n      276\n      50\n      RL\n      55.0\n      7264\n      Pave\n      NaN\n      Reg\n      Lvl\n      AllPub\n      ...\n      0\n      NaN\n      NaN\n      NaN\n      0\n      10\n      2009\n      WD\n      Normal\n      205000\n    \n  \n\n5 rows × 81 columns\n\n\ntrain.drop(train[(train[\"GrLivArea\"]&gt;4000)&amp;(train[\"SalePrice\"]&lt;300000)].index,inplace=True)\n\n\n\ny_raw = train['SalePrice']\nall_data = pd.concat([train, test],ignore_index=True)\nall_data.drop(['Id','SalePrice'],axis=1,inplace=True)\nall_cat = all_data.select_dtypes(include=[\"object\"])\nall_num = all_data.select_dtypes(exclude=[\"object\"])\n\nprint(\"Categorical variables: \\n{}\".format(\"\\n\".join(all_cat.columns)))\n\n\n\nCategorical variables: \nAlley\nBldgType\nBsmtCond\nBsmtExposure\nBsmtFinType1\nBsmtFinType2\nBsmtQual\nCentralAir\nCondition1\nCondition2\nElectrical\nExterCond\nExterQual\nExterior1st\nExterior2nd\nFence\nFireplaceQu\nFoundation\nFunctional\nGarageCond\nGarageFinish\nGarageQual\nGarageType\nHeating\nHeatingQC\nHouseStyle\nKitchenQual\nLandContour\nLandSlope\nLotConfig\nLotShape\nMSZoning\nMasVnrType\nMiscFeature\nNeighborhood\nPavedDrive\nPoolQC\nRoofMatl\nRoofStyle\nSaleCondition\nSaleType\nStreet\nUtilities\n\n\n\nprint(\"Numerical variables: \\n{}\".format(\"\\n\".join(all_num.columns)))\n\n\n\nNumerical variables: \n1stFlrSF\n2ndFlrSF\n3SsnPorch\nBedroomAbvGr\nBsmtFinSF1\nBsmtFinSF2\nBsmtFullBath\nBsmtHalfBath\nBsmtUnfSF\nEnclosedPorch\nFireplaces\nFullBath\nGarageArea\nGarageCars\nGarageYrBlt\nGrLivArea\nHalfBath\nKitchenAbvGr\nLotArea\nLotFrontage\nLowQualFinSF\nMSSubClass\nMasVnrArea\nMiscVal\nMoSold\nOpenPorchSF\nOverallCond\nOverallQual\nPoolArea\nScreenPorch\nTotRmsAbvGrd\nTotalBsmtSF\nWoodDeckSF\nYearBuilt\nYearRemodAdd\nYrSold\n\n\n\nmissed = all_data.isnull().sum()\nmissed[missed&gt;0].sort_values(ascending=False)\n\n\n\nPoolQC          2908\nMiscFeature     2812\nAlley           2719\nFence           2346\nFireplaceQu     1420\nLotFrontage      486\nGarageQual       159\nGarageCond       159\nGarageFinish     159\nGarageYrBlt      159\nGarageType       157\nBsmtExposure      82\nBsmtCond          82\nBsmtQual          81\nBsmtFinType2      80\nBsmtFinType1      79\nMasVnrType        24\nMasVnrArea        23\nMSZoning           4\nBsmtFullBath       2\nBsmtHalfBath       2\nUtilities          2\nFunctional         2\nElectrical         1\nBsmtUnfSF          1\nExterior1st        1\nExterior2nd        1\nTotalBsmtSF        1\nGarageArea         1\nGarageCars         1\nBsmtFinSF2         1\nBsmtFinSF1         1\nKitchenQual        1\nSaleType           1\ndtype: int64\n\n\n\nThere are many missing values in ‘PoolQC’, let’s have a check if it is related to ‘PoolArea’\n\nall_data['PoolArea'].describe()\n\n\n\ncount    2917.000000\nmean        2.088790\nstd        34.561371\nmin         0.000000\n25%         0.000000\n50%         0.000000\n75%         0.000000\nmax       800.000000\nName: PoolArea, dtype: float64\n\n\n\nall_data[all_data['PoolArea']==0]['PoolQC'].isnull().sum()\n\n\n\n2905\n\n\n\nall_data.loc[(all_data['PoolArea']!=0) &amp; (all_data['PoolQC'].isnull()),['PoolArea']]\n\n\n\n\n\n\n  \n    \n      \n      PoolArea\n    \n  \n  \n    \n      2418\n      368\n    \n    \n      2501\n      444\n    \n    \n      2597\n      561\n    \n  \n\n\n\nall_data['PoolQC'].value_counts()\n\n\n\nEx    4\nGd    3\nFa    2\nName: PoolQC, dtype: int64\n\n\n\nSeems that most of the missing values in ‘PoolQC’ come from houses without a pool. Let’s fill in the missing values here.\n\nall_data.loc[(all_data['PoolArea']==0) &amp; (all_data['PoolQC'].isnull()),['PoolQC']]='NA'\n\n\n\nMissing values in ‘LotFrontage’ can be imputed based on LotArea and Neighborhood\n\nall_data[\"LotAreaCut\"] = pd.qcut(all_data.LotArea,10)\nall_data['LotFrontage']=all_data.groupby(['LotAreaCut','Neighborhood'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))\nall_data['LotFrontage'].isnull().sum()\n\n\n\n\n9\n\n\n\nFor those with missing values in either LotArea or Neighborhood, we use LotAreaCut only.\n\nall_data['LotFrontage']=all_data.groupby(['LotAreaCut'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n\n\nall_data['LotFrontage'].isnull().sum()\n\n\n\n0\n\n\n\nfor col in missed[missed&lt;20].index:\n    all_data[col].fillna(all_data[col].mode()[0],inplace=True)\n\n\n\nfor col in all_num.columns:\n    all_data[col].fillna(0,inplace=True)\n\n\n\nfor col in all_cat.columns:\n    all_data[col].fillna(\"None\",inplace=True)\n\n\n\nsum(all_data.isnull().sum()&gt;0)\n\n\n\n0\n\n\n\nNumStr = [\"MSSubClass\",\"BsmtFullBath\",\"BsmtHalfBath\",\"HalfBath\",\"BedroomAbvGr\",\"KitchenAbvGr\",\"MoSold\",\"YrSold\",\"YearBuilt\",\"YearRemodAdd\",\"LowQualFinSF\",\"GarageYrBlt\"]\nfor col in NumStr:\n    all_data[col]=all_data[col].astype(str)\n\ndef map_values():\n    all_data[\"oMSSubClass\"] = all_data.MSSubClass.map({'180':1, \n                                        '30':2, '45':2, \n                                        '190':3, '50':3, '90':3, \n                                        '85':4, '40':4, '160':4, \n                                        '70':5, '20':5, '75':5, '80':5, '150':5,\n                                        '120': 6, '60':6})\n    \n    all_data[\"oMSZoning\"] = all_data.MSZoning.map({'C (all)':1, 'RH':2, 'RM':2, 'RL':3, 'FV':4})\n    \n    all_data[\"oNeighborhood\"] = all_data.Neighborhood.map({'MeadowV':1,\n                                               'IDOTRR':2, 'BrDale':2,\n                                               'OldTown':3, 'Edwards':3, 'BrkSide':3,\n                                               'Sawyer':4, 'Blueste':4, 'SWISU':4, 'NAmes':4,\n                                               'NPkVill':5, 'Mitchel':5,\n                                               'SawyerW':6, 'Gilbert':6, 'NWAmes':6,\n                                               'Blmngtn':7, 'CollgCr':7, 'ClearCr':7, 'Crawfor':7,\n                                               'Veenker':8, 'Somerst':8, 'Timber':8,\n                                               'StoneBr':9,\n                                               'NoRidge':10, 'NridgHt':10})\n    \n    all_data[\"oCondition1\"] = all_data.Condition1.map({'Artery':1,\n                                           'Feedr':2, 'RRAe':2,\n                                           'Norm':3, 'RRAn':3,\n                                           'PosN':4, 'RRNe':4,\n                                           'PosA':5 ,'RRNn':5})\n    \n    all_data[\"oBldgType\"] = all_data.BldgType.map({'2fmCon':1, 'Duplex':1, 'Twnhs':1, '1Fam':2, 'TwnhsE':2})\n    \n    all_data[\"oHouseStyle\"] = all_data.HouseStyle.map({'1.5Unf':1, \n                                           '1.5Fin':2, '2.5Unf':2, 'SFoyer':2, \n                                           '1Story':3, 'SLvl':3,\n                                           '2Story':4, '2.5Fin':4})\n    \n    all_data[\"oExterior1st\"] = all_data.Exterior1st.map({'BrkComm':1,\n                                             'AsphShn':2, 'CBlock':2, 'AsbShng':2,\n                                             'WdShing':3, 'Wd Sdng':3, 'MetalSd':3, 'Stucco':3, 'HdBoard':3,\n                                             'BrkFace':4, 'Plywood':4,\n                                             'VinylSd':5,\n                                             'CemntBd':6,\n                                             'Stone':7, 'ImStucc':7})\n    \n    all_data[\"oMasVnrType\"] = all_data.MasVnrType.map({'BrkCmn':1, 'None':1, 'BrkFace':2, 'Stone':3})\n    \n    all_data[\"oExterQual\"] = all_data.ExterQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\n    \n    all_data[\"oFoundation\"] = all_data.Foundation.map({'Slab':1, \n                                           'BrkTil':2, 'CBlock':2, 'Stone':2,\n                                           'Wood':3, 'PConc':4})\n    \n    all_data[\"oBsmtQual\"] = all_data.BsmtQual.map({'Fa':2, 'None':1, 'TA':3, 'Gd':4, 'Ex':5})\n    \n    all_data[\"oBsmtExposure\"] = all_data.BsmtExposure.map({'None':1, 'No':2, 'Av':3, 'Mn':3, 'Gd':4})\n    \n    all_data[\"oHeating\"] = all_data.Heating.map({'Floor':1, 'Grav':1, 'Wall':2, 'OthW':3, 'GasW':4, 'GasA':5})\n    \n    all_data[\"oHeatingQC\"] = all_data.HeatingQC.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n    \n    all_data[\"oKitchenQual\"] = all_data.KitchenQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\n    \n    all_data[\"oFunctional\"] = all_data.Functional.map({'Maj2':1, 'Maj1':2, 'Min1':2, 'Min2':2, 'Mod':2, 'Sev':2, 'Typ':3})\n    \n    all_data[\"oFireplaceQu\"] = all_data.FireplaceQu.map({'None':1, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n    \n    all_data[\"oGarageType\"] = all_data.GarageType.map({'CarPort':1, 'None':1,\n                                           'Detchd':2,\n                                           '2Types':3, 'Basment':3,\n                                           'Attchd':4, 'BuiltIn':5})\n    \n    all_data[\"oGarageFinish\"] = all_data.GarageFinish.map({'None':1, 'Unf':2, 'RFn':3, 'Fin':4})\n    \n    all_data[\"oPavedDrive\"] = all_data.PavedDrive.map({'N':1, 'P':2, 'Y':3})\n    \n    all_data[\"oSaleType\"] = all_data.SaleType.map({'COD':1, 'ConLD':1, 'ConLI':1, 'ConLw':1, 'Oth':1, 'WD':1,\n                                       'CWD':2, 'Con':3, 'New':3})\n    \n    all_data[\"oSaleCondition\"] = all_data.SaleCondition.map({'AdjLand':1, 'Abnorml':2, 'Alloca':2, 'Family':2, 'Normal':3, 'Partial':4})            \n                \n                        \n                        \n    \n    return \"Done!\"\n\nmap_values()\n\n\n\n\n\n'Done!'\n\n\n\nall_data.drop(\"LotAreaCut\",axis=1,inplace=True)\n\n\n\n\n\n#all_data.drop(['SalePrice'],axis=1,inplace=True)\n\nclass labelenc(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        lab=LabelEncoder()\n        X[\"YearBuilt\"] = lab.fit_transform(X[\"YearBuilt\"])\n        X[\"YearRemodAdd\"] = lab.fit_transform(X[\"YearRemodAdd\"])\n        X[\"GarageYrBlt\"] = lab.fit_transform(X[\"GarageYrBlt\"])\n        return X\n    \nclass skew_dummies(BaseEstimator, TransformerMixin):\n    def __init__(self,skew=0.5):\n        self.skew = skew\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        X_numeric=X.select_dtypes(exclude=[\"object\"])\n        skewness = X_numeric.apply(lambda x: skew(x))\n        skewness_features = skewness[abs(skewness) &gt;= self.skew].index\n        X[skewness_features] = np.log1p(X[skewness_features])\n        X = pd.get_dummies(X)\n        return X\n    \npipe = Pipeline([\n    ('labenc', labelenc()),\n    ('skew_dummies', skew_dummies(skew=1)),\n    ])\n\nall_data2 = all_data.copy()\n\ndata_pipe = pipe.fit_transform(all_data2)\n\nscaler = RobustScaler()\n\nn_train=train.shape[0]\n\nX = data_pipe[:n_train]\ntest_X = data_pipe[n_train:]\ny= train.SalePrice\n\nX_scaled = scaler.fit(X).transform(X)\ny_log = np.log(train.SalePrice)\ntest_X_scaled = scaler.transform(test_X)\n\n\n\nclass add_feature(BaseEstimator, TransformerMixin):\n    def __init__(self,additional=1):\n        self.additional = additional\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        if self.additional==1:\n            X[\"TotalHouse\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]   \n            X[\"TotalArea\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"]\n            \n        else:\n            X[\"TotalHouse\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]   \n            X[\"TotalArea\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"]\n            \n            X[\"+_TotalHouse_OverallQual\"] = X[\"TotalHouse\"] * X[\"OverallQual\"]\n            X[\"+_GrLivArea_OverallQual\"] = X[\"GrLivArea\"] * X[\"OverallQual\"]\n            X[\"+_oMSZoning_TotalHouse\"] = X[\"oMSZoning\"] * X[\"TotalHouse\"]\n            X[\"+_oMSZoning_OverallQual\"] = X[\"oMSZoning\"] + X[\"OverallQual\"]\n            X[\"+_oMSZoning_YearBuilt\"] = X[\"oMSZoning\"] + X[\"YearBuilt\"]\n            X[\"+_oNeighborhood_TotalHouse\"] = X[\"oNeighborhood\"] * X[\"TotalHouse\"]\n            X[\"+_oNeighborhood_OverallQual\"] = X[\"oNeighborhood\"] + X[\"OverallQual\"]\n            X[\"+_oNeighborhood_YearBuilt\"] = X[\"oNeighborhood\"] + X[\"YearBuilt\"]\n            X[\"+_BsmtFinSF1_OverallQual\"] = X[\"BsmtFinSF1\"] * X[\"OverallQual\"]\n            \n            X[\"-_oFunctional_TotalHouse\"] = X[\"oFunctional\"] * X[\"TotalHouse\"]\n            X[\"-_oFunctional_OverallQual\"] = X[\"oFunctional\"] + X[\"OverallQual\"]\n            X[\"-_LotArea_OverallQual\"] = X[\"LotArea\"] * X[\"OverallQual\"]\n            X[\"-_TotalHouse_LotArea\"] = X[\"TotalHouse\"] + X[\"LotArea\"]\n            X[\"-_oCondition1_TotalHouse\"] = X[\"oCondition1\"] * X[\"TotalHouse\"]\n            X[\"-_oCondition1_OverallQual\"] = X[\"oCondition1\"] + X[\"OverallQual\"]\n            \n           \n            X[\"Bsmt\"] = X[\"BsmtFinSF1\"] + X[\"BsmtFinSF2\"] + X[\"BsmtUnfSF\"]\n            X[\"Rooms\"] = X[\"FullBath\"]+X[\"TotRmsAbvGrd\"]\n            X[\"PorchArea\"] = X[\"OpenPorchSF\"]+X[\"EnclosedPorch\"]+X[\"3SsnPorch\"]+X[\"ScreenPorch\"]\n            X[\"TotalPlace\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"] + X[\"OpenPorchSF\"]+X[\"EnclosedPorch\"]+X[\"3SsnPorch\"]+X[\"ScreenPorch\"]\n\n    \n            return X\n\npipe = Pipeline([\n    ('labenc', labelenc()),\n    ('add_feature', add_feature(additional=2)),\n    ('skew_dummies', skew_dummies(skew=1)),\n    ])\n\npiped = pipe.fit_transform(all_data)\npiped.shape\n\n\n\n(2917, 427)\n\n\n\nnp.save(\"piped\",piped)\nnp.save(\"y_raw\",y_raw)"
					}
					
				
			
		
			
				
					,
					
					"machine-learning-bag-of-words": {
						"id": "machine-learning-bag-of-words",
						"title": "Bag of words",
						"categories": "Machine-Learning",
						"url": " /machine-learning/bag-of-words/",
						"content": "If text and images are the only data we got, it’s more convenient to apply specific approach for these types of data.\n\nThe common scenario is that we have text or images as additional data set, we need to grasp different features that can be input to machine learning models as a complementary to our main data frame of samples and features.\n\nBag of words (BOW)\n\nPipeline of applying BOW:\n\n  Preprocessing:\n Lowercase, stemming, lemmatization, stopwords\n  N-grams can help to use local context\n  Postprocessing: TF-IDF\n\n\nTF-IDF\n\nIntuition: normalize data column-wise\n\nsklearn.feature_extraction.text.TfidfVectorizer\n\n\n\nTerm Frequency (TF)\ntf = 1/x.sum(axis=1)[:,None]\nx = x*tf\n\n\n\nInverse Document Frequency (IDF)\nidf = np.log(x.shape[0]/(x&gt;0).sum(0))\nx=x*idf\n\n\n\nN-grams\n\nsklearn.feature_extraction.text.CountVectorizer: \nNgram_range, analyzer\n\n\n\nIf you have 28 unique symbols, the number of all possible combinations is 28x28\n\nNext: Embeddings\n\nEmbeddings like Word2Vec"
					}
					
				
			
		
			
				
					,
					
					"recommender-word2vec": {
						"id": "recommender-word2vec",
						"title": "From Word2Vec to Item Recommendation",
						"categories": "Recommender",
						"url": " /recommender/Word2Vec/",
						"content": "As a data analyst, I’m always looking for ways to leverage massive data set to automate customer-facing experiences.\n\nFollowing is a minimalist script explaining how to apply Word2Vec for item recommendation and the data science behind it.\n\nThe demo data can be downloaded here: https://github.com/zygmuntz/goodbooks-10k\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport math\nimport os\nimport random\nfrom tempfile import gettempdir\nimport zipfile\n\nimport numpy as np\nfrom six.moves import urllib\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\nimport pickle\nimport pandas as pd\n\n\n\n\ntr = pd.read_csv( 'to_read.csv' )\nb = pd.read_csv( 'books.csv' )\nbt = tr.merge( b[[ 'book_id', 'title']], on = 'book_id' )\n\n\n\nbt.sample(10)\n\n\n\nMake every user’s to-read list as a sentence and feed it into word2vec.\n\nGiven a dataset large enough, it can find out similar books\n\n\n\n\n  \n    \n      \n      user_id\n      book_id\n      title\n    \n  \n  \n    \n      493151\n      12921\n      98\n      The Girl Who Played with Fire (Millennium, #2)\n    \n    \n      458637\n      33014\n      51\n      City of Bones (The Mortal Instruments, #1)\n    \n    \n      594218\n      10492\n      2476\n      The Winter Sea (Slains, #1)\n    \n    \n      84267\n      11410\n      341\n      The Iliad\n    \n    \n      35157\n      17107\n      13\n      1984\n    \n    \n      712362\n      38510\n      3852\n      Deity (Covenant, #3)\n    \n    \n      467464\n      48549\n      17\n      Catching Fire (The Hunger Games, #2)\n    \n    \n      519192\n      48347\n      182\n      Beautiful Creatures (Caster Chronicles, #1)\n    \n    \n      366676\n      22024\n      188\n      A Dance with Dragons (A Song of Ice and Fire, #5)\n    \n    \n      588764\n      31407\n      3953\n      The New Jim Crow: Mass Incarceration in the Ag...\n    \n  \n\n\n\nDetailed exploration and data cleaning process -&gt;\n\nraw = bt.groupby('user_id')['title'].apply(list)\nassert len(raw)==tr.user_id.nunique()\n\n\n\nwords = [b for books in raw.values for b in books]\nn_words = bt.title.nunique()\nprint(\"There are {} books and {} to-read records\".format(n_words,len(words)))\n\n\n\nThere are 9951 books and 912705 to-read records\n\n\n\ndef build_dataset(words, n_words):\n    count = []\n    count.extend(collections.Counter(words).most_common(n_words))\n    dictionary = dict()\n    for word, _ in count:\n        dictionary[word] = len(dictionary)\n    data = list()\n    for word in words:\n        index = dictionary.get(word, 0)\n        data.append(index)\n    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n    return data, count, dictionary, reversed_dictionary\n\n\n\n\n\ndata, count, dictionary, reverse_dictionary = build_dataset(words,n_words)\n\n\n\nwords[0]\n\n\n\n\"The Aviator's Wife\"\n\n\n\nfrom tensorflow.python.client import device_lib\ndevice_lib.list_local_devices()\n\n\n\n[name: \"/cpu:0\"\n device_type: \"CPU\"\n memory_limit: 268435456\n locality {\n }\n incarnation: 15191507020555158226]\n\n\n\ndata_index = 0\n\n# Step 3: Function to generate a training batch for the skip-gram model.\ndef generate_batch(batch_size, num_skips, skip_window):\n    global data_index\n    assert batch_size % num_skips == 0\n    assert num_skips &lt;= 2*skip_window\n    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n    buffer = collections.deque(maxlen=span)\n    if data_index + span &gt; len(data):\n        data_index = 0\n    buffer.extend(data[data_index:data_index + span])\n    data_index += span\n    for i in range(batch_size // num_skips):\n        context_words = [w for w in range(span) if w != skip_window]\n        words_to_use = random.sample(context_words, num_skips)\n        for j, context_word in enumerate(words_to_use):\n            batch[i * num_skips + j] = buffer[skip_window]\n            labels[i * num_skips + j, 0] = buffer[context_word]\n        if data_index == len(data):\n            for book in data[:span]:\n                buffer.append(book)\n            data_index = span\n        else:\n            buffer.append(data[data_index])\n            data_index += 1\n  # Backtrack a little bit to avoid skipping words in the end of a batch\n    data_index = (data_index + len(data) - span) % len(data)\n    return batch, labels\n\nbatch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\nfor i in range(8):\n  print(batch[i], reverse_dictionary[batch[i]],\n        '-&gt;', labels[i, 0], reverse_dictionary[labels[i, 0]])\n\n\n\n\n\n86 Me Before You (Me Before You, #1) -&gt; 380 The Husband's Secret\n86 Me Before You (Me Before You, #1) -&gt; 721 The Aviator's Wife\n380 The Husband's Secret -&gt; 175 A Little Life\n380 The Husband's Secret -&gt; 86 Me Before You (Me Before You, #1)\n175 A Little Life -&gt; 51 Go Set a Watchman\n175 A Little Life -&gt; 380 The Husband's Secret\n51 Go Set a Watchman -&gt; 175 A Little Life\n51 Go Set a Watchman -&gt; 1094 The Marriage of Opposites\n\n\n\n# Step 4: Build and train a skip-gram model.\n\nbatch_size = 128\nembedding_size = 128  # Dimension of the embedding vector.\nskip_window = 1       # How many words to consider left and right.\nnum_skips = 2         # How many times to reuse an input to generate a label.\nnum_sampled = 64      # Number of negative examples to sample.\n\n# We pick a random validation set to sample nearest neighbors. Here we limit the\n# validation samples to the words that have a low numeric ID, which by\n# construction are also the most frequent. These 3 variables are used only for\n# displaying model accuracy, they don't affect calculation.\nvalid_size = 16     # Random set of words to evaluate similarity on.\nvalid_window = 100  # Only pick dev samples in the head of the distribution.\nvalid_examples = np.random.choice(valid_window, valid_size, replace=False)\n\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n\n  # Input data.\n    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n\n  # Ops and variables pinned to the CPU because of missing GPU implementation\n    with tf.device('/cpu:0'):\n        embeddings = tf.Variable(\n            tf.random_uniform([n_words,embedding_size],-1.0,1.0))\n        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n\n        # Construct the variables for the NCE loss\n        nce_weights = tf.Variable(\n            tf.truncated_normal([n_words, embedding_size],\n                                stddev=1.0 / math.sqrt(embedding_size)))\n        nce_biases = tf.Variable(tf.zeros([n_words]))\n\n    # Compute the average NCE loss for the batch.\n    # tf.nce_loss automatically draws a new sample of the negative labels each\n    # time we evaluate the loss.\n    # Explanation of the meaning of NCE loss:\n    #   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n    loss = tf.reduce_mean(\n      tf.nn.nce_loss(weights=nce_weights,\n                     biases=nce_biases,\n                     labels=train_labels,\n                     inputs=embed,\n                     num_sampled=num_sampled,\n                     num_classes=n_words))\n\n    # Construct the SGD optimizer using a learning rate of 1.0.\n    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n\n    # Compute the cosine similarity between minibatch examples and all embeddings.\n    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n    normalized_embeddings = embeddings / norm\n    valid_embeddings = tf.nn.embedding_lookup(\n      normalized_embeddings, valid_dataset)\n    similarity = tf.matmul(\n      valid_embeddings, normalized_embeddings, transpose_b=True)\n\n    # Add variable initializer.\n    init = tf.global_variables_initializer()\n\n# Step 5: Begin training.\nnum_steps = 100001\n\nwith tf.Session(graph=graph) as session:\n    # We must initialize all variables before we use them.\n    init.run()\n    print('Initialized')\n\n    average_loss = 0\n    for step in xrange(num_steps):\n        batch_inputs, batch_labels = generate_batch(\n            batch_size, num_skips, skip_window)\n        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n\n        # We perform one update step by evaluating the optimizer op (including it\n        # in the list of returned values for session.run()\n        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n        average_loss += loss_val\n\n        if step % 2000 == 0:\n            if step &gt; 0:\n                average_loss /= 2000\n          # The average loss is an estimate of the loss over the last 2000 batches.\n            print('Average loss at step ', step, ': ', average_loss)\n            average_loss = 0\n\n    # Note that this is expensive (~20% slowdown if computed every 500 steps)\n        if step % 10000 == 0:\n            sim = similarity.eval()\n            for i in xrange(valid_size):\n                valid_word = reverse_dictionary[valid_examples[i]]\n                top_k = 8  # number of nearest neighbors\n                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n                log_str = 'Nearest to %s:' % valid_word\n                for k in xrange(top_k):\n                    close_word = reverse_dictionary[nearest[k]]\n                    log_str = '%s %s,' % (log_str, close_word)\n                print(log_str)\n    final_embeddings = normalized_embeddings.eval()\n\n\n\n\n\nInitialized\nAverage loss at step  0 :  240.497894287\nNearest to The Girl with the Dragon Tattoo (Millennium, #1): Gerald's Game, Rat Queens, Vol. 1: Sass &amp; Sorcery, An American Tragedy, Purple Cow: Transform Your Business by Being Remarkable, Humans of New York: Stories, Beyond the Grave (The 39 Clues #4), The Weight of Silence, Certain Prey (Lucas Davenport, #10),\nNearest to Lord of the Flies: The Age of Miracles, Ida B. . . and Her Plans to Maximize Fun, Avoid Disaster, and (Possibly) Save the World, The Source of Magic (Xanth, #2), Blackwood Farm (The Vampire Chronicles, #9), The Power of One (The Power of One, #1), Whispers at Moonrise (Shadow Falls, #4), Stitches, Holiday in Death (In Death, #7),\nNearest to Bossypants: First Love, The Expats, Girlfriend in a Coma, The Boyfriend List: 15 Guys, 11 Shrink Appointments, 4 Ceramic Frogs and Me, Ruby Oliver (Ruby Oliver, #1), Ordinary People, I Want My Hat Back, Walk Two Moons, Restless,\nNearest to City of Bones (The Mortal Instruments, #1): A Moveable Feast, Travesuras de la niña mala, Outcast of Redwall (Redwall, #8), Twilight Director's Notebook : The Story of How We Made the Movie Based on the Novel by Stephenie Meyer, Red Seas Under Red Skies (Gentleman Bastard, #2), Cat on a Hot Tin Roof, The Unexpected Mrs. Pollifax  (Mrs. Pollifax #1), Once (Eve, #2),\nNearest to Jane Eyre: A Court of Mist and Fury (A Court of Thorns and Roses, #2), And the Mountains Echoed, Ever After (The Hollows, #11), The Darkest Pleasure (Lords of the Underworld #3), The Beekeeper's Apprentice (Mary Russell, #1), The Evolution of Mara Dyer (Mara Dyer, #2), Ten Tiny Breaths (Ten Tiny Breaths, #1), Thinking, Fast and Slow,\nNearest to The Lovely Bones: No Country for Old Men, The House Girl, Krakatoa: The Day the World Exploded, Heir of Fire (Throne of Glass, #3), This Side of Paradise, Porno, The Watchman (Elvis Cole, #11; Joe Pike, #1), A Hunger Like No Other (Immortals After Dark #2),\nNearest to The Giver (The Giver, #1): George, Hourglass (Hourglass, #1), Raise High the Roof Beam, Carpenters &amp; Seymour: An Introduction, J is for Judgment (Kinsey Millhone, #10), Deeper Than the Dead (Oak Knoll, #1), Book of the Dead (Kay Scarpetta, #15), The Good, the Bad, and the Undead (The Hollows, #2), The Temple of My Familiar,\nNearest to Atonement: Thirst No. 3: The Eternal Dawn (Thirst, #3), Barefoot Contessa Family Style: Easy Ideas and Recipes That Make Everyone Feel Like Family, Five Point Someone, Dark Legend (Dark, #8), Lord Loss (The Demonata, #1), The Pioneer Woman Cooks: Recipes from an Accidental Country Girl, Revelation (Matthew Shardlake, #4), Fairest (An Unfortunate Fairy Tale, #2),\nNearest to The Handmaid's Tale: The Tell-Tale Heart and Other Writings, Sloppy Firsts (Jessica Darling, #1), Clifford the Big Red Dog, Think Twice (Rosato &amp; Associates, #11), Hardwired (Hacker, #1), Homage to Catalonia, Jazz, L is for Lawless (Kinsey Millhone, #12),\nNearest to Animal Farm: Haunted, The Golem and the Jinni (The Golem and the Jinni, #1), Flat Stanley (Flat Stanley, #1), Gap Creek, Crazy For You, Rise of Empire (The Riyria Revelations, #3-4), Radiant Shadows (Wicked Lovely, #4), Killing Yourself to Live: 85% of a True Story,\nNearest to The Ocean at the End of the Lane: The Warrior Heir (The Heir Chronicles, #1), Killing Yourself to Live: 85% of a True Story, The Complete Novels, A Breath of Snow and Ashes (Outlander, #6), The Last Coyote (Harry Bosch, #4; Harry Bosch Universe, #4), Owl Moon, All the Birds in the Sky, Self-Reliance and Other Essays,\nNearest to Outlander (Outlander, #1): A God in Ruins, Mariana, Father Mine (Black Dagger Brotherhood, #6.5), The Apostle (Scot Harvath, #8), Voyager (Outlander, #3), The Rosie Effect (Don Tillman, #2), Rodrick Rules (Diary of a Wimpy Kid, #2), The Book of Joe,\nNearest to The Poisonwood Bible: Percy Jackson and the Sword of Hades (Percy Jackson and the Olympians, #4.5), Open Season, The Orphan Master's Son, Shōgun (Asian Saga, #1), Harold and the Purple Crayon, The Solitude of Prime Numbers, A Thousand Pieces of You (Firebird, #1), Heir of Fire (Throne of Glass, #3),\nNearest to 1984: The Mad Ship (Liveship Traders, #2), Rosencrantz and Guildenstern Are Dead, The Monk, Magic Steps (The Circle Opens, #1), The Science of Getting Rich, The Fountains of Paradise, The Red Knight (The Traitor Son Cycle, #1), The God Delusion,\nNearest to The Devil in the White City: Murder, Magic, and Madness at the Fair That Changed America: Songs of Innocence and of Experience, Little House in the Big Woods (Little House, #1), The Calling (Darkness Rising, #2), Complete Poems, 1904-1962, Gorky Park (Arkady Renko, #1), Burn for Burn (Burn for Burn, #1), Death in Venice, The Phantom Tollbooth,\nNearest to The Maze Runner (Maze Runner, #1): Trojan Odyssey (Dirk Pitt, #17), Love, Stargirl (Stargirl, #2), Assholes Finish First (Tucker Max, #2), The 39 Steps (Richard Hannay, #1), Moriarty (Sherlock Holmes #2), Neil Patrick Harris: Choose Your Own Autobiography, Vampire Knight, Vol. 4 (Vampire Knight, #4), The Path to Power (The Years of Lyndon Johnson, #1),\nAverage loss at step  2000 :  51.1207590363\nAverage loss at step  4000 :  8.39590741718\nAverage loss at step  6000 :  5.54902770615\nAverage loss at step  8000 :  4.92703645575\nAverage loss at step  10000 :  4.68519584727\nNearest to The Girl with the Dragon Tattoo (Millennium, #1): Purple Cow: Transform Your Business by Being Remarkable, Gerald's Game, Rat Queens, Vol. 1: Sass &amp; Sorcery, The Weight of Silence, The Highlander's Touch (Highlander, #3), Lucky Jim, Humans of New York: Stories, Rise of the Elgen (Michael Vey, #2),\nNearest to Lord of the Flies: Crime and Punishment, Zen and the Art of Motorcycle Maintenance: An Inquiry Into Values, The Great Gatsby, The Source of Magic (Xanth, #2), The Power of One (The Power of One, #1), The Shining (The Shining #1), Blackwood Farm (The Vampire Chronicles, #9), What We Keep ,\nNearest to Bossypants: The Expats, Kabul Beauty School: An American Woman Goes Behind the Veil, A Trick of the Light (Chief Inspector Armand Gamache, #7), Walk Two Moons, Miss Peregrine’s Home for Peculiar Children (Miss Peregrine’s Peculiar Children, #1), Chanakya's Chant, Glue, Everbound (Everneath, #2),\nNearest to City of Bones (The Mortal Instruments, #1): Twilight Director's Notebook : The Story of How We Made the Movie Based on the Novel by Stephenie Meyer, The Forgotten Garden, Clockwork Angel (The Infernal Devices, #1), One Day, Kitty and the Midnight Hour (Kitty Norville #1), Such a Rush, Once (Eve, #2), The Righteous Mind: Why Good People are Divided by Politics and Religion,\nNearest to Jane Eyre: The Subtle Knife (His Dark Materials, #2), Extremely Loud and Incredibly Close, The Lovely Bones, Eat, Pray, Love, Islands in the Stream, The Beekeeper's Apprentice (Mary Russell, #1), A Long Way Gone: Memoirs of a Boy Soldier, A Court of Mist and Fury (A Court of Thorns and Roses, #2),\nNearest to The Lovely Bones: No Country for Old Men, The Secret Life of Bees, The House Girl, Two Ravens and One Crow (The Iron Druid Chronicles, #4.5), Krakatoa: The Day the World Exploded, This Side of Paradise, Jane Eyre, Uglies (Uglies, #1),\nNearest to The Giver (The Giver, #1): Animal, Vegetable, Miracle: A Year of Food Life, Life of Pi, Wicked: The Life and Times of the Wicked Witch of the West (The Wicked Years, #1), The Old Man and the Sea, George, The Good, the Bad, and the Undead (The Hollows, #2), Till We Have Faces, J is for Judgment (Kinsey Millhone, #10),\nNearest to Atonement: Siddhartha, Lord Loss (The Demonata, #1), The Old Man and the Sea, Story of the Eye, To Kill a Mockingbird, Priestess of the White (Age of the Five, #1), A Deepness in the Sky (Zones of Thought, #2), Complete Works of Oscar Wilde,\nNearest to The Handmaid's Tale: The Tell-Tale Heart and Other Writings, Three Cups of Tea: One Man's Mission to Promote Peace ... One School at a Time, For Whom the Bell Tolls, Girl, Stolen (Girl, Stolen #1), Dragon Ball, Vol. 1: The Monkey King (Dragon Ball, #1), Providence (Providence, #1), The Mists of Avalon (Avalon, #1), Hardwired (Hacker, #1),\nNearest to Animal Farm: Haunted, Gap Creek, Flat Stanley (Flat Stanley, #1), Killing Yourself to Live: 85% of a True Story, Blood Magick (The Cousins O'Dwyer Trilogy, #3), Radiant Shadows (Wicked Lovely, #4), Crash &amp; Burn (Tessa Leoni, #3), Angels &amp; Demons  (Robert Langdon, #1),\nNearest to The Ocean at the End of the Lane: The Complete Novels, A Breath of Snow and Ashes (Outlander, #6), Owl Moon, Ms. Marvel, Vol. 2: Generation Why, One More Chance (Rosemary Beach, #8; Chance, #2), The Warrior Heir (The Heir Chronicles, #1), Rogues, Unlimited Power : The New Science Of Personal Achievement,\nNearest to Outlander (Outlander, #1): Leven Thumps and the Eyes of the Want (Leven Thumps, #3), Ender's Game (Ender's Saga, #1), The Mermaid Chair, A God in Ruins, When Calls the Heart (Canadian West, #1), The Rosie Effect (Don Tillman, #2), Father Mine (Black Dagger Brotherhood, #6.5), Redshirts,\nNearest to The Poisonwood Bible: Percy Jackson and the Sword of Hades (Percy Jackson and the Olympians, #4.5), Harold and the Purple Crayon, Open Season, The First World War, Mere Christianity, The Solitude of Prime Numbers, The Plot Against America, A Thousand Pieces of You (Firebird, #1),\nNearest to 1984: Rosencrantz and Guildenstern Are Dead, The God Delusion, The Red Knight (The Traitor Son Cycle, #1), Slaughterhouse-Five, The Science of Getting Rich, The Monk, Magic Steps (The Circle Opens, #1), Gone (Wake, #3),\nNearest to The Devil in the White City: Murder, Magic, and Madness at the Fair That Changed America: Complete Poems, 1904-1962, The Hobbit, The Joke, Slaughterhouse-Five, Songs of Innocence and of Experience, The Panther  (John Corey, #6), Little House in the Big Woods (Little House, #1), Gorky Park (Arkady Renko, #1),\nNearest to The Maze Runner (Maze Runner, #1): Trojan Odyssey (Dirk Pitt, #17), The 39 Steps (Richard Hannay, #1), Moriarty (Sherlock Holmes #2), Assholes Finish First (Tucker Max, #2), Neil Patrick Harris: Choose Your Own Autobiography, The Body Finder (The Body Finder, #1), Love, Stargirl (Stargirl, #2), Brotherhood in Death (In Death, #42),\nAverage loss at step  12000 :  4.50244684148\nAverage loss at step  14000 :  4.33098444271\nAverage loss at step  16000 :  4.28433195019\nAverage loss at step  18000 :  4.09698849154\nAverage loss at step  20000 :  4.04729236376\nNearest to The Girl with the Dragon Tattoo (Millennium, #1): Purple Cow: Transform Your Business by Being Remarkable, Rat Queens, Vol. 1: Sass &amp; Sorcery, Gerald's Game, The Highlander's Touch (Highlander, #3), Rise of the Elgen (Michael Vey, #2), Beautiful Bitch (Beautiful Bastard, #1.5), Treasure Island, The Weight of Silence,\nNearest to Lord of the Flies: Crime and Punishment, Zen and the Art of Motorcycle Maintenance: An Inquiry Into Values, The Great Gatsby, The Shining (The Shining #1), The Source of Magic (Xanth, #2), 1984, The Quiet American, Ulysses,\nNearest to Bossypants: Miss Peregrine’s Home for Peculiar Children (Miss Peregrine’s Peculiar Children, #1), The Expats, State of Wonder, In the Garden of Beasts: Love, Terror, and an American Family in Hitler's Berlin, Rapture (Fallen, #4), A Trick of the Light (Chief Inspector Armand Gamache, #7), Chanakya's Chant, National Velvet,\nNearest to City of Bones (The Mortal Instruments, #1): One Day, Clockwork Angel (The Infernal Devices, #1), Twilight Director's Notebook : The Story of How We Made the Movie Based on the Novel by Stephenie Meyer, The Help, Chains (Seeds of America, #1), The Forgotten Garden, Graceling (Graceling Realm, #1), Kitty and the Midnight Hour (Kitty Norville #1),\nNearest to Jane Eyre: The Subtle Knife (His Dark Materials, #2), The Lovely Bones, Extremely Loud and Incredibly Close, Eat, Pray, Love, Les Misérables, A Long Way Gone: Memoirs of a Boy Soldier, Memoirs of a Geisha, The Beekeeper's Apprentice (Mary Russell, #1),\nNearest to The Lovely Bones: The Secret Life of Bees, No Country for Old Men, Two Ravens and One Crow (The Iron Druid Chronicles, #4.5), Jane Eyre, Uglies (Uglies, #1), The House Girl, This Side of Paradise, Divided in Death (In Death, #18),\nNearest to The Giver (The Giver, #1): The Old Man and the Sea, Life of Pi, Wicked: The Life and Times of the Wicked Witch of the West (The Wicked Years, #1), Animal, Vegetable, Miracle: A Year of Food Life, Memoirs of a Geisha, Team of Rivals: The Political Genius of Abraham Lincoln, George, Till We Have Faces,\nNearest to Atonement: Siddhartha, The Old Man and the Sea, Lord Loss (The Demonata, #1), To Kill a Mockingbird, Memoirs of a Geisha, Story of the Eye, Breakfast of Champions, Choke,\nNearest to The Handmaid's Tale: Three Cups of Tea: One Man's Mission to Promote Peace ... One School at a Time, For Whom the Bell Tolls, The Mists of Avalon (Avalon, #1), The Portrait of a Lady, White Noise, No Country for Old Men, Wintersmith (Discworld, #35; Tiffany Aching, #3), Running with Scissors,\nNearest to Animal Farm: Haunted, Gone with the Wind, Artemis Fowl (Artemis Fowl, #1), Killing Yourself to Live: 85% of a True Story, Gap Creek, Angels &amp; Demons  (Robert Langdon, #1), Flat Stanley (Flat Stanley, #1), Black Hole,\nNearest to The Ocean at the End of the Lane: The Complete Novels, Owl Moon, Unlimited Power : The New Science Of Personal Achievement, Ms. Marvel, Vol. 2: Generation Why, Z: A Novel of Zelda Fitzgerald, The Runaway King (The Ascendance Trilogy, #2), Rogues, One More Chance (Rosemary Beach, #8; Chance, #2),\nNearest to Outlander (Outlander, #1): The Mermaid Chair, Leven Thumps and the Eyes of the Want (Leven Thumps, #3), A God in Ruins, Ender's Game (Ender's Saga, #1), Redshirts, When Calls the Heart (Canadian West, #1), The Golden Compass (His Dark Materials, #1), Father Mine (Black Dagger Brotherhood, #6.5),\nNearest to The Poisonwood Bible: The Plot Against America, The Kite Runner, Mere Christianity, Harold and the Purple Crayon, Percy Jackson and the Sword of Hades (Percy Jackson and the Olympians, #4.5), Open Season, The First World War, Anil's Ghost,\nNearest to 1984: The God Delusion, Rosencrantz and Guildenstern Are Dead, Slaughterhouse-Five, Lolita, Guns, Germs, and Steel: The Fates of Human Societies, The Red Knight (The Traitor Son Cycle, #1), The Time Traveler's Wife, Magic Steps (The Circle Opens, #1),\nNearest to The Devil in the White City: Murder, Magic, and Madness at the Fair That Changed America: Slaughterhouse-Five, The Joke, Salt: A World History, The Hobbit, Interpreter of Maladies, The Panther  (John Corey, #6), The Hammer of Thor (Magnus Chase and the Gods of Asgard, #2), Complete Poems, 1904-1962,\nNearest to The Maze Runner (Maze Runner, #1): Trojan Odyssey (Dirk Pitt, #17), The 39 Steps (Richard Hannay, #1), The Body Finder (The Body Finder, #1), Moriarty (Sherlock Holmes #2), Apathy and Other Small Victories, Assholes Finish First (Tucker Max, #2), Brotherhood in Death (In Death, #42), The Soldier's Wife,\nAverage loss at step  22000 :  3.99871511078\nAverage loss at step  24000 :  3.91173057818\nAverage loss at step  26000 :  3.85994793272\nAverage loss at step  28000 :  3.77732089114\nAverage loss at step  30000 :  3.83034379864\nNearest to The Girl with the Dragon Tattoo (Millennium, #1): Purple Cow: Transform Your Business by Being Remarkable, Rat Queens, Vol. 1: Sass &amp; Sorcery, Midnight Sun (Twilight, #1.5), The Highlander's Touch (Highlander, #3), Gerald's Game, The Prince of Tides, The Sirens of Titan, Treasure Island,\nNearest to Lord of the Flies: Crime and Punishment, Zen and the Art of Motorcycle Maintenance: An Inquiry Into Values, The Great Gatsby, The Quiet American, 1984, The Shining (The Shining #1), The Source of Magic (Xanth, #2), The Holy Bible: English Standard Version,\nNearest to Bossypants: Miss Peregrine’s Home for Peculiar Children (Miss Peregrine’s Peculiar Children, #1), The Expats, State of Wonder, In the Garden of Beasts: Love, Terror, and an American Family in Hitler's Berlin, Rapture (Fallen, #4), Barbarians at the Gate: The Fall of RJR Nabisco, A Trick of the Light (Chief Inspector Armand Gamache, #7), Chanakya's Chant,\nNearest to City of Bones (The Mortal Instruments, #1): One Day, The Help, Chains (Seeds of America, #1), Clockwork Angel (The Infernal Devices, #1), Twilight Director's Notebook : The Story of How We Made the Movie Based on the Novel by Stephenie Meyer, Graceling (Graceling Realm, #1), Best Friends Forever, City of Glass (The Mortal Instruments, #3),\nNearest to Jane Eyre: The Subtle Knife (His Dark Materials, #2), The Lovely Bones, Eat, Pray, Love, Les Misérables, Memoirs of a Geisha, Extremely Loud and Incredibly Close, Beloved, A Long Way Gone: Memoirs of a Boy Soldier,\nNearest to The Lovely Bones: The Secret Life of Bees, No Country for Old Men, Jane Eyre, Uglies (Uglies, #1), Two Ravens and One Crow (The Iron Druid Chronicles, #4.5), The Bookseller of Kabul, The House Girl, The Diary of a Young Girl,\nNearest to The Giver (The Giver, #1): The Old Man and the Sea, Life of Pi, Wicked: The Life and Times of the Wicked Witch of the West (The Wicked Years, #1), Animal, Vegetable, Miracle: A Year of Food Life, Memoirs of a Geisha, Till We Have Faces, Beloved, Prodigal Summer,\nNearest to Atonement: Siddhartha, The Old Man and the Sea, To Kill a Mockingbird, Lord Loss (The Demonata, #1), Memoirs of a Geisha, Breakfast of Champions, Story of the Eye, Choke,\nNearest to The Handmaid's Tale: Three Cups of Tea: One Man's Mission to Promote Peace ... One School at a Time, For Whom the Bell Tolls, White Noise, The Portrait of a Lady, The Mists of Avalon (Avalon, #1), No Country for Old Men, Dry, Running with Scissors,\nNearest to Animal Farm: Haunted, Gone with the Wind, Killing Yourself to Live: 85% of a True Story, Artemis Fowl (Artemis Fowl, #1), Ishmael:A Novel, The Crucible, A Farewell to Arms, Black Hole,\nNearest to The Ocean at the End of the Lane: Owl Moon, Brain on Fire: My Month of Madness, The Complete Novels, Z: A Novel of Zelda Fitzgerald, The Runaway King (The Ascendance Trilogy, #2), Ms. Marvel, Vol. 2: Generation Why, Unlimited Power : The New Science Of Personal Achievement, One More Chance (Rosemary Beach, #8; Chance, #2),\nNearest to Outlander (Outlander, #1): The Mermaid Chair, Ender's Game (Ender's Saga, #1), Leven Thumps and the Eyes of the Want (Leven Thumps, #3), A God in Ruins, The Golden Compass (His Dark Materials, #1), When Calls the Heart (Canadian West, #1), Redshirts, A Thousand Splendid Suns,\nNearest to The Poisonwood Bible: The Plot Against America, The Kite Runner, Mere Christianity, The Color Purple, Cloud Atlas, Anil's Ghost, Harold and the Purple Crayon, Guns, Germs, and Steel: The Fates of Human Societies,\nNearest to 1984: The God Delusion, Slaughterhouse-Five, Rosencrantz and Guildenstern Are Dead, The Time Traveler's Wife, Lord of the Flies, Lolita, Ender's Game (Ender's Saga, #1), The Red Knight (The Traitor Son Cycle, #1),\nNearest to The Devil in the White City: Murder, Magic, and Madness at the Fair That Changed America: Slaughterhouse-Five, The Joke, The Hammer of Thor (Magnus Chase and the Gods of Asgard, #2), Salt: A World History, Interpreter of Maladies, The Hobbit, The Panther  (John Corey, #6), The Man Who Mistook His Wife for a Hat and Other Clinical Tales,\nNearest to The Maze Runner (Maze Runner, #1): The Body Finder (The Body Finder, #1), The 39 Steps (Richard Hannay, #1), Trojan Odyssey (Dirk Pitt, #17), Soulless (Parasol Protectorate, #1), Apathy and Other Small Victories, Beautiful Darkness (Caster Chronicles, #2), Beautiful Creatures (Caster Chronicles, #1), Boneshaker (The Clockwork Century, #1),\nAverage loss at step  32000 :  3.72505885243\nAverage loss at step  34000 :  3.67791794622\nAverage loss at step  36000 :  3.65302069092\nAverage loss at step  38000 :  3.60529408705\nAverage loss at step  40000 :  3.57170747757\nNearest to The Girl with the Dragon Tattoo (Millennium, #1): Midnight Sun (Twilight, #1.5), Purple Cow: Transform Your Business by Being Remarkable, The Sirens of Titan, The Graveyard Book, The Mote in God's Eye, The Prince of Tides, James and the Giant Peach, Outliers: The Story of Success,\nNearest to Lord of the Flies: Crime and Punishment, The Great Gatsby, Zen and the Art of Motorcycle Maintenance: An Inquiry Into Values, 1984, The Quiet American, The Shining (The Shining #1), The Source of Magic (Xanth, #2), Ulysses,\nNearest to Bossypants: Miss Peregrine’s Home for Peculiar Children (Miss Peregrine’s Peculiar Children, #1), State of Wonder, The Expats, In the Garden of Beasts: Love, Terror, and an American Family in Hitler's Berlin, Barbarians at the Gate: The Fall of RJR Nabisco, Rapture (Fallen, #4), A Trick of the Light (Chief Inspector Armand Gamache, #7), Chanakya's Chant,\nNearest to City of Bones (The Mortal Instruments, #1): One Day, Chains (Seeds of America, #1), The Help, Graceling (Graceling Realm, #1), Clockwork Angel (The Infernal Devices, #1), Twilight Director's Notebook : The Story of How We Made the Movie Based on the Novel by Stephenie Meyer, City of Glass (The Mortal Instruments, #3), Best Friends Forever,\nNearest to Jane Eyre: The Lovely Bones, Eat, Pray, Love, Memoirs of a Geisha, The Subtle Knife (His Dark Materials, #2), Les Misérables, Beloved, The Importance of Being Earnest, A Long Way Gone: Memoirs of a Boy Soldier,\nNearest to The Lovely Bones: The Secret Life of Bees, Jane Eyre, No Country for Old Men, Uglies (Uglies, #1), The Diary of a Young Girl, The Bookseller of Kabul, Middlesex, The Memory Keeper's Daughter,\nNearest to The Giver (The Giver, #1): Life of Pi, The Old Man and the Sea, Memoirs of a Geisha, Wicked: The Life and Times of the Wicked Witch of the West (The Wicked Years, #1), Animal, Vegetable, Miracle: A Year of Food Life, Till We Have Faces, Catch-22, The Children of Húrin,\nNearest to Atonement: Siddhartha, The Old Man and the Sea, To Kill a Mockingbird, Breakfast of Champions, Lord Loss (The Demonata, #1), Memoirs of a Geisha, Choke, Beloved,\nNearest to The Handmaid's Tale: The Mists of Avalon (Avalon, #1), White Noise, The Portrait of a Lady, For Whom the Bell Tolls, Three Cups of Tea: One Man's Mission to Promote Peace ... One School at a Time, No Country for Old Men, Dry, The Hot Zone: The Terrifying True Story of the Origins of the Ebola Virus,\nNearest to Animal Farm: Haunted, Gone with the Wind, Artemis Fowl (Artemis Fowl, #1), Ishmael:A Novel, The Crucible, Killing Yourself to Live: 85% of a True Story, Black Hole, A Farewell to Arms,\nNearest to The Ocean at the End of the Lane: The Runaway King (The Ascendance Trilogy, #2), Owl Moon, Brain on Fire: My Month of Madness, Lone Wolf, The Twelve Tribes of Hattie, The Ship of Brides, Anya's Ghost, One More Chance (Rosemary Beach, #8; Chance, #2),\nNearest to Outlander (Outlander, #1): The Mermaid Chair, Ender's Game (Ender's Saga, #1), Leven Thumps and the Eyes of the Want (Leven Thumps, #3), Life of Pi, A God in Ruins, The Golden Compass (His Dark Materials, #1), When Calls the Heart (Canadian West, #1), Cloud Atlas,\nNearest to The Poisonwood Bible: The Kite Runner, The Plot Against America, Cloud Atlas, The Color Purple, Mere Christianity, Anil's Ghost, Jitterbug Perfume, Guns, Germs, and Steel: The Fates of Human Societies,\nNearest to 1984: The God Delusion, Slaughterhouse-Five, Rosencrantz and Guildenstern Are Dead, The Time Traveler's Wife, Lolita, Lord of the Flies, Ender's Game (Ender's Saga, #1), Crime and Punishment,\nNearest to The Devil in the White City: Murder, Magic, and Madness at the Fair That Changed America: The Joke, Slaughterhouse-Five, Salt: A World History, Interpreter of Maladies, The Hammer of Thor (Magnus Chase and the Gods of Asgard, #2), So Long, and Thanks for All the Fish (Hitchhiker's Guide to the Galaxy, #4), The Panther  (John Corey, #6), The Hobbit,\nNearest to The Maze Runner (Maze Runner, #1): The Body Finder (The Body Finder, #1), The 39 Steps (Richard Hannay, #1), Trojan Odyssey (Dirk Pitt, #17), Infinity (Chronicles of Nick, #1), Beautiful Darkness (Caster Chronicles, #2), The Ugly Truth (Diary of a Wimpy Kid, #5), The Black Circle (The 39 Clues, #5), Soulless (Parasol Protectorate, #1),\nAverage loss at step  42000 :  3.54164383775\nAverage loss at step  44000 :  3.61175434327\nAverage loss at step  46000 :  3.55967957151\nAverage loss at step  48000 :  3.49804186225\nAverage loss at step  50000 :  3.46732353914\nNearest to The Girl with the Dragon Tattoo (Millennium, #1): Midnight Sun (Twilight, #1.5), James and the Giant Peach, The Graveyard Book, The Sirens of Titan, The Prince of Tides, Purple Cow: Transform Your Business by Being Remarkable, The Mote in God's Eye, Lock and Key,\nNearest to Lord of the Flies: Crime and Punishment, 1984, The Great Gatsby, The Shining (The Shining #1), Zen and the Art of Motorcycle Maintenance: An Inquiry Into Values, The Quiet American, Memories of My Melancholy Whores, The Kite Runner,\nNearest to Bossypants: Miss Peregrine’s Home for Peculiar Children (Miss Peregrine’s Peculiar Children, #1), State of Wonder, The Expats, Barbarians at the Gate: The Fall of RJR Nabisco, In the Garden of Beasts: Love, Terror, and an American Family in Hitler's Berlin, A Trick of the Light (Chief Inspector Armand Gamache, #7), Unfamiliar Fishes, The Paris Wife,\nNearest to City of Bones (The Mortal Instruments, #1): Graceling (Graceling Realm, #1), Chains (Seeds of America, #1), The Help, One Day, Twilight Director's Notebook : The Story of How We Made the Movie Based on the Novel by Stephenie Meyer, Clockwork Angel (The Infernal Devices, #1), City of Glass (The Mortal Instruments, #3), Best Friends Forever,\nNearest to Jane Eyre: Les Misérables, The Lovely Bones, Eat, Pray, Love, Memoirs of a Geisha, The Subtle Knife (His Dark Materials, #2), To Kill a Mockingbird, Beloved, The Importance of Being Earnest,\nNearest to The Lovely Bones: The Secret Life of Bees, Jane Eyre, Uglies (Uglies, #1), The Diary of a Young Girl, The Bookseller of Kabul, The Memory Keeper's Daughter, No Country for Old Men, Middlesex,\nNearest to The Giver (The Giver, #1): The Old Man and the Sea, Life of Pi, Memoirs of a Geisha, Wicked: The Life and Times of the Wicked Witch of the West (The Wicked Years, #1), Animal, Vegetable, Miracle: A Year of Food Life, The Children of Húrin, The Diary of a Young Girl, Catch-22,\nNearest to Atonement: Siddhartha, The Old Man and the Sea, To Kill a Mockingbird, Breakfast of Champions, Lord Loss (The Demonata, #1), Choke, Memoirs of a Geisha, The Secret Life of Bees,\nNearest to The Handmaid's Tale: The Mists of Avalon (Avalon, #1), White Noise, The Hot Zone: The Terrifying True Story of the Origins of the Ebola Virus, The Portrait of a Lady, Three Cups of Tea: One Man's Mission to Promote Peace ... One School at a Time, Fight Club, No Country for Old Men, Notes from Underground, White Nights, The Dream of a Ridiculous Man, and Selections from The House of the Dead,\nNearest to Animal Farm: Haunted, Gone with the Wind, Killing Yourself to Live: 85% of a True Story, Ishmael:A Novel, The Crucible, The Book Thief, Artemis Fowl (Artemis Fowl, #1), Nineteen Minutes,\nNearest to The Ocean at the End of the Lane: The Runaway King (The Ascendance Trilogy, #2), Owl Moon, Lone Wolf, The Ship of Brides, Brain on Fire: My Month of Madness, The Twelve Tribes of Hattie, Seating Arrangements, The Golem and the Jinni (The Golem and the Jinni, #1),\nNearest to Outlander (Outlander, #1): The Mermaid Chair, Ender's Game (Ender's Saga, #1), The Golden Compass (His Dark Materials, #1), Gates of Fire: An Epic Novel of the Battle of Thermopylae, A God in Ruins, Leven Thumps and the Eyes of the Want (Leven Thumps, #3), Zero to One: Notes on Startups, or How to Build the Future, Life of Pi,\nNearest to The Poisonwood Bible: The Plot Against America, The Kite Runner, The Color Purple, Cloud Atlas, Anil's Ghost, Mere Christianity, Jitterbug Perfume, The Historian,\nNearest to 1984: Slaughterhouse-Five, The God Delusion, Lord of the Flies, The Time Traveler's Wife, Lolita, Ender's Game (Ender's Saga, #1), Rosencrantz and Guildenstern Are Dead, A Clockwork Orange,\nNearest to The Devil in the White City: Murder, Magic, and Madness at the Fair That Changed America: The Joke, Slaughterhouse-Five, Interpreter of Maladies, Salt: A World History, The Hammer of Thor (Magnus Chase and the Gods of Asgard, #2), So Long, and Thanks for All the Fish (Hitchhiker's Guide to the Galaxy, #4), The Panther  (John Corey, #6), Please Kill Me: The Uncensored Oral History of Punk,\nNearest to The Maze Runner (Maze Runner, #1): The Body Finder (The Body Finder, #1), The 39 Steps (Richard Hannay, #1), Trojan Odyssey (Dirk Pitt, #17), Infinity (Chronicles of Nick, #1), Beautiful Darkness (Caster Chronicles, #2), The Ugly Truth (Diary of a Wimpy Kid, #5), The Black Circle (The 39 Clues, #5), Matched (Matched, #1),\nAverage loss at step  52000 :  3.43458542252\nAverage loss at step  54000 :  3.4167132166\nAverage loss at step  56000 :  3.39902982748\nAverage loss at step  58000 :  3.47961636269\nAverage loss at step  60000 :  3.46612338716\nNearest to The Girl with the Dragon Tattoo (Millennium, #1): Midnight Sun (Twilight, #1.5), James and the Giant Peach, The Graveyard Book, Lock and Key, The Professor, The Prince of Tides, The Sirens of Titan, Purple Cow: Transform Your Business by Being Remarkable,\nNearest to Lord of the Flies: 1984, Crime and Punishment, The Great Gatsby, Memories of My Melancholy Whores, The Kite Runner, The Quiet American, The Shining (The Shining #1), I Know Why the Caged Bird Sings,\nNearest to Bossypants: Miss Peregrine’s Home for Peculiar Children (Miss Peregrine’s Peculiar Children, #1), Barbarians at the Gate: The Fall of RJR Nabisco, The Expats, State of Wonder, In the Garden of Beasts: Love, Terror, and an American Family in Hitler's Berlin, Unfamiliar Fishes, A Trick of the Light (Chief Inspector Armand Gamache, #7), The Paris Wife,\nNearest to City of Bones (The Mortal Instruments, #1): Graceling (Graceling Realm, #1), Chains (Seeds of America, #1), The Help, One Day, Twilight Director's Notebook : The Story of How We Made the Movie Based on the Novel by Stephenie Meyer, City of Glass (The Mortal Instruments, #3), Best Friends Forever, Clockwork Angel (The Infernal Devices, #1),\nNearest to Jane Eyre: Les Misérables, The Lovely Bones, Memoirs of a Geisha, Eat, Pray, Love, The Subtle Knife (His Dark Materials, #2), To Kill a Mockingbird, Beloved, The Diary of a Young Girl,\nNearest to The Lovely Bones: The Secret Life of Bees, Jane Eyre, The Diary of a Young Girl, Uglies (Uglies, #1), The Memory Keeper's Daughter, Middlesex, The Bookseller of Kabul, No Country for Old Men,\nNearest to The Giver (The Giver, #1): The Old Man and the Sea, Life of Pi, Memoirs of a Geisha, The Children of Húrin, The Diary of a Young Girl, Catch-22, Wicked: The Life and Times of the Wicked Witch of the West (The Wicked Years, #1), Animal, Vegetable, Miracle: A Year of Food Life,\nNearest to Atonement: Siddhartha, The Old Man and the Sea, To Kill a Mockingbird, Breakfast of Champions, Memoirs of a Geisha, The Secret Life of Bees, Lord Loss (The Demonata, #1), Choke,\nNearest to The Handmaid's Tale: The Hot Zone: The Terrifying True Story of the Origins of the Ebola Virus, White Noise, The Mists of Avalon (Avalon, #1), Fight Club, The Portrait of a Lady, Dry, The Tin Drum, No Country for Old Men,\nNearest to Animal Farm: Haunted, Gone with the Wind, Ishmael:A Novel, The Crucible, Artemis Fowl (Artemis Fowl, #1), The Book Thief, Killing Yourself to Live: 85% of a True Story, Into Thin Air: A Personal Account of the Mount Everest Disaster,\nNearest to The Ocean at the End of the Lane: The Runaway King (The Ascendance Trilogy, #2), The Ship of Brides, Lone Wolf, Owl Moon, The Twelve Tribes of Hattie, Brain on Fire: My Month of Madness, Never Fade (The Darkest Minds, #2), Lexicon,\nNearest to Outlander (Outlander, #1): Ender's Game (Ender's Saga, #1), The Mermaid Chair, 1984, Life of Pi, Pilgrim at Tinker Creek, Gates of Fire: An Epic Novel of the Battle of Thermopylae, Reading Lolita in Tehran, The Golden Compass (His Dark Materials, #1),\nNearest to The Poisonwood Bible: The Kite Runner, The Plot Against America, Cloud Atlas, Anil's Ghost, Jitterbug Perfume, The Alchemist, The Color Purple, Mere Christianity,\nNearest to 1984: Slaughterhouse-Five, Lord of the Flies, The Time Traveler's Wife, The God Delusion, Lolita, Ender's Game (Ender's Saga, #1), A Clockwork Orange, Outlander (Outlander, #1),\nNearest to The Devil in the White City: Murder, Magic, and Madness at the Fair That Changed America: The Joke, Slaughterhouse-Five, Salt: A World History, Interpreter of Maladies, The Hammer of Thor (Magnus Chase and the Gods of Asgard, #2), So Long, and Thanks for All the Fish (Hitchhiker's Guide to the Galaxy, #4), The Panther  (John Corey, #6), Please Kill Me: The Uncensored Oral History of Punk,\nNearest to The Maze Runner (Maze Runner, #1): The Body Finder (The Body Finder, #1), The 39 Steps (Richard Hannay, #1), The Ugly Truth (Diary of a Wimpy Kid, #5), Beautiful Darkness (Caster Chronicles, #2), Infinity (Chronicles of Nick, #1), Trojan Odyssey (Dirk Pitt, #17), The Black Circle (The 39 Clues, #5), Torment (Fallen, #2),\nAverage loss at step  62000 :  3.39424202061\nAverage loss at step  64000 :  3.36651503634\nAverage loss at step  66000 :  3.33070478445\nAverage loss at step  68000 :  3.32326644897\nAverage loss at step  70000 :  3.31573734951\nNearest to The Girl with the Dragon Tattoo (Millennium, #1): The Graveyard Book, Midnight Sun (Twilight, #1.5), James and the Giant Peach, Lock and Key, The Professor, The Sirens of Titan, Islands in the Stream, Purple Cow: Transform Your Business by Being Remarkable,\nNearest to Lord of the Flies: 1984, Crime and Punishment, The Great Gatsby, The Kite Runner, Memories of My Melancholy Whores, The Quiet American, Fahrenheit 451, The Time Traveler's Wife,\nNearest to Bossypants: Barbarians at the Gate: The Fall of RJR Nabisco, Miss Peregrine’s Home for Peculiar Children (Miss Peregrine’s Peculiar Children, #1), The Expats, In the Garden of Beasts: Love, Terror, and an American Family in Hitler's Berlin, State of Wonder, Unfamiliar Fishes, The Bedwetter: Stories of Courage, Redemption, and Pee, A Trick of the Light (Chief Inspector Armand Gamache, #7),\nNearest to City of Bones (The Mortal Instruments, #1): Graceling (Graceling Realm, #1), The Help, City of Glass (The Mortal Instruments, #3), Chains (Seeds of America, #1), Twilight Director's Notebook : The Story of How We Made the Movie Based on the Novel by Stephenie Meyer, One Day, Best Friends Forever, Frostbite (Vampire Academy, #2),\nNearest to Jane Eyre: The Lovely Bones, Memoirs of a Geisha, Les Misérables, Eat, Pray, Love, To Kill a Mockingbird, Beloved, A Thousand Splendid Suns, The Subtle Knife (His Dark Materials, #2),\nNearest to The Lovely Bones: The Secret Life of Bees, Jane Eyre, Uglies (Uglies, #1), The Memory Keeper's Daughter, The Diary of a Young Girl, The Bookseller of Kabul, Middlesex, The Perks of Being a Wallflower,\nNearest to The Giver (The Giver, #1): The Old Man and the Sea, Life of Pi, Memoirs of a Geisha, The Children of Húrin, Catch-22, Me Talk Pretty One Day, Wicked: The Life and Times of the Wicked Witch of the West (The Wicked Years, #1), The Diary of a Young Girl,\nNearest to Atonement: The Old Man and the Sea, Siddhartha, To Kill a Mockingbird, Breakfast of Champions, The Secret Life of Bees, Memoirs of a Geisha, Beloved, Lord Loss (The Demonata, #1),\nNearest to The Handmaid's Tale: The Hot Zone: The Terrifying True Story of the Origins of the Ebola Virus, The Mists of Avalon (Avalon, #1), White Noise, Fight Club, The Tin Drum, The Book Thief, The Crying of Lot 49, Dry,\nNearest to Animal Farm: Haunted, Gone with the Wind, Ishmael:A Novel, The Book Thief, Civil Disobedience and Other Essays, Artemis Fowl (Artemis Fowl, #1), The Crucible, Killing Yourself to Live: 85% of a True Story,\nNearest to The Ocean at the End of the Lane: The Runaway King (The Ascendance Trilogy, #2), The Ship of Brides, The Twelve Tribes of Hattie, Lone Wolf, Owl Moon, Never Fade (The Darkest Minds, #2), The Golem and the Jinni (The Golem and the Jinni, #1), Lexicon,\nNearest to Outlander (Outlander, #1): The Mermaid Chair, Ender's Game (Ender's Saga, #1), Pilgrim at Tinker Creek, 1984, Gates of Fire: An Epic Novel of the Battle of Thermopylae, Life of Pi, Cloud Atlas, Zero to One: Notes on Startups, or How to Build the Future,\nNearest to The Poisonwood Bible: The Kite Runner, Cloud Atlas, The Plot Against America, The Color Purple, Anil's Ghost, Jitterbug Perfume, The Alchemist, Mere Christianity,\nNearest to 1984: Lord of the Flies, Slaughterhouse-Five, The Time Traveler's Wife, The God Delusion, Ender's Game (Ender's Saga, #1), Lolita, The Hobbit, Sense and Sensibility,\nNearest to The Devil in the White City: Murder, Magic, and Madness at the Fair That Changed America: The Joke, Salt: A World History, Slaughterhouse-Five, Interpreter of Maladies, The Hammer of Thor (Magnus Chase and the Gods of Asgard, #2), So Long, and Thanks for All the Fish (Hitchhiker's Guide to the Galaxy, #4), Please Kill Me: The Uncensored Oral History of Punk, The Panther  (John Corey, #6),\nNearest to The Maze Runner (Maze Runner, #1): The Ugly Truth (Diary of a Wimpy Kid, #5), The Body Finder (The Body Finder, #1), Beautiful Darkness (Caster Chronicles, #2), The 39 Steps (Richard Hannay, #1), Infinity (Chronicles of Nick, #1), The Black Circle (The 39 Clues, #5), I Don't Know How She Does It, Torment (Fallen, #2),\nAverage loss at step  72000 :  3.37959288603\nAverage loss at step  74000 :  3.4224101547\nAverage loss at step  76000 :  3.32037075186\nAverage loss at step  78000 :  3.29888544989\nAverage loss at step  80000 :  3.26403364515\nNearest to The Girl with the Dragon Tattoo (Millennium, #1): James and the Giant Peach, Midnight Sun (Twilight, #1.5), The Graveyard Book, The Professor, Lock and Key, The Mote in God's Eye, The Sirens of Titan, The Prince of Tides,\nNearest to Lord of the Flies: 1984, The Great Gatsby, Crime and Punishment, Memories of My Melancholy Whores, The Kite Runner, Slaughterhouse-Five, Fahrenheit 451, The Shining (The Shining #1),\nNearest to Bossypants: Barbarians at the Gate: The Fall of RJR Nabisco, Miss Peregrine’s Home for Peculiar Children (Miss Peregrine’s Peculiar Children, #1), The Expats, Pathfinder (Pathfinder, #1), Unfamiliar Fishes, State of Wonder, Save Me, The Bedwetter: Stories of Courage, Redemption, and Pee,\nNearest to City of Bones (The Mortal Instruments, #1): Graceling (Graceling Realm, #1), City of Glass (The Mortal Instruments, #3), The Help, Chains (Seeds of America, #1), Marked (House of Night, #1), Best Friends Forever, Twilight Director's Notebook : The Story of How We Made the Movie Based on the Novel by Stephenie Meyer, Frostbite (Vampire Academy, #2),\nNearest to Jane Eyre: The Lovely Bones, Memoirs of a Geisha, To Kill a Mockingbird, Les Misérables, Eat, Pray, Love, Beloved, The Diary of a Young Girl, A Thousand Splendid Suns,\nNearest to The Lovely Bones: The Secret Life of Bees, Jane Eyre, Uglies (Uglies, #1), The Memory Keeper's Daughter, The Diary of a Young Girl, Middlesex, The Bookseller of Kabul, The Perks of Being a Wallflower,\nNearest to The Giver (The Giver, #1): The Old Man and the Sea, Memoirs of a Geisha, Life of Pi, The Children of Húrin, Wicked: The Life and Times of the Wicked Witch of the West (The Wicked Years, #1), The Lovely Bones, The Diary of a Young Girl, Me Talk Pretty One Day,\nNearest to Atonement: To Kill a Mockingbird, The Old Man and the Sea, Siddhartha, The Secret Life of Bees, Breakfast of Champions, Choke, Memoirs of a Geisha, Beloved,\nNearest to The Handmaid's Tale: The Hot Zone: The Terrifying True Story of the Origins of the Ebola Virus, The Mists of Avalon (Avalon, #1), The Tin Drum, Fight Club, White Noise, The Book Thief, The Crying of Lot 49, No Country for Old Men,\nNearest to Animal Farm: Haunted, Gone with the Wind, Ishmael:A Novel, Civil Disobedience and Other Essays, The Crucible, Artemis Fowl (Artemis Fowl, #1), Into Thin Air: A Personal Account of the Mount Everest Disaster, Killing Yourself to Live: 85% of a True Story,\nNearest to The Ocean at the End of the Lane: The Runaway King (The Ascendance Trilogy, #2), The Twelve Tribes of Hattie, The Ship of Brides, Lone Wolf, The Golem and the Jinni (The Golem and the Jinni, #1), Anna Karenina, Owl Moon, Seating Arrangements,\nNearest to Outlander (Outlander, #1): The Mermaid Chair, Pilgrim at Tinker Creek, Life of Pi, 1984, Gates of Fire: An Epic Novel of the Battle of Thermopylae, Ender's Game (Ender's Saga, #1), Extremely Loud and Incredibly Close, The Miniaturist,\nNearest to The Poisonwood Bible: The Plot Against America, The Kite Runner, Cloud Atlas, Anil's Ghost, The Color Purple, Jitterbug Perfume, Lolita, The Alchemist,\nNearest to 1984: Slaughterhouse-Five, Lord of the Flies, The Time Traveler's Wife, Life of Pi, The God Delusion, Lolita, Outlander (Outlander, #1), A Clockwork Orange,\nNearest to The Devil in the White City: Murder, Magic, and Madness at the Fair That Changed America: The Joke, Salt: A World History, Interpreter of Maladies, Slaughterhouse-Five, The Hammer of Thor (Magnus Chase and the Gods of Asgard, #2), Please Kill Me: The Uncensored Oral History of Punk, So Long, and Thanks for All the Fish (Hitchhiker's Guide to the Galaxy, #4), Night (The Night Trilogy #1),\nNearest to The Maze Runner (Maze Runner, #1): The Ugly Truth (Diary of a Wimpy Kid, #5), The Body Finder (The Body Finder, #1), Beautiful Darkness (Caster Chronicles, #2), Infinity (Chronicles of Nick, #1), I Don't Know How She Does It, The 39 Steps (Richard Hannay, #1), The Black Circle (The 39 Clues, #5), Torment (Fallen, #2),\nAverage loss at step  82000 :  3.25146297926\nAverage loss at step  84000 :  3.25321617091\nAverage loss at step  86000 :  3.29557959634\nAverage loss at step  88000 :  3.39157348788\nAverage loss at step  90000 :  3.27380904102\nNearest to The Girl with the Dragon Tattoo (Millennium, #1): James and the Giant Peach, Midnight Sun (Twilight, #1.5), The Professor, The Graveyard Book, Lock and Key, Islands in the Stream, The Prince of Tides, Nefertiti,\nNearest to Lord of the Flies: 1984, The Great Gatsby, The Kite Runner, Crime and Punishment, Memories of My Melancholy Whores, Slaughterhouse-Five, The Time Traveler's Wife, Fahrenheit 451,\nNearest to Bossypants: Barbarians at the Gate: The Fall of RJR Nabisco, The Expats, Miss Peregrine’s Home for Peculiar Children (Miss Peregrine’s Peculiar Children, #1), Unfamiliar Fishes, In the Garden of Beasts: Love, Terror, and an American Family in Hitler's Berlin, Teacher Man (Frank McCourt, #3), The Paris Wife, Save Me,\nNearest to City of Bones (The Mortal Instruments, #1): Graceling (Graceling Realm, #1), City of Glass (The Mortal Instruments, #3), The Help, Chains (Seeds of America, #1), Marked (House of Night, #1), Best Friends Forever, Frostbite (Vampire Academy, #2), Twilight Director's Notebook : The Story of How We Made the Movie Based on the Novel by Stephenie Meyer,\nNearest to Jane Eyre: The Lovely Bones, Memoirs of a Geisha, To Kill a Mockingbird, Les Misérables, Eat, Pray, Love, Beloved, A Thousand Splendid Suns, The Diary of a Young Girl,\nNearest to The Lovely Bones: The Secret Life of Bees, Jane Eyre, The Memory Keeper's Daughter, Uglies (Uglies, #1), The Diary of a Young Girl, The Perks of Being a Wallflower, The Bookseller of Kabul, Middlesex,\nNearest to The Giver (The Giver, #1): The Old Man and the Sea, Memoirs of a Geisha, Life of Pi, The Children of Húrin, The Lovely Bones, Catch-22, The Diary of a Young Girl, Me Talk Pretty One Day,\nNearest to Atonement: To Kill a Mockingbird, The Old Man and the Sea, Siddhartha, The Secret Life of Bees, Breakfast of Champions, Memoirs of a Geisha, Beloved, Jane Eyre,\nNearest to The Handmaid's Tale: The Hot Zone: The Terrifying True Story of the Origins of the Ebola Virus, The Book Thief, The Tin Drum, Fight Club, White Noise, The Mists of Avalon (Avalon, #1), The Crying of Lot 49, Winter's Tale,\nNearest to Animal Farm: Haunted, Gone with the Wind, The Book Thief, Ishmael:A Novel, The Fellowship of the Ring (The Lord of the Rings, #1), Into Thin Air: A Personal Account of the Mount Everest Disaster, Artemis Fowl (Artemis Fowl, #1), Les Misérables,\nNearest to The Ocean at the End of the Lane: The Runaway King (The Ascendance Trilogy, #2), The Ship of Brides, The Twelve Tribes of Hattie, Lone Wolf, Never Fade (The Darkest Minds, #2), The Golem and the Jinni (The Golem and the Jinni, #1), Anna Karenina, Lexicon,\nNearest to Outlander (Outlander, #1): 1984, Pilgrim at Tinker Creek, The Mermaid Chair, Life of Pi, Ender's Game (Ender's Saga, #1), Gates of Fire: An Epic Novel of the Battle of Thermopylae, The Namesake, Extremely Loud and Incredibly Close,\nNearest to The Poisonwood Bible: The Kite Runner, The Plot Against America, Cloud Atlas, Anil's Ghost, Lolita, The Alchemist, Jitterbug Perfume, The Color Purple,\nNearest to 1984: Lord of the Flies, Slaughterhouse-Five, Lolita, The Time Traveler's Wife, Outlander (Outlander, #1), Life of Pi, A Clockwork Orange, Ender's Game (Ender's Saga, #1),\nNearest to The Devil in the White City: Murder, Magic, and Madness at the Fair That Changed America: The Joke, Salt: A World History, Interpreter of Maladies, Slaughterhouse-Five, The Hammer of Thor (Magnus Chase and the Gods of Asgard, #2), So Long, and Thanks for All the Fish (Hitchhiker's Guide to the Galaxy, #4), Please Kill Me: The Uncensored Oral History of Punk, Night (The Night Trilogy #1),\nNearest to The Maze Runner (Maze Runner, #1): The Ugly Truth (Diary of a Wimpy Kid, #5), Beautiful Darkness (Caster Chronicles, #2), The Body Finder (The Body Finder, #1), Infinity (Chronicles of Nick, #1), I Don't Know How She Does It, The 39 Steps (Richard Hannay, #1), The Black Circle (The 39 Clues, #5), Torment (Fallen, #2),\nAverage loss at step  92000 :  3.245428859\nAverage loss at step  94000 :  3.21859055138\nAverage loss at step  96000 :  3.20974324721\nAverage loss at step  98000 :  3.19917794842\nAverage loss at step  100000 :  3.22653954786\nNearest to The Girl with the Dragon Tattoo (Millennium, #1): James and the Giant Peach, Midnight Sun (Twilight, #1.5), The Graveyard Book, The Professor, Lock and Key, The Sirens of Titan, Purple Cow: Transform Your Business by Being Remarkable, Inkdeath (Inkworld, #3),\nNearest to Lord of the Flies: 1984, The Great Gatsby, The Kite Runner, Crime and Punishment, Memories of My Melancholy Whores, Fahrenheit 451, Slaughterhouse-Five, The Time Traveler's Wife,\nNearest to Bossypants: Barbarians at the Gate: The Fall of RJR Nabisco, Pathfinder (Pathfinder, #1), The Expats, Miss Peregrine’s Home for Peculiar Children (Miss Peregrine’s Peculiar Children, #1), Unfamiliar Fishes, Teacher Man (Frank McCourt, #3), Save Me, In the Garden of Beasts: Love, Terror, and an American Family in Hitler's Berlin,\nNearest to City of Bones (The Mortal Instruments, #1): Graceling (Graceling Realm, #1), City of Glass (The Mortal Instruments, #3), Marked (House of Night, #1), The Help, Best Friends Forever, Chains (Seeds of America, #1), Frostbite (Vampire Academy, #2), Pride and Prejudice and Zombies (Pride and Prejudice and Zombies, #1),\nNearest to Jane Eyre: Memoirs of a Geisha, The Lovely Bones, Beloved, To Kill a Mockingbird, Eat, Pray, Love, A Thousand Splendid Suns, Les Misérables, The Diary of a Young Girl,\nNearest to The Lovely Bones: The Memory Keeper's Daughter, The Secret Life of Bees, Jane Eyre, Uglies (Uglies, #1), The Diary of a Young Girl, The Perks of Being a Wallflower, The Bookseller of Kabul, The Giver (The Giver, #1),\nNearest to The Giver (The Giver, #1): The Old Man and the Sea, Memoirs of a Geisha, Life of Pi, Catch-22, The Children of Húrin, The Lovely Bones, The Diary of a Young Girl, Me Talk Pretty One Day,\nNearest to Atonement: The Old Man and the Sea, To Kill a Mockingbird, Siddhartha, The Secret Life of Bees, Breakfast of Champions, Beloved, Memoirs of a Geisha, Choke,\nNearest to The Handmaid's Tale: The Hot Zone: The Terrifying True Story of the Origins of the Ebola Virus, The Tin Drum, Fight Club, Winter's Tale, The Book Thief, White Noise, Running with Scissors, The Mists of Avalon (Avalon, #1),\nNearest to Animal Farm: Haunted, Gone with the Wind, Ishmael:A Novel, Civil Disobedience and Other Essays, Artemis Fowl (Artemis Fowl, #1), Les Misérables, The Fellowship of the Ring (The Lord of the Rings, #1), Into Thin Air: A Personal Account of the Mount Everest Disaster,\nNearest to The Ocean at the End of the Lane: The Runaway King (The Ascendance Trilogy, #2), The Twelve Tribes of Hattie, Lone Wolf, Never Fade (The Darkest Minds, #2), The Ship of Brides, Anna Karenina, The Golem and the Jinni (The Golem and the Jinni, #1), Promise of Blood (Powder Mage, #1),\nNearest to Outlander (Outlander, #1): 1984, Pilgrim at Tinker Creek, The Mermaid Chair, Ender's Game (Ender's Saga, #1), The Namesake, Life of Pi, Gates of Fire: An Epic Novel of the Battle of Thermopylae, Cloud Atlas,\nNearest to The Poisonwood Bible: The Kite Runner, Cloud Atlas, The Plot Against America, Anil's Ghost, The Omnivore's Dilemma: A Natural History of Four Meals, The Color Purple, The Alchemist, Lolita,\nNearest to 1984: Lord of the Flies, Slaughterhouse-Five, The Time Traveler's Wife, Outlander (Outlander, #1), Ender's Game (Ender's Saga, #1), Lolita, A Clockwork Orange, Life of Pi,\nNearest to The Devil in the White City: Murder, Magic, and Madness at the Fair That Changed America: The Joke, Salt: A World History, Interpreter of Maladies, Slaughterhouse-Five, The Hammer of Thor (Magnus Chase and the Gods of Asgard, #2), So Long, and Thanks for All the Fish (Hitchhiker's Guide to the Galaxy, #4), Night (The Night Trilogy #1), Please Kill Me: The Uncensored Oral History of Punk,\nNearest to The Maze Runner (Maze Runner, #1): The Ugly Truth (Diary of a Wimpy Kid, #5), Beautiful Darkness (Caster Chronicles, #2), I Don't Know How She Does It, The Body Finder (The Body Finder, #1), Infinity (Chronicles of Nick, #1), The 39 Steps (Richard Hannay, #1), Torment (Fallen, #2), The Black Circle (The 39 Clues, #5),\n\n\n\nfinal_embeddings.shape\n\n\n\n(9951, 128)"
					}
					
				
			
		
			
				
					,
					
					"recommender-deep-learning": {
						"id": "recommender-deep-learning",
						"title": "Deep Neural Networds",
						"categories": "Recommender",
						"url": " /recommender/Deep-Learning/",
						"content": ""
					}
					
				
			
		
			
				
					,
					
					"recommender-collaborative-filtering": {
						"id": "recommender-collaborative-filtering",
						"title": "Collaborative Filtering",
						"categories": "Recommender",
						"url": " /recommender/Collaborative-Filtering/",
						"content": ""
					}
					
				
			
		
			
				
					,
					
					"machine-learning-machine-learning-recap": {
						"id": "machine-learning-machine-learning-recap",
						"title": "Machine Learning Recap",
						"categories": "Machine-Learning",
						"url": " /machine-learning/machine-learning-recap/",
						"content": "No Free Lunch Theorem\n\nBasically, No Free Lunch Theorem states that there is no methods which outperform all others on all tasks.\n\nThe reason behind is that every method relies on certain assumptions about data or task. If these assumptions fail, it will perform poorly.\n\nFor us, this means that we cannot every competition with just a single algorithm. So it is important for us to have a clear mind map of various models based off different assumptions.\n\nThen let’s start getting familiar with the four popular families of machine learning algorithms.\n\nLinear Model\n\nLinear models try to separate objects with a plane which divides space into two parts.\n\nWith 2 sets of points, it is quite intuitive to separate them using a line. This approach can be generalized for a higher dimensional space. This is the main idea behind linear models.\n\nLogistic regression or SVM are all linear models withdifferent loss functions.\n\nLinear models are especially good for sparse high dimensional data.\n\nTree-Based Methods\n\nIn general, tree-based models are very powerful and can be a good default method for tabular data.\n\nIntuitively, a single decision tree can be considered as dividing space into boxes and approximating data with a constant inside of these boxes. It uses divide-and-conquer approach to recur sub-split spaces into sub-spaces.\n\nThe way of true axis splits and corresponding constants produces several approaches for building decision trees. Moreover, such trees can be combined together in a lot of ways. All this leads to a wide variety of tree-based algorithms, most famous of them being random forest and Gradient Boosted Decision Trees.\n\nScikit-Learn contains quite good implementation of random forest. For gradient boost decision trees, you may find XGBoost and LightGBM with higher speed and accuracy.\n\nk-Nearest Neighbors (k-NN)\n\nThe intuition behind k-NN is very simple. Closer objects will likely to have same labels.\n\nNeural Networks\n\nNeural Nets is a special class of machine learning models, which deserve a separate topic."
					}
					
				
			
		
			
				
					,
					
					"machine-learning-dm-stat-ml": {
						"id": "machine-learning-dm-stat-ml",
						"title": "Data mining, statistics, machine learning and AI",
						"categories": "Machine-Learning",
						"url": " /machine-learning/dm-stat-ml/",
						"content": "Artificial Intelligence is the study of how to create intelligent agents. Most tasks that require intelligence require an ability to induce new knowledge from experiences. Thus, a large area within AI is machine learning. Procedures in machine learning include ideas derived directly from, or inspired by, classical statistics, but they don’t have to be. Data mining is an area that has taken much of its inspiration and techniques from machine learning (and some, also, from statistics), but aims at either to discover / generate some preliminary insights in an area where there really was little knowledge beforehand, or to be able to predict future observations accurately.\n\nThere are considerable overlaps among them. If we have to make some distinctions, they are:\n\nStatistics is about numbers. It is mostly employed towards better understanding particular data generating processes. Thus, it usually starts with a formally specified model, and from this are derived procedures to accurately extract that model from noisy instances (i.e., estimation–by optimizing some loss function) and to be able to distinguish it from other possibilities (i.e., inferences based on known properties of sampling distributions). The prototypical statistical technique is regression.\n\nData Mining is about using Statistics as well as other programming methods to find patterns hidden in the data so that you can explain some phenomenon. Data Mining builds intuition about what is really happening in some data and is still little more towards math than programming, but uses both. Common data mining techniques would include cluster analyses, classification and regression trees, and neural networks.\n\nMachine Learning uses Data Mining techniques and other learning algorithms to build models of what is happening behind some data so that it can predict future outcomes. Math is the basis for many of the algorithms, but this is more towards programming. A computer program is said to learn some task from experience if its performance at the task improves with experience, according to some performance measure. Machine learning involves the study of algorithms that can extract information automatically (i.e., without on-line human guidance).\n\nArtificial Intelligence uses models built by Machine Learning and other ways to reason about the world and give rise to intelligent behavior whether this is playing a game or driving a robot/car. Artificial Intelligence has some goal to achieve by predicting how actions will affect the model of the world and chooses the actions that will best achieve that goal. Very programming based."
					}
					
				
			
		
			
				
					,
					
					"machine-learning-cv": {
						"id": "machine-learning-cv",
						"title": "Test set vs. Validation set",
						"categories": "Machine-Learning",
						"url": " /machine-learning/cv/",
						"content": "When you have a large data set, it’s recommended to split it into 3 parts:\n\n\n  \n    Training set: This is used to build up our prediction algorithm. Our algorithm tries to tune itself to the quirks of the training data sets. In this phase we usually create multiple algorithms in order to compare their performances during the Cross-Validation Phase.\n  \n  \n    Validation set: This data set is used to compare the performances of the prediction algorithms that were created based on the training set. We choose the algorithm that has the best performance.\n  \n  \n    Test set: Now we have chosen our preferred prediction algorithm but we don’t know yet how it’s going to perform on completely unseen real-world data. So, we apply our chosen prediction algorithm on our test set in order to see how it’s going to perform so we can have an idea about our algorithm’s performance on unseen data.\n  \n\n\nNotes:\n\n-It’s very important to keep in mind that skipping the test phase is not recommended, because the algorithm that performed well during the validation phase doesn’t really mean that it’s truly the best one, because the algorithms are compared based on the cross-validation set and its quirks and noises.\n\n-During the Test Phase, the purpose is to see how our final model is going to deal in the wild, so in case its performance is very poor we should repeat the whole process starting from the Training Phase."
					}
					
				
			
		
			
				
					,
					
					"machine-learning-bias-variance-tradeoff": {
						"id": "machine-learning-bias-variance-tradeoff",
						"title": "Bias Variance Tradeoff",
						"categories": "Machine-Learning",
						"url": " /machine-learning/bias-variance-Tradeoff/",
						"content": "In short,\n\n  bias is how far away a model’s predictions are from true values,\n  variance is how scattered these predictions are among model iterations.\n\n\n\n\n\n\n  Error due to Bias: The error due to bias is taken as the difference between the expected (or average) prediction of our model and the correct value which we are trying to predict. Of course you only have one model so talking about expected or average prediction values might seem a little strange. However, imagine you could repeat the whole model building process more than once: each time you gather new data and run a new analysis creating a new model. Due to randomness in the underlying data sets, the resulting models will have a range of predictions. Bias measures how far off in general these models’ predictions are from the correct value.\nScott Fortmann-Roe\n\n\n\n\n  Error due to Variance: The error due to variance is taken as the variability of a model prediction for a given data point. Again, imagine you can repeat the entire model building process multiple times. The variance is how much the predictions for a given point vary between different realizations of the model.\nScott Fortmann-Roe"
					}
					
				
			
		
			
				
					,
					
					"machine-learning-generative-discriminative": {
						"id": "machine-learning-generative-discriminative",
						"title": "Generative vs. Discriminative",
						"categories": "Machine-Learning",
						"url": " /machine-learning/Generative-Discriminative/",
						"content": "The fundamental distinction between discriminative models and generative models is:\n\n\n  Discriminative models learn the (hard or soft) boundary between classes\n  Generative models model the distribution of individual classes\n\n\nDiscriminative\n\nSVMs and decision trees are discriminative because they learn explicit boundaries between classes.\n\nSVM is a maximal margin classifier, meaning that it learns a decision boundary that maximizes the distance between samples of the two classes, given a kernel. The distance between a sample and the learned decision boundary can be used to make the SVM a “soft” classifier.\n\nDecision trees learn the decision boundary by recursively partitioning the space in a manner that maximizes the information gain (or another criterion).\n\nGenerative\n\nGenerative models are typically specified as probabilistic graphical models. They offer rich representations of the independence relations in the dataset.\n\nDiscriminative models do not offer such clear representations of relations between features and classes in the dataset. Instead of using resources to fully model each class, they focus on richly modeling the boundary between classes. Given the same amount of capacity (say, bits in a computer program executing the model), a discriminative model thus may yield more complex representations of this boundary than a generative model.\n\nWhen you are dealing with non-stationary distributions where the online test data may be generated by different underlying distributions than the training data, it is typically more straightforward to detect distribution changes and update a generative model accordingly than do this for a decision boundary in an SVM, especially if the online updates need to be unsupervised.\n\nTo sum up:\n\n\n  \n    \n      Discriminative models\n      Generative models\n    \n  \n  \n    \n      learn the (hard or soft) boundary between classes\n      model the distribution of individual classes\n    \n    \n      generally do not function for outlier detection\n      generally function for outlier detection\n    \n    \n      do not offer clear representations of relations between features and classes in the dataset\n      offer rich representations of the independence relations in the dataset"
					}
					
				
			
		
			
				
					,
					
					"machine-learning-ensemble-methods": {
						"id": "machine-learning-ensemble-methods",
						"title": "Ensemble Learning",
						"categories": "Machine-Learning",
						"url": " /machine-learning/Ensemble-methods/",
						"content": "Bagging and boosting are two families of ensemble methods.\n\nEnsemble methods aim at combining the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.\n\nBagging(short for Bootstrap Aggregating):\n\n\n  build several base estimators independently and then to average their predictions.\n  aim to decrease variance by generating additional data for training from the original dataset\n  suitable for models with high variance low bias (complex models)\n  Examples: Random forest, which develop fully grown trees (note that RF modifies the grown procedure to reduce the correlation between trees)\n\n\nBoosting:\n\n\n  build several base estimators sequentialy and one tries to reduce the bias of the combined estimator\n  aim to decrease bias\n  suitable for models with low variance high bias\n  Examples:  Gradient boosting"
					}
					
				
			
		
			
				
					,
					
					"machine-learning-decision-tree": {
						"id": "machine-learning-decision-tree",
						"title": "Decision Trees",
						"categories": "Machine-Learning",
						"url": " /machine-learning/Decision-Tree/",
						"content": "Decision tree classifiers are incredibly simple in theory. In their simplest form, decision tree classifiers ask a series of Yes/No questions about the data — each time getting closer to finding out the class of each entry — until they either classify the data set perfectly or simply can’t differentiate a set of entries. Think of it like a game of Twenty Questions, except the computer is much, much better at it.\n\nThe nice part about decision tree classifiers is that they are scale-invariant, i.e., the scale of the features does not affect their performance.\n\nA common problem that decision trees face is that they’re prone to overfitting: They complexify to the point that they classify the training set near-perfectly, but fail to generalize to data they have not seen before.\nRandom Forest classifiers work around that limitation by creating a whole bunch of decision trees (hence “forest”) — each trained on random subsets of training samples (drawn with replacement) and features (drawn without replacement) — and have the decision trees work together to make a more accurate classification.\n\nRandom Forest\n\nIn RandomForest model we average n similar performing trees (“forest”), trained independently. So two RF with 1000 trees is essentially the same as single RF model with 2000 trees.\n\nGradient Boosted Decision Trees\n\nIn GBDT model we have sequence of trees, each improve predictions of all previous.\n\nSo taking other settings the same, dropping the n-th tree has different effects on these two models.\n\nFor Random Forest, the order of trees does not matter in RandomForest and performance drop will be very similar on average.\n\nFor GBDT:\n\n  if we drop first tree, sum of all the rest trees will be biased and overall performance should drop.\n  if we drop the last tree, sum of all previous tree won’t be affected, and therefore performance will change insignificantly (in case we have enough trees)"
					}
					
				
			
		
			
				
					,
					
					"machine-learning-cross-validation": {
						"id": "machine-learning-cross-validation",
						"title": "Cross Validation",
						"categories": "Machine-Learning",
						"url": " /machine-learning/Cross-Validation/",
						"content": "What is cross-validation?\n\nIt’s a model validation technique for assessing how the results of a statistical analysis will generalize to an independent data set.\n\nIt’s common to find that our model performs differently depending on the subset of the data it’s trained on. This phenomenon is known as overfitting: The model is learning to classify the training set so well that it doesn’t generalize and perform well on data it hasn’t seen before.\n\nSo we split the original data set into k subsets, use one of the subsets as the testing set, and the rest of the subsets are used as the training set. This process is then repeated k times such that each subset is used as the testing set exactly once.\n\nThis process is what we called cross-validation.\n\nExamples: leave-one-out cross validation, K-fold cross validation\n\nHow to do it right?\n\nFirstly, the training and validation data sets should be drawn from the same population.\n\npredicting stock prices: trained for a certain 5-year period, it’s unrealistic to treat the subsequent 5-year a draw from the same population\ncommon mistake: for instance the step of choosing the kernel parameters of a SVM should be cross-validated as well\n\nBias-variance trade-off for k-fold cross validation\n\nLeave-one-out cross-validation: gives approximately unbiased estimates of the test error since each training set contains almost the entire data set (n−1 observations).\n\nBut: we average the outputs of n fitted models, each of which is trained on an almost identical set of observations hence the outputs are highly correlated. Since the variance of a mean of quantities increases when correlation of these quantities increase, the test error estimate from a LOOCV has higher variance than the one obtained with k-fold cross validation\n\nTypically, we choose k=5 or k=10, as these values have been shown empirically to yield test error estimates that suffer neither from excessively high bias nor high variance.\n\nRobust or accurate algorithms, how do you choose?\n\nSimpler models are preferred if more complex models do not significantly improve the quality of the description for the observations.\n\nOur ultimate goal is to design systems with good generalization capacity, that is, systems that correctly identify patterns in data instances not seen before. While the generalization performance of a learning system strongly depends on the complexity of the model assumed.\n\nIf the model is too simple, the system can only capture the actual data regularities in a rough manner. In this case, the system has poor generalization properties and is said to suffer from underfitting.\n\nBy contrast, if the model is too complex, the system can identify accidental patterns in the training data that need not be present in the test set. These spurious patterns can be the result of random fluctuations or of measurement errors during the data collection process. In this case, the generalization capacity of the learning system is also poor. The learning system is said to be affected by overfitting. Spurious patterns, which are only present by accident in the data, tend to have complex forms.\n\nBy the way, ensemble learning can help balancing bias/variance. Several weak learners together = strong learner.\n\nHow do you select metrics?\n\nClassification\n\n\n  \n    Recall / Sensitivity / True positive rate\n  \n  \n    Precision / Positive Predictive value\n  \n  \n    Specificity / True negative rate\n  \n  \n    Accuracy\n  \n  \n    ROC / AUC\n  \n\n\nROC is a graphical plot that illustrates the performance of a binary classifier (SensitivitySensitivity Vs 1−Specificity1−Specificity or SensitivitySensitivity Vs SpecificitySpecificity). They are not sensitive to unbalanced classes.\nAUC is the area under the ROC curve. Perfect classifier: AUC=1, fall on (0,1); 100% sensitivity (no FN) and 100% specificity (no FP)\n\n\n  Logarithmic loss\n\n\nPunishes infinitely the deviation from the true value! It’s better to be somewhat wrong than emphatically wrong!\n\n\n  \n    Misclassification Rate\n  \n  \n    F1-Score\n  \n\n\nRegression\n\n\n  Mean Squared Error Vs Mean Absolute Error\n\n\nRMSE gives a relatively high weight to large errors. The RMSE is most useful when large errors are particularly undesirable.\n\nThe MAE is a linear score: all the individual differences are weighted equally in the average. MAE is more robust to outliers than MSE.\n\n\n  Root Mean Squared Logarithmic Error\n\n\nRMSLE penalizes an under-predicted estimate greater than an over-predicted estimate (opposite to RMSE)\n\n\n  Weighted Mean Absolute Error\n\n\nThe weighted average of absolute errors. MAE and RMSE consider that each prediction provides equally precise information about the error variation, i.e. the standard variation of the error term is constant over all the predictions. Examples: recommender systems (differences between past and recent products)"
					}
					
				
			
		
	};
</script>
<script src="/js/lunr.min.js"></script>
<script src="/js/search.js"></script>
			</div>
		</section>

		<footer>
	<div class="wrapper">
		<p class="edit-footer"><a class="editor-link btn" href="cloudcannon:collections/_data/footer.yml" class="btn" style="padding: 5px;"><strong>&#9998;</strong> Edit footer</a></p>
		<ul class="footer-links">
			
				<li><a target="_blank" href="https://facebook.com/" class="Facebook-icon">
					
						
		<svg class="facebook" fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M19,4V7H17A1,1 0 0,0 16,8V10H19V13H16V20H13V13H11V10H13V7.5C13,5.56 14.57,4 16.5,4M20,2H4A2,2 0 0,0 2,4V20A2,2 0 0,0 4,22H20A2,2 0 0,0 22,20V4C22,2.89 21.1,2 20,2Z" /></svg>
	

					
					</a></li>
			
				<li><a target="_blank" href="https://twitter.com/" class="Twitter-icon">
					
						
		<svg class="twitter" fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M22.46,6C21.69,6.35 20.86,6.58 20,6.69C20.88,6.16 21.56,5.32 21.88,4.31C21.05,4.81 20.13,5.16 19.16,5.36C18.37,4.5 17.26,4 16,4C13.65,4 11.73,5.92 11.73,8.29C11.73,8.63 11.77,8.96 11.84,9.27C8.28,9.09 5.11,7.38 3,4.79C2.63,5.42 2.42,6.16 2.42,6.94C2.42,8.43 3.17,9.75 4.33,10.5C3.62,10.5 2.96,10.3 2.38,10C2.38,10 2.38,10 2.38,10.03C2.38,12.11 3.86,13.85 5.82,14.24C5.46,14.34 5.08,14.39 4.69,14.39C4.42,14.39 4.15,14.36 3.89,14.31C4.43,16 6,17.26 7.89,17.29C6.43,18.45 4.58,19.13 2.56,19.13C2.22,19.13 1.88,19.11 1.54,19.07C3.44,20.29 5.7,21 8.12,21C16,21 20.33,14.46 20.33,8.79C20.33,8.6 20.33,8.42 20.32,8.23C21.16,7.63 21.88,6.87 22.46,6Z" /></svg>
	

					
					</a></li>
			
				<li><a target="_blank" href="https://youtube.com/" class="YouTube-icon">
					
						
		<svg class="youtube" fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M10,16.5V7.5L16,12M20,4.4C19.4,4.2 15.7,4 12,4C8.3,4 4.6,4.19 4,4.38C2.44,4.9 2,8.4 2,12C2,15.59 2.44,19.1 4,19.61C4.6,19.81 8.3,20 12,20C15.7,20 19.4,19.81 20,19.61C21.56,19.1 22,15.59 22,12C22,8.4 21.56,4.91 20,4.4Z" /></svg>
	

					
					</a></li>
			
				<li><a  href="/feed.xml" class="RSS-icon">
					
						
		<svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="none"/><circle cx="6.18" cy="17.82" r="2.18"/><path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"/></svg>
		

					
					</a></li>
			
		</ul>
		<p class="copyright">&copy; Base 2018. All rights reserved.</p>
	</div>
</footer>
		<script>
			$(function() {
				$('a[href*=\\#]').not(".no-smooth").on('click', function(event){
					var el = $(this.hash);
					if (el.length > 0) {
						// event.preventDefault();
						$('html,body').animate({scrollTop:$(this.hash).offset().top - 50}, 500);
					}
				});

				$('svg').click(function() {
					$(this).parent('form').submit();
				});
			});

			document.getElementById("open-nav").addEventListener("click", function (event) {
				event.preventDefault();
				document.body.classList.toggle("nav-open");
			});
		</script>
		
	</body>
</html>
