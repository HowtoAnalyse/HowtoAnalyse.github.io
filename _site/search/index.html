<!DOCTYPE html>

<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<meta name="generator" content="Jekyll v3.4.3">

		<link rel="stylesheet" href="/css/screen.css">
		<link rel="apple-touch-icon" href="/apple-touch-icon.png">
		<link rel="icon" type="image/png" href="/touch-icon.png" sizes="192x192">
		<link rel="icon" type="image/png" href="/images/favicon.png">
		<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Merriweather:400italic,400,300italic,300,700,700italic|Open+Sans:400italic,600italic,700italic,700,600,400|Inconsolata:400,700">
		<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>

		<!-- Begin Jekyll SEO tag v2.1.0 -->
<title>Search - Base</title>
<meta property="og:title" content="Search" />
<meta name="description" content="Knowledge base template for Jekyll." />
<meta property="og:description" content="Knowledge base template for Jekyll." />
<link rel="canonical" href="https://orange-ape.cloudvent.net//search/" />
<meta property="og:url" content="https://orange-ape.cloudvent.net//search/" />
<meta property="og:site_name" content="Base" />
<script type="application/ld+json">
{"@context": "http://schema.org",
"@type": "WebPage",
"headline": "Search",
"description": "Knowledge base template for Jekyll.",
"publisher": {"@type": "Organization",
"logo": {"@type": "ImageObject",
"url": "https://orange-ape.cloudvent.net//siteicon.png"}},
"url": "https://orange-ape.cloudvent.net//search/"}</script>
<!-- End Jekyll SEO tag -->

		<link type="application/atom+xml" rel="alternate" href="https://orange-ape.cloudvent.net//feed.xml" title="Base" />

		
	</head>

	<body class="">
		<header>
			<div class="wrapper">
				<section class="top-bar">
					<a class="nav-toggle" id="open-nav" href="#">&#9776;</a>
<nav>
	<a class="editor-link btn" href="cloudcannon:collections/_data/navigation.yml" class="btn" style="padding: 5px;"><strong>&#9998;</strong> Edit navigation</a>
	
	

		
		<a href="/" class="">Tutorials</a>
	
	

		
		<a href="/videos/" class="">Videos</a>
	
	

		
		<a href="/faq/" class="">FAQ</a>
	
</nav>

				</section>
				<section class="hero_search">
					<h1>Tutorials</h1>
					<p>Everything you need to know about running our software.</p>
					<form action="/search/" method="get">
	<input type="search" name="q"  placeholder="What would you like to know?" autofocus>
	<svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"/>
    <path d="M0 0h24v24H0z" fill="none"/>
</svg>
	<input type="submit" value="Search" style="display: none;">
</form>
				</section>
			</div>

		</header>
		<section class="content">
			<div class="wrapper">
				<p><span id="search-process">Loading</span> results <span id="search-query-container" style="display: none;">for "<strong id="search-query"></strong>"</span></p>
<ul id="search-results"></ul>

<script>
	window.data = {
		
			
				
					
					
					"machine-learning-numeric-feature-engineering": {
						"id": "machine-learning-numeric-feature-engineering",
						"title": "Feature Engineering on Numeric features",
						"categories": "Machine-Learning",
						"url": " /machine-learning/Numeric-feature-engineering/",
						"content": "Feature Preprocessing\n\nScaling\n\nScaling numeric features ensures that their initial impact on models are relatively the same.\n\nWinsorization or Rank Transformation to deal with Outliers\n\nOutliers are not only exist in features but our target as well.\n\nWinsorization\n\nTo protect linear models from outliers, we can clip features values between two chosen values of lower bound and upper bound. We can choose them as some percentiles of that feature. For example, first and 99s percentiles. This procedure of clipping is known as winsorization\n\nRank Transformation\n\nRank transformation sets spaces between properly assorted values to be equal. It can be a better choice than MinMaxScaler if we have outliers bacause it moves outliers closer to normal objects.\n\nThere are 2 choice while applying rank transformation to the test set:\n\n\n  store the creative mapping from features values to their rank values\n  concatenate train and test sets before applying the rank transformation.\n\n\nscipy.stats.rankdata\n\n\n\nLog transformation\n\nLog transformation acts as a representative of methametical transformations that often helps non-tree-based models and especially neural networks. As an alternative, you may extract a square root of the data.\n\nBoth of these transformations can:\n\n  drive extremely big values closer to the features’ average value.\n  make values near zero to be more distinguishable.\n\n\nDespite the simplicity, one of these transformations can improve your neural network’s results significantly.\n\nIn conclusion, linear models, KNN, and neural networks can benefit hugely from this.\n\nFeature Generation\n\nFeature generation is a process of deriving new features using logic or knowledge derived after data exploration and hypothesis checking.\n\nExamples:\n\n\n  Real Estate price and Real Estate squared area -&gt; price per meter square\n  Horizontal distance and the vertical difference in heights -&gt; direct distance.\n\n\nSuch examples can be of help not only for linear models. For example, although gradient within decision tree is a very powerful model, it still experiences difficulties with approximation of multiplications and divisions. And adding size features explicitly can lead to a more robust model with less amount of trees.\n\nApart from examples like adding, multiplications, divisions, and other features interactions, we can also:\n\n\n  A new feature indicating fractional part of these prices. For example, if some product costs 2.49, the fractional part of its price is 0.49. This feature can help the model utilize the differences in people’s perception of these prices.\n\n\nAlso, we can find similar patterns in tasks which require distinguishing between a human and a robot. For example, if we will have some kind of financial data like auctions, we could observe that people tend to set round numbers as prices, and there are something like 0.935, blah, blah,, blah, very long number here. Or, if we are trying to find spambots on social networks, we can be sure that no human ever read messages with an exact interval of one second."
					}
					
				
			
		
			
				
					,
					
					"machine-learning-missing-data-imputation": {
						"id": "machine-learning-missing-data-imputation",
						"title": "Feature Engineering Part II -- Missing Values",
						"categories": "Machine-Learning",
						"url": " /machine-learning/Missing-data-imputation/",
						"content": "Imputation\n\na. Replace it with a number outside the normal range like -999 \nb. Replace it with mean or median\nc. Reconstruct values\n\nMethod a is useful because it gives trees the possibility to take missing values into a separate category. The downside is that performance of linear models and neural networks will suffer.\n\nMethod b is beneficial for simple linear models and neural networks. But tree-based methods can be harder to select object with missing values in the first place.\n\nMethod c: Reconstruct values\n\nWhen data points are dependent to each other like time series data, we can approximate NAs using observations in the neighborhood.\n\nChallenges come when data points are independent.\n\nFeature Generation\n\nSometimes we create a binary feature, isnull, indicating which rows have missing values for this feature. Through this way, we can address concerns about trees and neural networks while computing mean or median. The drawback is that we will double the number of columns.\n\nThis method is especially worth trying for missing data occured in test set only. The intuition behind is that categories absent in the training set will be treated randomly eventually.\n\nAs an alternative, we may have a try on frequency encoding with missing values as a new category.\n\nXgboost can handle NaN."
					}
					
				
			
		
			
				
					,
					
					"machine-learning-categorical-feature-engineering": {
						"id": "machine-learning-categorical-feature-engineering",
						"title": "Feature Engineering on Categorical and Ordinal features&quot;",
						"categories": "Machine-Learning",
						"url": " /machine-learning/Categorical-feature-engineering/",
						"content": "Ordinal features refer to ordered categorical features. Examples include\n\n  Driver’s license: A,B,C,D\n  Education level: Bachelor, Master, Doctoral\n  …\n\n\nDifference between Numeric and Ordinal Features with values 1,2,3…\n\nFor numerical features with values 1,2,3…, we can conclude that the distance between first, and the second class is equal to the distance between second and the third class, but because for ordinal features, we can’t tell which distance is bigger.\n\nAs these numeric features, we can’t sort and integrate an ordinal feature the other way, and expect to get similar performance.\n\nLabel Encoding\n\nThe simplest way to encode a categorical feature is to map it’s unique values to different numbers.\n\nThere are different method. Be creative in constructing them\n\nFollowing are three examples:\n\n\n  Sorted alphabetical order\n    sklearn.preprocessing.LabelEncoding\n\n    \n  \n  Order of appearance\n    pandas.factorize\n\n    \n  \n\n\nFrequency Encoding\n\nencoding = df.groupby('col1').size()\nencoding = encoding / len(df)\ndf['enc']=df.col1.map(encoding)\n\n\n\nLabel encoding and frequency encoding work fine with tree-based models because models can split feature, and extract most of the useful values in categories on its own.\n\nBut linear models might get confused by non linear dependence of categorical features. The non-linear dependence refers to cases like:\n+——+————+———–+\n| id   | Cat       | target    |\n+——+————+———–+\n| 5    | 1 | 1       |\n| 6    | 2 | 0     |\n| 7    | 3 | 1       |\n| 8    | 4 | 1       |\n+——+————+———–+\n\nOne-hot Encoding\n\nOne-hot Encoding creates a new column for each unique value of our categorical feature and put one in the appropriate place. Everything else will be 0. This works well for linear models, k-NN and Neural Nets.\n\nSituations where we have a few important numeric features and hundreds of binary features generated by one-hot encoding may slow down tree-based models, not always improving their results.\n\nFeature Generation\n\nFeature interaction between several categorical features\n\nThis method works well with non-tree based models."
					}
					
				
			
		
			
				
					,
					
					"machine-learning-bag-of-words": {
						"id": "machine-learning-bag-of-words",
						"title": "Bag of words",
						"categories": "Machine-Learning",
						"url": " /machine-learning/bag-of-words/",
						"content": "If text and images are the only data we got, it’s more convenient to apply specific approach for these types of data.\n\nThe common scenario is that we have text or images as additional data set, we need to grasp different features that can be input to machine learning models as a complementary to our main data frame of samples and features.\n\nBag of words (BOW)\n\nPipeline of applying BOW:\n\n  Preprocessing:\n Lowercase, stemming, lemmatization, stopwords\n  N-grams can help to use local context\n  Postprocessing: TF-IDF\n\n\nTF-IDF\n\nIntuition: normalize data column-wise\n\nsklearn.feature_extraction.text.TfidfVectorizer\n\n\n\nTerm Frequency (TF)\ntf = 1/x.sum(axis=1)[:,None]\nx = x*tf\n\n\n\nInverse Document Frequency (IDF)\nidf = np.log(x.shape[0]/(x&gt;0).sum(0))\nx=x*idf\n\n\n\nN-grams\n\nsklearn.feature_extraction.text.CountVectorizer: \nNgram_range, analyzer\n\n\n\nIf you have 28 unique symbols, the number of all possible combinations is 28x28\n\nNext: Embeddings\n\nEmbeddings like Word2Vec"
					}
					
				
			
		
			
				
					,
					
					"recommender-word2vec": {
						"id": "recommender-word2vec",
						"title": "Word2Vec",
						"categories": "Recommender",
						"url": " /recommender/Word2Vec/",
						"content": "At Openrice, I’m always looking for ways to leverage massive data set to automate customer-facing experiences."
					}
					
				
			
		
			
				
					,
					
					"statistics-statistical-tests": {
						"id": "statistics-statistical-tests",
						"title": "Statistical tests",
						"categories": "Statistics",
						"url": " /statistics/Statistical-tests/",
						"content": ""
					}
					
				
			
		
			
				
					,
					
					"etl-pymongo": {
						"id": "etl-pymongo",
						"title": "PyMongo",
						"categories": "ETL",
						"url": " /etl/PyMongo/",
						"content": ""
					}
					
				
			
		
			
				
					,
					
					"recommender-deep-learning": {
						"id": "recommender-deep-learning",
						"title": "Deep Neural Networds",
						"categories": "Recommender",
						"url": " /recommender/Deep-Learning/",
						"content": ""
					}
					
				
			
		
			
				
					,
					
					"recommender-collaborative-filtering": {
						"id": "recommender-collaborative-filtering",
						"title": "Collaborative Filtering",
						"categories": "Recommender",
						"url": " /recommender/Collaborative-Filtering/",
						"content": ""
					}
					
				
			
		
			
				
					,
					
					"statistics-ab-testing": {
						"id": "statistics-ab-testing",
						"title": "A/B Testing",
						"categories": "Statistics",
						"url": " /statistics/AB-testing/",
						"content": ""
					}
					
				
			
		
			
				
					,
					
					"machine-learning-machine-learning-recap": {
						"id": "machine-learning-machine-learning-recap",
						"title": "Machine Learning Recap",
						"categories": "Machine-Learning",
						"url": " /machine-learning/machine-learning-recap/",
						"content": "No Free Lunch Theorem\n\nBasically, No Free Lunch Theorem states that there is no methods which outperform all others on all tasks.\n\nThe reason behind is that every method relies on certain assumptions about data or task. If these assumptions fail, it will perform poorly.\n\nFor us, this means that we cannot every competition with just a single algorithm. So it is important for us to have a clear mind map of various models based off different assumptions.\n\nThen let’s start getting familiar with the four popular families of machine learning algorithms.\n\nLinear Model\n\nLinear models try to separate objects with a plane which divides space into two parts.\n\nWith 2 sets of points, it is quite intuitive to separate them using a line. This approach can be generalized for a higher dimensional space. This is the main idea behind linear models.\n\nLogistic regression or SVM are all linear models withdifferent loss functions.\n\nLinear models are especially good for sparse high dimensional data.\n\nTree-Based Methods\n\nIn general, tree-based models are very powerful and can be a good default method for tabular data.\n\nIntuitively, a single decision tree can be considered as dividing space into boxes and approximating data with a constant inside of these boxes. It uses divide-and-conquer approach to recur sub-split spaces into sub-spaces.\n\nThe way of true axis splits and corresponding constants produces several approaches for building decision trees. Moreover, such trees can be combined together in a lot of ways. All this leads to a wide variety of tree-based algorithms, most famous of them being random forest and Gradient Boosted Decision Trees.\n\nScikit-Learn contains quite good implementation of random forest. For gradient boost decision trees, you may find XGBoost and LightGBM with higher speed and accuracy.\n\nk-Nearest Neighbors (k-NN)\n\nThe intuition behind k-NN is very simple. Closer objects will likely to have same labels.\n\nNeural Networks\n\nNeural Nets is a special class of machine learning models, which deserve a separate topic."
					}
					
				
			
		
			
				
					,
					
					"machine-learning-dm-stat-ml": {
						"id": "machine-learning-dm-stat-ml",
						"title": "Data mining, statistics, machine learning and AI",
						"categories": "Machine-Learning",
						"url": " /machine-learning/dm-stat-ml/",
						"content": "Artificial Intelligence is the study of how to create intelligent agents. Most tasks that require intelligence require an ability to induce new knowledge from experiences. Thus, a large area within AI is machine learning. Procedures in machine learning include ideas derived directly from, or inspired by, classical statistics, but they don’t have to be. Data mining is an area that has taken much of its inspiration and techniques from machine learning (and some, also, from statistics), but aims at either to discover / generate some preliminary insights in an area where there really was little knowledge beforehand, or to be able to predict future observations accurately.\n\nThere are considerable overlaps among them. If we have to make some distinctions, they are:\n\nStatistics is about numbers. It is mostly employed towards better understanding particular data generating processes. Thus, it usually starts with a formally specified model, and from this are derived procedures to accurately extract that model from noisy instances (i.e., estimation–by optimizing some loss function) and to be able to distinguish it from other possibilities (i.e., inferences based on known properties of sampling distributions). The prototypical statistical technique is regression.\n\nData Mining is about using Statistics as well as other programming methods to find patterns hidden in the data so that you can explain some phenomenon. Data Mining builds intuition about what is really happening in some data and is still little more towards math than programming, but uses both. Common data mining techniques would include cluster analyses, classification and regression trees, and neural networks.\n\nMachine Learning uses Data Mining techniques and other learning algorithms to build models of what is happening behind some data so that it can predict future outcomes. Math is the basis for many of the algorithms, but this is more towards programming. A computer program is said to learn some task from experience if its performance at the task improves with experience, according to some performance measure. Machine learning involves the study of algorithms that can extract information automatically (i.e., without on-line human guidance).\n\nArtificial Intelligence uses models built by Machine Learning and other ways to reason about the world and give rise to intelligent behavior whether this is playing a game or driving a robot/car. Artificial Intelligence has some goal to achieve by predicting how actions will affect the model of the world and chooses the actions that will best achieve that goal. Very programming based."
					}
					
				
			
		
			
				
					,
					
					"machine-learning-cv": {
						"id": "machine-learning-cv",
						"title": "Test set vs. Validation set",
						"categories": "Machine-Learning",
						"url": " /machine-learning/cv/",
						"content": "When you have a large data set, it’s recommended to split it into 3 parts:\n\n\n  \n    Training set: This is used to build up our prediction algorithm. Our algorithm tries to tune itself to the quirks of the training data sets. In this phase we usually create multiple algorithms in order to compare their performances during the Cross-Validation Phase.\n  \n  \n    Validation set: This data set is used to compare the performances of the prediction algorithms that were created based on the training set. We choose the algorithm that has the best performance.\n  \n  \n    Test set: Now we have chosen our preferred prediction algorithm but we don’t know yet how it’s going to perform on completely unseen real-world data. So, we apply our chosen prediction algorithm on our test set in order to see how it’s going to perform so we can have an idea about our algorithm’s performance on unseen data.\n  \n\n\nNotes:\n\n-It’s very important to keep in mind that skipping the test phase is not recommended, because the algorithm that performed well during the validation phase doesn’t really mean that it’s truly the best one, because the algorithms are compared based on the cross-validation set and its quirks and noises.\n\n-During the Test Phase, the purpose is to see how our final model is going to deal in the wild, so in case its performance is very poor we should repeat the whole process starting from the Training Phase."
					}
					
				
			
		
			
				
					,
					
					"machine-learning-bias-variance-tradeoff": {
						"id": "machine-learning-bias-variance-tradeoff",
						"title": "Bias Variance Tradeoff",
						"categories": "Machine-Learning",
						"url": " /machine-learning/bias-variance-Tradeoff/",
						"content": "In short,\n\n  bias is how far away a model’s predictions are from true values,\n  variance is how scattered these predictions are among model iterations.\n\n\n\n\n\n\n  Error due to Bias: The error due to bias is taken as the difference between the expected (or average) prediction of our model and the correct value which we are trying to predict. Of course you only have one model so talking about expected or average prediction values might seem a little strange. However, imagine you could repeat the whole model building process more than once: each time you gather new data and run a new analysis creating a new model. Due to randomness in the underlying data sets, the resulting models will have a range of predictions. Bias measures how far off in general these models’ predictions are from the correct value.\nScott Fortmann-Roe\n\n\n\n\n  Error due to Variance: The error due to variance is taken as the variability of a model prediction for a given data point. Again, imagine you can repeat the entire model building process multiple times. The variance is how much the predictions for a given point vary between different realizations of the model.\nScott Fortmann-Roe"
					}
					
				
			
		
			
				
					,
					
					"machine-learning-generative-discriminative": {
						"id": "machine-learning-generative-discriminative",
						"title": "Generative vs. Discriminative",
						"categories": "Machine-Learning",
						"url": " /machine-learning/Generative-Discriminative/",
						"content": "The fundamental distinction between discriminative models and generative models is:\n\n\n  Discriminative models learn the (hard or soft) boundary between classes\n  Generative models model the distribution of individual classes\n\n\nDiscriminative\n\nSVMs and decision trees are discriminative because they learn explicit boundaries between classes.\n\nSVM is a maximal margin classifier, meaning that it learns a decision boundary that maximizes the distance between samples of the two classes, given a kernel. The distance between a sample and the learned decision boundary can be used to make the SVM a “soft” classifier.\n\nDecision trees learn the decision boundary by recursively partitioning the space in a manner that maximizes the information gain (or another criterion).\n\nGenerative\n\nGenerative models are typically specified as probabilistic graphical models. They offer rich representations of the independence relations in the dataset.\n\nDiscriminative models do not offer such clear representations of relations between features and classes in the dataset. Instead of using resources to fully model each class, they focus on richly modeling the boundary between classes. Given the same amount of capacity (say, bits in a computer program executing the model), a discriminative model thus may yield more complex representations of this boundary than a generative model.\n\nWhen you are dealing with non-stationary distributions where the online test data may be generated by different underlying distributions than the training data, it is typically more straightforward to detect distribution changes and update a generative model accordingly than do this for a decision boundary in an SVM, especially if the online updates need to be unsupervised.\n\nTo sum up:\n\n\n  \n    \n      Discriminative models\n      Generative models\n    \n  \n  \n    \n      learn the (hard or soft) boundary between classes\n      model the distribution of individual classes\n    \n    \n      generally do not function for outlier detection\n      generally function for outlier detection\n    \n    \n      do not offer clear representations of relations between features and classes in the dataset\n      offer rich representations of the independence relations in the dataset"
					}
					
				
			
		
			
				
					,
					
					"machine-learning-ensemble-methods": {
						"id": "machine-learning-ensemble-methods",
						"title": "Ensemble Learning",
						"categories": "Machine-Learning",
						"url": " /machine-learning/Ensemble-methods/",
						"content": "Bagging and boosting are two families of ensemble methods.\n\nEnsemble methods aim at combining the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.\n\nBagging(short for Bootstrap Aggregating):\n\n\n  build several base estimators independently and then to average their predictions.\n  aim to decrease variance by generating additional data for training from the original dataset\n  suitable for models with high variance low bias (complex models)\n  Examples: Random forest, which develop fully grown trees (note that RF modifies the grown procedure to reduce the correlation between trees)\n\n\nBoosting:\n\n\n  build several base estimators sequentialy and one tries to reduce the bias of the combined estimator\n  aim to decrease bias\n  suitable for models with low variance high bias\n  Examples:  Gradient boosting"
					}
					
				
			
		
			
				
					,
					
					"machine-learning-decision-tree": {
						"id": "machine-learning-decision-tree",
						"title": "Decision Trees",
						"categories": "Machine-Learning",
						"url": " /machine-learning/Decision-Tree/",
						"content": "Decision tree classifiers are incredibly simple in theory. In their simplest form, decision tree classifiers ask a series of Yes/No questions about the data — each time getting closer to finding out the class of each entry — until they either classify the data set perfectly or simply can’t differentiate a set of entries. Think of it like a game of Twenty Questions, except the computer is much, much better at it.\n\nThe nice part about decision tree classifiers is that they are scale-invariant, i.e., the scale of the features does not affect their performance.\n\nA common problem that decision trees face is that they’re prone to overfitting: They complexify to the point that they classify the training set near-perfectly, but fail to generalize to data they have not seen before.\nRandom Forest classifiers work around that limitation by creating a whole bunch of decision trees (hence “forest”) — each trained on random subsets of training samples (drawn with replacement) and features (drawn without replacement) — and have the decision trees work together to make a more accurate classification.\n\nRandom Forest\n\nIn RandomForest model we average n similar performing trees (“forest”), trained independently. So two RF with 1000 trees is essentially the same as single RF model with 2000 trees.\n\nGradient Boosted Decision Trees\n\nIn GBDT model we have sequence of trees, each improve predictions of all previous.\n\nSo taking other settings the same, dropping the n-th tree has different effects on these two models.\n\nFor Random Forest, the order of trees does not matter in RandomForest and performance drop will be very similar on average.\n\nFor GBDT:\n\n  if we drop first tree, sum of all the rest trees will be biased and overall performance should drop.\n  if we drop the last tree, sum of all previous tree won’t be affected, and therefore performance will change insignificantly (in case we have enough trees)"
					}
					
				
			
		
			
				
					,
					
					"communication-line-plot": {
						"id": "communication-line-plot",
						"title": "Line Plot",
						"categories": "Communication",
						"url": " /communication/Line-Plot/",
						"content": "Visualization\n\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport datetime as dt\n\ndates = []\nfor month in range(2, 12):\n    dates.append(dt.datetime(year=2017, month=month, day=1))\n\n\ndf=pd.DataFrame({'x': dates, 'y1': np.random.randn(10), 'y3': np.random.randn(10)+range(11,21), 'y5': np.random.randn(10)+range(4,14)+(0,0,9,0,0,0,0,1,2,0), })\nplt.style.use('seaborn-darkgrid')\nmy_dpi=96\nplt.figure(figsize=(960/my_dpi, 480/my_dpi), dpi=my_dpi)\n \n# multiple line plot\nfor column in df.drop('x', axis=1):\n   plt.plot(df['x'], df[column], marker='', color='grey', linewidth=1, alpha=0.4)\n \n\nplt.plot(df['x'], df['y5'], marker='', color='orange', linewidth=4, alpha=0.7)\n\n# num=0\n# for i in df.values[9][1:]:\n#    num+=1\n#    name=list(df)[num]\n#    if name != 'y5':\n#       plt.text(0.000000000005, i, name, horizontalalignment='left', size='small', color='grey')\n\n# And add a special annotation for the group we are interested in\n# plt.text(10.2, df.y5.tail(1), 'Mr Orange', horizontalalignment='left', size='small', color='orange')\n\n# Add titles\nplt.title(\"Pageviews of Machine Learning posts vs other categories\", loc='left', fontsize=12, fontweight=0, color='orange')\nplt.xlabel(\"YearMonth\")\nplt.ylabel(\"Pageviews\")\n\n\n\n\n\n\n\n\nDescription\n\nThis graph shows the growth of the pageviews of howtoanalyse.github.io from February,2017 to December,2017. It is taken from Google Analytics.\n\nThere are eight graphs in the chart. Each graph deals with the pageviews of a category. The orange graph shows the pageviews on Machine Learning posts. In April,2017 there was an enourmous growth. In the following months the total growth went down to about 7 in June, 2017. From that time on the pageviews of Machine Learning posts has been gradually growing again although the natural increase slows down. So we can say that the growth of the pageviews in Machine Learning posts is based on Ads."
					}
					
				
			
		
			
				
					,
					
					"pandas-convert-pandas-dataframe-to-dictionary": {
						"id": "pandas-convert-pandas-dataframe-to-dictionary",
						"title": "DataFrame to Dictionary",
						"categories": "Pandas",
						"url": " /pandas/Convert-Pandas-DataFrame-to-Dictionary/",
						"content": "to_dict() method\n\nConsider the following simple DataFrame:\n\nimport pandas as pd\ndf=pd.DataFrame(\n{'Users':['a','b'],'Events':[5,75]},index=['0','1'])\ndf\n\n\n\n\n\n\n  \n    \n      \n      Events\n      Users\n    \n  \n  \n    \n      0\n      5\n      a\n    \n    \n      1\n      75\n      b\n    \n  \n\n\n\nto_dict() converts it into a dictionary.\n\ndf.to_dict()\n\n\n\n{'Events': {'0': 5, '1': 75}, 'Users': {'0': 'a', '1': 'b'}}\n\n\n\nSpecify the return orientation\n\ndict is the default format as shown above.\n\nIn case a different dictionary format is needed, here are examples of the possible orient arguments:\n\nlist - Keys of the converted dictionary are columns names, values are lists of column data\n\ndf.to_dict('list')\n\n\n\n{'Events': [5, 75], 'Users': ['a', 'b']}\n\n\n\nseries - like list while values are Series\n\ndf.to_dict('series')\n\n\n\n{'Events': 0     5\n 1    75\n Name: Events, dtype: int64, 'Users': 0    a\n 1    b\n Name: Users, dtype: object}\n\n\n\nsplit - splits columns, data and index as keys\n\ndf.to_dict('split')\n\n\n\n{'columns': ['Events', 'Users'],\n 'data': [[5, 'a'], [75, 'b']],\n 'index': ['0', '1']}\n\n\n\nrecords - each row becomes a dictionary where key is the column name and value is the data in the cell\n\ndf.to_dict('records')\n\n\n\n[{'Events': 5, 'Users': 'a'}, {'Events': 75, 'Users': 'b'}]\n\n\n\nindex - like records, but a dictionary of dictionaries with keys as index labels (rather than a list)\n\ndf.to_dict('index')\n\n\n\n{'0': {'Events': 5, 'Users': 'a'}, '1': {'Events': 75, 'Users': 'b'}}"
					}
					
				
			
		
			
				
					,
					
					"python-default-parameter-values": {
						"id": "python-default-parameter-values",
						"title": "Default Parameter Values",
						"categories": "Python",
						"url": " /python/Default-parameter-values/",
						"content": "Let’s have an example of what this post is going to talk about:\n\ndef foo(a=[]):\n    a.append(5)\n    return a\n\n\n\nfoo()\n\n\n\n[5]\n\n\n\nfoo()\n\n\n\n[5, 5]\n\n\n\nfoo()\n\n\n\n[5, 5, 5]\n\n\n\nThis happens because\n\n\n  Default parameter values are evaluated when the function definition is executed.\n\n\nThis means that the expression is evaluated once, when the function is defined, and that the same “pre-computed” value is used for each call.\n\nThis is especially important to understand when a default parameter is a mutable object, such as a list or a dictionary: if the function modifies the object (e.g. by appending an item to a list), the default value is in effect modified.\n\nA way around this is to use None as the default, and explicitly test for it in the body of the function.\n\ndef foo(a=None):\n    if a==None:\n        a=[]\n    else:\n        a.append(5)\n    return a\n\n\n\nfoo()\n\n\n\n[]\n\n\n\nfoo()\n\n\n\n[]\n\n\n\nfoo()\n\n\n\n[]"
					}
					
				
			
		
			
				
					,
					
					"machine-learning-cross-validation": {
						"id": "machine-learning-cross-validation",
						"title": "Cross Validation",
						"categories": "Machine-Learning",
						"url": " /machine-learning/Cross-Validation/",
						"content": "What is cross-validation?\n\nIt’s a model validation technique for assessing how the results of a statistical analysis will generalize to an independent data set.\n\nIt’s common to find that our model performs differently depending on the subset of the data it’s trained on. This phenomenon is known as overfitting: The model is learning to classify the training set so well that it doesn’t generalize and perform well on data it hasn’t seen before.\n\nSo we split the original data set into k subsets, use one of the subsets as the testing set, and the rest of the subsets are used as the training set. This process is then repeated k times such that each subset is used as the testing set exactly once.\n\nThis process is what we called cross-validation.\n\nExamples: leave-one-out cross validation, K-fold cross validation\n\nHow to do it right?\n\nFirstly, the training and validation data sets should be drawn from the same population.\n\npredicting stock prices: trained for a certain 5-year period, it’s unrealistic to treat the subsequent 5-year a draw from the same population\ncommon mistake: for instance the step of choosing the kernel parameters of a SVM should be cross-validated as well\n\nBias-variance trade-off for k-fold cross validation\n\nLeave-one-out cross-validation: gives approximately unbiased estimates of the test error since each training set contains almost the entire data set (n−1 observations).\n\nBut: we average the outputs of n fitted models, each of which is trained on an almost identical set of observations hence the outputs are highly correlated. Since the variance of a mean of quantities increases when correlation of these quantities increase, the test error estimate from a LOOCV has higher variance than the one obtained with k-fold cross validation\n\nTypically, we choose k=5 or k=10, as these values have been shown empirically to yield test error estimates that suffer neither from excessively high bias nor high variance.\n\nRobust or accurate algorithms, how do you choose?\n\nSimpler models are preferred if more complex models do not significantly improve the quality of the description for the observations.\n\nOur ultimate goal is to design systems with good generalization capacity, that is, systems that correctly identify patterns in data instances not seen before. While the generalization performance of a learning system strongly depends on the complexity of the model assumed.\n\nIf the model is too simple, the system can only capture the actual data regularities in a rough manner. In this case, the system has poor generalization properties and is said to suffer from underfitting.\n\nBy contrast, if the model is too complex, the system can identify accidental patterns in the training data that need not be present in the test set. These spurious patterns can be the result of random fluctuations or of measurement errors during the data collection process. In this case, the generalization capacity of the learning system is also poor. The learning system is said to be affected by overfitting. Spurious patterns, which are only present by accident in the data, tend to have complex forms.\n\nBy the way, ensemble learning can help balancing bias/variance. Several weak learners together = strong learner.\n\nHow do you select metrics?\n\nClassification\n\n\n  \n    Recall / Sensitivity / True positive rate\n  \n  \n    Precision / Positive Predictive value\n  \n  \n    Specificity / True negative rate\n  \n  \n    Accuracy\n  \n  \n    ROC / AUC\n  \n\n\nROC is a graphical plot that illustrates the performance of a binary classifier (SensitivitySensitivity Vs 1−Specificity1−Specificity or SensitivitySensitivity Vs SpecificitySpecificity). They are not sensitive to unbalanced classes.\nAUC is the area under the ROC curve. Perfect classifier: AUC=1, fall on (0,1); 100% sensitivity (no FN) and 100% specificity (no FP)\n\n\n  Logarithmic loss\n\n\nPunishes infinitely the deviation from the true value! It’s better to be somewhat wrong than emphatically wrong!\n\n\n  \n    Misclassification Rate\n  \n  \n    F1-Score\n  \n\n\nRegression\n\n\n  Mean Squared Error Vs Mean Absolute Error\n\n\nRMSE gives a relatively high weight to large errors. The RMSE is most useful when large errors are particularly undesirable.\n\nThe MAE is a linear score: all the individual differences are weighted equally in the average. MAE is more robust to outliers than MSE.\n\n\n  Root Mean Squared Logarithmic Error\n\n\nRMSLE penalizes an under-predicted estimate greater than an over-predicted estimate (opposite to RMSE)\n\n\n  Weighted Mean Absolute Error\n\n\nThe weighted average of absolute errors. MAE and RMSE consider that each prediction provides equally precise information about the error variation, i.e. the standard variation of the error term is constant over all the predictions. Examples: recommender systems (differences between past and recent products)"
					}
					
				
			
		
	};
</script>
<script src="/js/lunr.min.js"></script>
<script src="/js/search.js"></script>
			</div>
		</section>

		<footer>
	<div class="wrapper">
		<p class="edit-footer"><a class="editor-link btn" href="cloudcannon:collections/_data/footer.yml" class="btn" style="padding: 5px;"><strong>&#9998;</strong> Edit footer</a></p>
		<ul class="footer-links">
			
				<li><a target="_blank" href="https://facebook.com/" class="Facebook-icon">
					
						
		<svg class="facebook" fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M19,4V7H17A1,1 0 0,0 16,8V10H19V13H16V20H13V13H11V10H13V7.5C13,5.56 14.57,4 16.5,4M20,2H4A2,2 0 0,0 2,4V20A2,2 0 0,0 4,22H20A2,2 0 0,0 22,20V4C22,2.89 21.1,2 20,2Z" /></svg>
	

					
					</a></li>
			
				<li><a target="_blank" href="https://twitter.com/" class="Twitter-icon">
					
						
		<svg class="twitter" fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M22.46,6C21.69,6.35 20.86,6.58 20,6.69C20.88,6.16 21.56,5.32 21.88,4.31C21.05,4.81 20.13,5.16 19.16,5.36C18.37,4.5 17.26,4 16,4C13.65,4 11.73,5.92 11.73,8.29C11.73,8.63 11.77,8.96 11.84,9.27C8.28,9.09 5.11,7.38 3,4.79C2.63,5.42 2.42,6.16 2.42,6.94C2.42,8.43 3.17,9.75 4.33,10.5C3.62,10.5 2.96,10.3 2.38,10C2.38,10 2.38,10 2.38,10.03C2.38,12.11 3.86,13.85 5.82,14.24C5.46,14.34 5.08,14.39 4.69,14.39C4.42,14.39 4.15,14.36 3.89,14.31C4.43,16 6,17.26 7.89,17.29C6.43,18.45 4.58,19.13 2.56,19.13C2.22,19.13 1.88,19.11 1.54,19.07C3.44,20.29 5.7,21 8.12,21C16,21 20.33,14.46 20.33,8.79C20.33,8.6 20.33,8.42 20.32,8.23C21.16,7.63 21.88,6.87 22.46,6Z" /></svg>
	

					
					</a></li>
			
				<li><a target="_blank" href="https://youtube.com/" class="YouTube-icon">
					
						
		<svg class="youtube" fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M10,16.5V7.5L16,12M20,4.4C19.4,4.2 15.7,4 12,4C8.3,4 4.6,4.19 4,4.38C2.44,4.9 2,8.4 2,12C2,15.59 2.44,19.1 4,19.61C4.6,19.81 8.3,20 12,20C15.7,20 19.4,19.81 20,19.61C21.56,19.1 22,15.59 22,12C22,8.4 21.56,4.91 20,4.4Z" /></svg>
	

					
					</a></li>
			
				<li><a  href="/feed.xml" class="RSS-icon">
					
						
		<svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="none"/><circle cx="6.18" cy="17.82" r="2.18"/><path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"/></svg>
		

					
					</a></li>
			
		</ul>
		<p class="copyright">&copy; Base 2018. All rights reserved.</p>
	</div>
</footer>
		<script>
			$(function() {
				$('a[href*=\\#]').not(".no-smooth").on('click', function(event){
					var el = $(this.hash);
					if (el.length > 0) {
						// event.preventDefault();
						$('html,body').animate({scrollTop:$(this.hash).offset().top - 50}, 500);
					}
				});

				$('svg').click(function() {
					$(this).parent('form').submit();
				});
			});

			document.getElementById("open-nav").addEventListener("click", function (event) {
				event.preventDefault();
				document.body.classList.toggle("nav-open");
			});
		</script>
		
	</body>
</html>
