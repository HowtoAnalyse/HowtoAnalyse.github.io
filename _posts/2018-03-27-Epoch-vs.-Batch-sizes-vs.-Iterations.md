---
layout: post
title: Epoch vs. Batch sizes vs. Iterations
author: sara
categories: [Machine Learning]
image: assets/images/dogs/pexels-photo-247937.jpeg
featured: True
---
# Epoch 

 

One epoch is when an entire dataset is passed forward and backward through the neural net only once.  

 

Gradient descent is an iterative process, so we need to feed the entire dataset multiple times to the same neural network to update weights for optimal result. 

 

The number of epochs is related to how diverse the data is. 

 

# Batch sizes 

 

Mini-batch sizes are commonly called 'batch sizes' for brevity. Since in most cases it is not feasible to feed the whole dataset into the neural network at one, we divide dataset into number of batches. 

 

The number of training samples present in a single batch is named batch size. 

 

Batch size is a slider on the learning process. 

Small values give a learning process that converges quickly at the cost of noise in the training process. 

Large values give a learning process that converges slowly with accurate estimates of the error gradient. 

 

 

# Iterations 

 

Iterations is the number of batches needed to complete one epoch 

 

#batches = #iterations for one epoch 

 

Let's say we divide a dataset with 50,000 training samples into batches of 1,000, then it needs 50 iterations to complete 1 epoch. 