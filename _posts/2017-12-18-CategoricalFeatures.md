---
layout: page
sidebar: right
title: "Feature Engineering"
subheadline: "Feature Engineering Part 1: Categorical and Ordinal features"
teaser: "Feature Preprocessing and generation with respect to models"
tags:
  - overview
categories:
  - Machine Learning
---

Ordinal features refer to ordered categorical features. Examples include 
* Driver's license: A,B,C,D
* Education level: Bachelor, Master, Doctoral
* ...

### Difference between Numeric and Ordinal Features with values 1,2,3...

For numerical features with values 1,2,3..., we can conclude that the distance between first, and the second class is equal to the distance between second and the third class, but because for ordinal features, we can't tell which distance is bigger. 

As these numeric features, we can't sort and integrate an ordinal feature the other way, and expect to get similar performance. 

### Label Encoding

The simplest way to encode a categorical feature is to map it's unique values to different numbers. 

There are different method. Be creative in constructing them

Following are three examples:

1. Sorted alphabetical order
```python
sklearn.preprocessing.LabelEncoding
```
2. Order of appearance
```python
pandas.factorize
```

### Frequency Encoding

```python
encoding = df.groupby('col1').size()
encoding = encoding / len(df)
df['enc']=df.col1.map(encoding)
```

Label encoding and frequency encoding work fine with tree-based models because models can split feature, and extract most of the useful values in categories on its own. 

But linear models might get confused by non linear dependence of categorical features. The non-linear dependence refers to cases like:
+------+------------+-----------+
| id   | Cat       | target    |
+------+------------+-----------+
| 5    | 1 | 1       |
| 6    | 2 | 0     |
| 7    | 3 | 1       |
| 8    | 4 | 1       |
+------+------------+-----------+

### One-hot Encoding

One-hot Encoding creates a new column for each unique value of our categorical feature and put one in the appropriate place. Everything else will be 0. This works well for linear models, k-NN and Neural Nets.

Situations where we have a few important numeric features and hundreds of binary features generated by one-hot encoding may slow down tree-based models, not always improving their results.

## Feature Generation

### Feature interaction between several categorical features

This method works well with non-tree based models.
